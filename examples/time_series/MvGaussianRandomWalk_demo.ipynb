{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec7d5a70-51ca-4c9c-a4bb-dcfc293e1e47",
   "metadata": {},
   "source": [
    "(MvGaussianRandomWalk)=\n",
    "# Multivariate Gaussian Random Walk\n",
    ":::{post} Feb 2, 2023\n",
    ":tags: linear model, regression, time series \n",
    ":category: beginner\n",
    ":author: Lorenzo Itoniazzi, Chris Fonnesbeck\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461c2a1-9e07-4a60-8df0-5f8e70a9d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "\n",
    "from scipy.linalg import cholesky\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dbc074-72ff-436b-8063-682779163661",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 8927\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d86071-b975-430d-973d-87295a17c8ac",
   "metadata": {},
   "source": [
    "This notebook shows how to [fit a correlated time series](https://en.wikipedia.org/wiki/Curve_fitting) using multivariate [Gaussian random walks](https://en.wikipedia.org/wiki/Random_walk#Gaussian_random_walk) (GRWs). In particular, we perform a Bayesian [regression](https://en.wikipedia.org/wiki/Regression_analysis) of the time series data against a model dependent on GRWs.\n",
    "\n",
    "We generate data as the 3-dimensional time series\n",
    "\n",
    "$$\n",
    "\\mathbf y = \\alpha_{i[\\mathbf t]} +\\beta_{i[\\mathbf t]} *\\frac{\\mathbf t}{300} +\\xi_{\\mathbf t},\\quad \\mathbf t = [0,1,...,299], \n",
    "$$ (eqn:model)\n",
    "\n",
    "where \n",
    "- $i\\mapsto\\alpha_{i}$ and $i\\mapsto\\beta_{i}$, $i\\in\\{0,1,2,3,4\\}$, are two 3-dimensional Gaussian random walks for two correlation matrices $\\Sigma_\\alpha$ and $\\Sigma_\\beta$,\n",
    "- we define the index \n",
    "$$\n",
    "i[t]= j\\quad\\text{for}\\quad t = 60j,60j+1,...,60j+59, \\quad\\text{and}\\quad j = 0,1,2,3,4,\n",
    "$$ \n",
    "- $*$ means that we multiply the $j$-th column of the $3\\times300$ matrix with the $j$-th entry of the vector for each $j=0,1,...,299$, and \n",
    "- $\\xi_{\\mathbf t}$ is a $3\\times300$ matrix with iid normal entries $N(0,\\sigma^2)$.\n",
    "\n",
    "\n",
    "So the series $\\mathbf y$ changes due to the GRW $\\alpha$ in five occasions, namely steps $0,60,120,180,240$. Meanwhile  $\\mathbf y$ changes at steps $1,60,120,180,240$ due to the increments of the GRW $\\beta$ and  at every step  due to the weighting of  $\\beta$ with $\\mathbf t/300$. Intuitively, we have a noisy ($\\xi$) system that is shocked five times over a period of 300 steps, but the impact of the $\\beta$ shocks gradually becomes more significant at every step. \n",
    "\n",
    "## Data generation\n",
    "\n",
    "Let's generate and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c329308-80b9-4e88-9698-199bdab66b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 3  # Dimension of random walks\n",
    "N = 300  # Number of steps\n",
    "sections = 5  # Number of sections\n",
    "period = N / sections  # Number steps in each section\n",
    "\n",
    "Sigma_alpha = rng.standard_normal((D, D))\n",
    "Sigma_alpha = Sigma_alpha.T.dot(Sigma_alpha)  # Construct covariance matrix for alpha\n",
    "L_alpha = cholesky(Sigma_alpha, lower=True)  # Obtain its Cholesky decomposition\n",
    "\n",
    "Sigma_beta = rng.standard_normal((D, D))\n",
    "Sigma_beta = Sigma_beta.T.dot(Sigma_beta)  # Construct covariance matrix for beta\n",
    "L_beta = cholesky(Sigma_beta, lower=True)  # Obtain its Cholesky decomposition\n",
    "\n",
    "# Gaussian random walks:\n",
    "alpha = np.cumsum(L_alpha.dot(rng.standard_normal((D, sections))), axis=1).T\n",
    "beta = np.cumsum(L_beta.dot(rng.standard_normal((D, sections))), axis=1).T\n",
    "t = np.arange(N)[:, None] / N\n",
    "alpha = np.repeat(alpha, period, axis=0)\n",
    "beta = np.repeat(beta, period, axis=0)\n",
    "# Correlated series\n",
    "sigma = 0.1\n",
    "y = alpha + beta * t + sigma * rng.standard_normal((N, 1))\n",
    "\n",
    "# Plot the correlated series\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(t, y, \".\", markersize=2, label=(\"y_0 data\", \"y_1 data\", \"y_2 data\"))\n",
    "plt.title(\"Three Correlated Series\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32971238-df46-4651-adc5-87033c191aa0",
   "metadata": {},
   "source": [
    "## Model\n",
    "First we introduce a scaling class to rescale our data and the time parameter before the sampling and then rescale the predictions to match the unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3717e-d634-42c1-891c-2db16a5c08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler:\n",
    "    def __init__(self):\n",
    "        mean_ = None\n",
    "        std_ = None\n",
    "\n",
    "    def transform(self, x):\n",
    "        return (x - self.mean_) / self.std_\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.mean_ = x.mean(axis=0)\n",
    "        self.std_ = x.std(axis=0)\n",
    "        return self.transform(x)\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return x * self.std_ + self.mean_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a67fa460-2ca1-4899-98a6-1b428c2215f3",
   "metadata": {},
   "source": [
    "We now construct the regression model in {eq}`eqn:model` imposing priors on the GRWs $\\alpha$ and $\\beta$, on the standard deviation $\\sigma$ and hyperpriors on the Cholesky matrices. We use the LKJ prior {cite:p}`lewandowski2009generating` for the Cholesky matrices (see this {func}`link for the documentation <pymc.LKJCholeskyCov>` and also the PyMC notebook {doc}`/case_studies/LKJ` for some usage examples.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232abfe-9129-49c3-869e-b6eb9bc6945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(t, y, sections, n_samples=100):\n",
    "    N, D = y.shape\n",
    "\n",
    "    # Standardize y and t\n",
    "    y_scaler = Scaler()\n",
    "    t_scaler = Scaler()\n",
    "    y = y_scaler.fit_transform(y)\n",
    "    t = t_scaler.fit_transform(t)\n",
    "    # Create a section index\n",
    "    t_section = np.repeat(np.arange(sections), N / sections)\n",
    "\n",
    "    # Create PyTensor equivalent\n",
    "    t_t = pytensor.shared(np.repeat(t, D, axis=1))\n",
    "    y_t = pytensor.shared(y)\n",
    "    t_section_t = pytensor.shared(t_section)\n",
    "\n",
    "    coords = {\"y_\": [\"y_0\", \"y_1\", \"y_2\"], \"steps\": np.arange(N)}\n",
    "    with pm.Model(coords=coords) as model:\n",
    "        # Hyperpriors on Cholesky matrices\n",
    "        chol_alpha, *_ = pm.LKJCholeskyCov(\n",
    "            \"chol_cov_alpha\", n=D, eta=2, sd_dist=pm.HalfCauchy.dist(2.5), compute_corr=True\n",
    "        )\n",
    "        chol_beta, *_ = pm.LKJCholeskyCov(\n",
    "            \"chol_cov_beta\", n=D, eta=2, sd_dist=pm.HalfCauchy.dist(2.5), compute_corr=True\n",
    "        )\n",
    "\n",
    "        # Priors on Gaussian random walks\n",
    "        alpha = pm.MvGaussianRandomWalk(\n",
    "            \"alpha\", mu=np.zeros(D), chol=chol_alpha, shape=(sections, D)\n",
    "        )\n",
    "        beta = pm.MvGaussianRandomWalk(\"beta\", mu=np.zeros(D), chol=chol_beta, shape=(sections, D))\n",
    "\n",
    "        # Deterministic construction of the correlated random walk\n",
    "        alpha_r = alpha[t_section_t]\n",
    "        beta_r = beta[t_section_t]\n",
    "        regression = alpha_r + beta_r * t_t\n",
    "\n",
    "        # Prior on noise Î¾\n",
    "        sigma = pm.HalfNormal(\"sigma\", 1.0)\n",
    "\n",
    "        # Likelihood\n",
    "        likelihood = pm.Normal(\"y\", mu=regression, sigma=sigma, observed=y_t, dims=(\"steps\", \"y_\"))\n",
    "\n",
    "        # MCMC sampling\n",
    "        trace = pm.sample(n_samples, tune=1000, chains=4, target_accept=0.9)\n",
    "\n",
    "        # Posterior predictive sampling\n",
    "        pm.sample_posterior_predictive(trace, extend_inferencedata=True)\n",
    "\n",
    "    return trace, y_scaler, t_scaler, t_section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8989c-3013-433b-9290-ca4008de8919",
   "metadata": {},
   "source": [
    "## Inference\n",
    "We now sample from our model and we return the trace, the scaling functions for space and time and the scaled time index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8e8b7-ce8e-4b57-abb5-440db4b9c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace, y_scaler, t_scaler, t_section = inference(t, y, sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be27698-c47b-47c9-ac96-2fd8c2287ae9",
   "metadata": {},
   "source": [
    "We now display the energy plot using {func}`arviz.plot_energy` for a visual check for the model's convergence. Then, using {func}`arviz.plot_ppc`,  we plot the distribution of the {doc}`posterior predictive samples </diagnostics_and_criticism/posterior_predictive>` against the observed data $\\mathbf y$. This plot provides a general idea of the accuracy of the model (note that the values of $\\mathbf y$ actually correspond to the scaled version of $\\mathbf y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f72a40-faae-4fa7-ba14-2de3c83ca2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(trace)\n",
    "az.plot_ppc(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffefc1ed-5fb5-44c1-93ab-3be3d6268ea8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Posterior visualisation\n",
    "The graphs above look good. Now we plot the observed 3-dimensional series against the average predicted 3-dimensional series, or in other words, we plot the data against the estimated regression curve from the model {eq}`eqn:model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c209fb9-26d5-4bd3-a77c-5f31db0f2545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the predicted mean of the multivariate GRWs\n",
    "alpha_mean = trace.posterior[\"alpha\"].mean(dim=(\"chain\", \"draw\"))\n",
    "beta_mean = trace.posterior[\"beta\"].mean(dim=(\"chain\", \"draw\"))\n",
    "\n",
    "# Compute the predicted mean of the correlated series\n",
    "y_pred = y_scaler.inverse_transform(\n",
    "    alpha_mean[t_section].values + beta_mean[t_section].values * t_scaler.transform(t)\n",
    ")\n",
    "\n",
    "# Plot the predicted mean\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "ax.plot(t, y, \".\", markersize=2, label=(\"y_0 data\", \"y_1 data\", \"y_2 data\"))\n",
    "plt.gca().set_prop_cycle(None)\n",
    "ax.plot(t, y_pred, label=(\"y_0 pred\", \"y_1 pred\", \"y_2 pred\"))\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Predicted Mean of Three Correlated Series\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b52ba-9b95-4f25-9420-e1cfa0a0d4d3",
   "metadata": {},
   "source": [
    "Finally, we plot the data against the posterior predictive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39b6792-71d4-4639-8804-63e145a9de78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rescale the posterior predictive samples\n",
    "ppc_y = y_scaler.inverse_transform(trace.posterior_predictive[\"y\"].mean(\"chain\"))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "# Plot the data\n",
    "ax.plot(t, y, \".\", markersize=3, label=(\"y_0 data\", \"y_1 data\", \"y_2 data\"))\n",
    "# Plot the posterior predictive samples\n",
    "ax.plot(t, ppc_y.sel(y_=\"y_0\").T, color=\"C0\", alpha=0.003)\n",
    "ax.plot(t, ppc_y.sel(y_=\"y_1\").T, color=\"C1\", alpha=0.003)\n",
    "ax.plot(t, ppc_y.sel(y_=\"y_2\").T, color=\"C2\", alpha=0.003)\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Posterior Predictive Samples and the Three Correlated Series\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39c47db9",
   "metadata": {},
   "source": [
    "## Authors\n",
    "* updated to best practices by Lorenzon Itoniazzi in October, 2021 ([pymc-examples#195](https://github.com/pymc-devs/pymc-examples/pull/195))\n",
    "* updated to v5 by Chris Fonnesbeck in February, 2023 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c177e79-e53d-446e-97c3-809df9177f1f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    ":::{bibliography}\n",
    ":filter: docname in docnames\n",
    ":::\n",
    "\n",
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07099e2-5167-4df3-a826-09e2b3e9950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p theano,xarray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f513c30",
   "metadata": {},
   "source": [
    ":::{include} ../page_footer.md\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "970ac73df0f14d7a1f980febd250c4ded990984ec0e2432b12dcbf556b464244"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
