{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(stochastic_volatility)=\n",
    "# Stochastic Volatility model\n",
    "\n",
    ":::{post} June 17, 2022\n",
    ":tags: time series, case study\n",
    ":category: beginner\n",
    ":author: John Salvatier, Colin Carroll, Abhipsha Das\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asset prices have time-varying volatility (variance of day over day `returns`). In some periods, returns are highly variable, while in others very stable. Stochastic volatility models model this with a latent volatility variable, modeled as a stochastic process. The following model is similar to the one described in the No-U-Turn Sampler paper, {cite:p}`hoffman2014nuts`.\n",
    "\n",
    "$$ \\sigma \\sim Exponential(50) $$\n",
    "\n",
    "$$ \\nu \\sim Exponential(.1) $$\n",
    "\n",
    "$$ s_i \\sim Normal(s_{i-1}, \\sigma^{-2}) $$\n",
    "\n",
    "$$ \\log(r_i) \\sim t(\\nu, 0, \\exp(-2 s_i)) $$\n",
    "\n",
    "Here, $r$ is the daily return series and $s$ is the latent log volatility process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load daily returns of the S&P 500, and calculate the daily log returns. This data is from May 2008 to November 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    returns = pd.read_csv(os.path.join(\"..\", \"data\", \"SP500.csv\"), index_col=\"Date\")\n",
    "except FileNotFoundError:\n",
    "    returns = pd.read_csv(pm.get_data(\"SP500.csv\"), index_col=\"Date\")\n",
    "\n",
    "returns[\"change\"] = np.log(returns[\"Close\"]).diff()\n",
    "returns = returns.dropna()\n",
    "\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the volatility seems to change over time quite a bit but cluster around certain time-periods. For example, the 2008 financial crisis is easy to pick out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "returns.plot(y=\"change\", label=\"S&P 500\", ax=ax)\n",
    "ax.set(xlabel=\"time\", ylabel=\"returns\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the model in `PyMC` mirrors its statistical specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stochastic_volatility_model(data):\n",
    "    with pm.Model(coords={\"time\": data.index.values}) as model:\n",
    "        step_size = pm.Exponential(\"step_size\", 10)\n",
    "        volatility = pm.GaussianRandomWalk(\n",
    "            \"volatility\", sigma=step_size, dims=\"time\", init_dist=pm.Normal.dist(0, 100)\n",
    "        )\n",
    "        nu = pm.Exponential(\"nu\", 0.1)\n",
    "        returns = pm.StudentT(\n",
    "            \"returns\", nu=nu, lam=np.exp(-2 * volatility), observed=data[\"change\"], dims=\"time\"\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "stochastic_vol_model = make_stochastic_volatility_model(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the model\n",
    "\n",
    "Two good things to do to make sure our model is what we expect is to \n",
    "1. Take a look at the model structure. This lets us know we specified the priors we wanted and the connections we wanted. It is also handy to remind ourselves of the size of the random variables.\n",
    "2. Take a look at the prior predictive samples. This helps us interpret what our priors imply about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(stochastic_vol_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with stochastic_vol_model:\n",
    "    idata = pm.sample_prior_predictive(500, random_seed=rng)\n",
    "\n",
    "prior_predictive = az.extract(idata, group=\"prior_predictive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot and inspect the prior predictive. This is *many* orders of magnitude larger than the actual returns we observed. In fact, I cherry-picked a few draws to keep the plot from looking silly. This may suggest changing our priors: a return that our model considers plausible would violate all sorts of constraints by a huge margin: the total value of all goods and services the world produces is ~$\\$10^9$, so we might reasonably *not* expect any returns above that magnitude.\n",
    "\n",
    "That said, we get somewhat reasonable results fitting this model anyways, and it is standard, so we leave it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "returns[\"change\"].plot(ax=ax, lw=1, color=\"black\")\n",
    "ax.plot(\n",
    "    prior_predictive[\"returns\"][:, 0::10],\n",
    "    \"g\",\n",
    "    alpha=0.5,\n",
    "    lw=1,\n",
    "    zorder=-10,\n",
    ")\n",
    "\n",
    "max_observed, max_simulated = np.max(np.abs(returns[\"change\"])), np.max(\n",
    "    np.abs(prior_predictive[\"returns\"].values)\n",
    ")\n",
    "ax.set_title(f\"Maximum observed: {max_observed:.2g}\\nMaximum simulated: {max_simulated:.2g}(!)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are happy with our model, we can sample from the posterior. This is a somewhat tricky model to fit even with NUTS, so we sample and tune a little longer than default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with stochastic_vol_model:\n",
    "    idata.extend(pm.sample(random_seed=rng))\n",
    "\n",
    "posterior = az.extract(idata)\n",
    "posterior[\"exp_volatility\"] = np.exp(posterior[\"volatility\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with stochastic_vol_model:\n",
    "    idata.extend(pm.sample_posterior_predictive(idata, random_seed=rng))\n",
    "\n",
    "posterior_predictive = az.extract(idata, group=\"posterior_predictive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `step_size` parameter does not look perfect: the different chains look somewhat different. This again indicates some weakness in our model: it may make sense to allow the step_size to change over time, especially over this 11 year time span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata, var_names=[\"step_size\", \"nu\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at our posterior estimates of the volatility in S&P 500 returns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "y_vals = posterior[\"exp_volatility\"]\n",
    "x_vals = y_vals.time.astype(np.datetime64)\n",
    "\n",
    "plt.plot(x_vals, y_vals, \"k\", alpha=0.002)\n",
    "ax.set_xlim(x_vals.min(), x_vals.max())\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set(title=\"Estimated volatility over time\", xlabel=\"Date\", ylabel=\"Volatility\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the posterior predictive distribution to see the how the learned volatility could have effected returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, figsize=(14, 7), sharex=True)\n",
    "returns[\"change\"].plot(ax=axes[0], color=\"black\")\n",
    "\n",
    "axes[1].plot(posterior[\"exp_volatility\"], \"r\", alpha=0.5)\n",
    "axes[0].plot(\n",
    "    posterior_predictive[\"returns\"],\n",
    "    \"g\",\n",
    "    alpha=0.5,\n",
    "    zorder=-10,\n",
    ")\n",
    "axes[0].set_title(\"True log returns (black) and posterior predictive log returns (green)\")\n",
    "axes[1].set_title(\"Posterior volatility\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p pytensor,aeppl,xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    ":::{bibliography}\n",
    ":filter: docname in docnames\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Written by John Salvatier\n",
    "* Updated by Kyle Meyer\n",
    "* Updated by Thomas Wiecki\n",
    "* Updated by Chris Fonnesbeck\n",
    "* Updated by Aaron Maxwell on May 18, 2018 ([pymc#2978](https://github.com/pymc-devs/pymc/pull/2978))\n",
    "* Updated by Colin Carroll on November 16, 2019 ([pymc#3682](https://github.com/pymc-devs/pymc/pull/3682))\n",
    "* Updated by Abhipsha Das on July 24, 2021 ([pymc-examples#155](https://github.com/pymc-devs/pymc-examples/pull/155))\n",
    "* Updated by Michael Osthege on June 1, 2022 ([pymc-examples#343](https://github.com/pymc-devs/pymc-examples/pull/343))\n",
    "* Updated by Christopher Krapu on June 17, 2022 ([pymc-examples#378](https://github.com/pymc-devs/pymc-examples/pull/378))\n",
    "* Updated for compatibility with PyMC v5 by Beryl Kanali and Sangam Swadik on Jan 22, 2023 ([pymc-examples#517](https://github.com/pymc-devs/pymc-examples/pull/517))\n",
    "* Updated by [Benjamin T. Vincent](https://github.com/drbenvincent) to use `az.extract` ([pymc-examples#522](https://github.com/pymc-devs/pymc-examples/pull/522))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{include} ../page_footer.md\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "087adf61916f8b9a562e3919cc7201fe0599d07c87f54bc57443476208d67f09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
