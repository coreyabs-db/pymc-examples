{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generalized Extreme Value Distribution\n",
    "\n",
    ":::{post} Sept 27, 2022\n",
    ":tags: extreme, inference, posterior predictive\n",
    ":category: beginner \n",
    ":author: Colin Caprani \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The Generalized Extreme Value (GEV) distribution is a meta-distribution containing the Weibull, Gumbel, and Frechet families of extreme value distributions. It is used for modelling the distribution of extremes (maxima or minima) of stationary processes, such as the annual maximum wind speed, annual maximum truck weight on a bridge, and so on, without needing *a priori* decision on the tail behaviour.\n",
    "\n",
    "Following the parametrization used in {cite:t}`coles2001gev`, the GEV distribution for maxima is given by:\n",
    "\n",
    "$$G(x) = \\exp \\left\\{ \\left[ 1 - \\xi \\left( \\frac{x - \\mu}{\\sigma} \\right) \\right]^{-\\frac{1}{\\xi}} \\right\\}$$\n",
    "\n",
    "when:\n",
    "- $\\xi < 0$ we get the Weibull distribution with a bounded upper tail;\n",
    "- $\\xi = 0$, in the limit, we get the Gumbel distribution, unbonded in both tails;\n",
    "- $\\xi > 0$, we get the Frechet distribution which is bounded in the lower tail.\n",
    "\n",
    "Note that this parametrization of the shape parameter $\\xi$ is opposite in sign to that used in SciPy (where it is denoted `c`). Further, the distribution for minima is readily examined by studying the distribution of maxima of the negative of the data.\n",
    "\n",
    "We will use the example of the Port Pirie annual maximum sea-level data used in {cite:t}`coles2001gev`, and compare with the frequentist results presented there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pymc_experimental.distributions as pmx\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "from arviz.plots import plot_utils as azpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The Port Pirie data is provided by {cite:t}`coles2001gev`, and repeated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "data = np.array([4.03, 3.83, 3.65, 3.88, 4.01, 4.08, 4.18, 3.80, \n",
    "                 4.36, 3.96, 3.98, 4.69, 3.85, 3.96, 3.85, 3.93, \n",
    "                 3.75, 3.63, 3.57, 4.25, 3.97, 4.05, 4.24, 4.22, \n",
    "                 3.73, 4.37, 4.06, 3.71, 3.96, 4.06, 4.55, 3.79, \n",
    "                 3.89, 4.11, 3.85, 3.86, 3.86, 4.21, 4.01, 4.11, \n",
    "                 4.24, 3.96, 4.21, 3.74, 3.85, 3.88, 3.66, 4.11, \n",
    "                 3.71, 4.18, 3.90, 3.78, 3.91, 3.72, 4.00, 3.66, \n",
    "                 3.62, 4.33, 4.55, 3.75, 4.08, 3.90, 3.88, 3.94, \n",
    "                 4.33])\n",
    "# fmt: on\n",
    "plt.hist(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling & Prediction\n",
    "In the modelling we wish to do two thing:\n",
    "\n",
    "- parameter inference on the GEV parameters, based on some fairly non-informative priors, and;\n",
    "- prediction of the 10-year return level.\n",
    "\n",
    "Predictions of extreme values considering parameter uncertainty are easily accomplished in the Bayesian setting. It is interesting to compare this ease to the difficulties encountered by {cite:t}`caprani2010gev` in doing this in a frequentist setting. In any case, the predicted value at a probability of exceedance $p$ is given by:\n",
    "\n",
    "$$ x_p = \\mu - \\frac{\\sigma}{\\xi} \\left\\{1 - \\left[-\\log\\left(1-p\\right)\\right] \\right\\} $$\n",
    "\n",
    "This is a deterministic function of the parameter values, and so is accomplished using `pm.Deterministic` within the model context.\n",
    "\n",
    "Consider then, the 10-year return period, for which $p = 1/10$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1 / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now set up the model using priors estimated from a quick review of the historgram above:\n",
    "\n",
    "- $\\mu$: there is no real basis for considering anything other than a `Normal` distribution with a standard deviation limiting negative outcomes;\n",
    "- $\\sigma$: this must be positive, and has a small value, so use `HalfNormal` with a unit standard deviation;\n",
    "- $\\xi$: we are agnostic to the tail behaviour so centre this at zero, but limit to physically reasonable bounds of $\\pm 0.6$, and keep it somewhat tight near zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optionally centre the data, depending on fitting and divergences\n",
    "# cdata = (data - data.mean())/data.std()\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    μ = pm.Normal(\"μ\", mu=3.8, sigma=0.2)\n",
    "    σ = pm.HalfNormal(\"σ\", sigma=0.3)\n",
    "    ξ = pm.TruncatedNormal(\"ξ\", mu=0, sigma=0.2, lower=-0.6, upper=0.6)\n",
    "\n",
    "    # Estimation\n",
    "    gev = pmx.GenExtreme(\"gev\", mu=μ, sigma=σ, xi=ξ, observed=data)\n",
    "    # Return level\n",
    "    z_p = pm.Deterministic(\"z_p\", μ - σ / ξ * (1 - (-np.log(1 - p)) ** (-ξ)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Predictive Checks\n",
    "Let's get a feel for how well our selected priors cover the range of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = pm.sample_prior_predictive(samples=1000, model=model)\n",
    "az.plot_ppc(idata, group=\"prior\", figsize=(12, 6))\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([2, 6])\n",
    "ax.set_ylim([0, 2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can look at the sampled values of the parameters, using the `plot_posterior` function, but passing in the `idata` object and specifying the `group` to be `\"prior\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(\n",
    "    idata, group=\"prior\", var_names=[\"μ\", \"σ\", \"ξ\"], hdi_prob=\"hide\", point_estimate=None\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Press the magic Inference Button$^{\\mathrm{TM}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(\n",
    "        5000,\n",
    "        cores=4,\n",
    "        chains=4,\n",
    "        tune=2000,\n",
    "        initvals={\"μ\": -0.5, \"σ\": 1.0, \"ξ\": -0.1},\n",
    "        target_accept=0.98,\n",
    "    )\n",
    "# add trace to existing idata object\n",
    "idata.extend(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata, var_names=[\"μ\", \"σ\", \"ξ\"], figsize=(12, 12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divergences\n",
    "The trace exhibits divergences (usually). The HMC/NUTS sampler can have problems when the bounds of support for parameters are approached. And since the bounds of the GEV change with the sign of $\\xi$, it is difficult to offer a transformation that resolves this problem. One possible transformation - the Box-Cox - has been proposed by {cite:t}`bali2003gev`, but {cite:t}`caprani2009gev` find it numerically unstable, even for just maximum likelihood estimation. In any case, recommendations to alleviate divergence problems are:\n",
    "\n",
    "- Increase the target acceptance ratio;\n",
    "- Use more informative priors, especially limit the shape parameter to physically reasonable values, typically $\\xi \\in [-0.5,0.5]$;\n",
    "- Decide upon the domain of attraction of the tail (i.e. Weibull, Gumbel, or Frechet), and use that distribution directly.\n",
    "\n",
    "\n",
    "### Inferences\n",
    "The 95% credible interval range of the parameter estimates is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.hdi(idata, hdi_prob=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And examine the prediction distribution, considering parameter variability (and without needing to assume normality):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(idata, hdi_prob=0.95, var_names=[\"z_p\"], round_to=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's compare the prior and posterior predictions of $z_p$ to see how the data has influenced things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_dist_comparison(idata, var_names=[\"z_p\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "To compare with the results given in {cite:t}`coles2001gev`, we approximate the maximum likelihood estimates (MLE) using the mode of the posterior distributions (the *maximum a posteriori* or MAP estimate). These are close when the prior is reasonably flat around the posterior estimate.\n",
    "\n",
    "The MLE results given in {cite:t}`coles2001gev` are:\n",
    "\n",
    "$$\\left(\\hat{\\mu}, \\hat{\\sigma}, \\hat{\\xi} \\right) = \\left( 3.87, 0.198, -0.050 \\right) $$\n",
    "\n",
    "\n",
    "And the variance-covariance matrix of the estimates is:\n",
    "\n",
    "$$ V = \\left[ \\begin{array} 0.000780 & 0.000197 & -0.00107 \\\\\n",
    "                            0.000197 & 0.000410 &  -0.000778 \\\\\n",
    "                            -0.00107 & -0.000778 & 0.00965 \n",
    "              \\end{array} \\right] $$\n",
    "\n",
    "\n",
    "Note that extracting the MLE estimates from our inference involves accessing some of the Arviz back end functions to bash the xarray into something examinable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, vals = az.sel_utils.xarray_to_ndarray(idata[\"posterior\"], var_names=[\"μ\", \"σ\", \"ξ\"])\n",
    "mle = [azpu.calculate_point_estimate(\"mode\", val) for val in vals]\n",
    "mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata[\"posterior\"].drop_vars(\"z_p\").to_dataframe().cov().round(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are a good match, but the benefit of doing this in a Bayesian setting is we get the full posterior joint distribution of the parameters and the return level, essentially for free. Compare this to the loose normality assumption and computational effort to get even the variance-covarince matrix, as done in {cite:t}`coles2001gev`.\n",
    "\n",
    "Finally, we examine the pairs plots and see where any difficulties in inference lie using the divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_pair(idata, var_names=[\"μ\", \"σ\", \"ξ\"], kind=\"kde\", marginals=True, divergences=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "* Authored by [Colin Caprani](https://github.com/ccaprani), October 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    ":::{bibliography}\n",
    ":filter: docname in docnames\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p pytensor,arviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc4-dev",
   "language": "python",
   "name": "pymc4-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
