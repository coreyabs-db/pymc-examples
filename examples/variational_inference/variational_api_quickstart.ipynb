{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(variational_api_quickstart)=\n",
    "\n",
    "# Introduction to Variational Inference with PyMC\n",
    "\n",
    "The most common strategy for computing posterior quantities of Bayesian models is via sampling,  particularly Markov chain Monte Carlo (MCMC) algorithms. While sampling algorithms and associated computing have continually improved in performance and efficiency, MCMC methods still scale poorly with data size, and become prohibitive for more than a few thousand observations. A more scalable alternative to sampling is variational inference (VI), which re-frames the problem of computing the posterior distribution as an optimization problem. \n",
    "\n",
    "In PyMC, the variational inference API is focused on approximating posterior distributions through a suite of modern algorithms. Common use cases to which this module can be applied include:\n",
    "\n",
    "* Sampling from model posterior and computing arbitrary expressions\n",
    "* Conducting Monte Carlo approximation of expectation, variance, and other statistics\n",
    "* Removing symbolic dependence on PyMC random nodes and evaluate expressions (using `eval`)\n",
    "* Providing a bridge to arbitrary PyTensor code\n",
    "\n",
    ":::{post} Jan 13, 2023 \n",
    ":tags: variational inference\n",
    ":category: intermediate, how-to\n",
    ":author: Maxim Kochurov, Chris Fonnesbeck\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Approximations\n",
    "\n",
    "There are severa methods in statistics that use a simpler distribution to approximate a more complex distribution. Perhaps the best-known example is the **Laplace (normal) approximation**. This involves constructing a Taylor series of the target posterior, but retaining only the terms of quadratic order and using those to construct a multivariate normal approximation.\n",
    "\n",
    "Similarly, variational inference is another distributional approximation method where, rather than leveraging a Taylor series, some class of approximating distribution is chosen and its parameters are optimized such that the resulting distribution is as close as possible to the posterior. In essence, VI is a deterministic approximation that places bounds on the density of interest, then uses opimization to choose from that bounded set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_data = np.random.gamma(2, 0.5, size=200)\n",
    "sns.histplot(gamma_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as gamma_model:\n",
    "    alpha = pm.Exponential(\"alpha\", 0.1)\n",
    "    beta = pm.Exponential(\"beta\", 0.1)\n",
    "\n",
    "    y = pm.Gamma(\"y\", alpha, beta, observed=gamma_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gamma_model:\n",
    "    # mean_field = pm.fit()\n",
    "    mean_field = pm.fit(obj_optimizer=pm.adagrad_window(learning_rate=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gamma_model:\n",
    "    trace = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_field.hist);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_sample = mean_field.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(trace.posterior[\"alpha\"].values.flatten(), label=\"NUTS\")\n",
    "sns.kdeplot(approx_sample.posterior[\"alpha\"].values.flatten(), label=\"ADVI\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup\n",
    "\n",
    "We do not need complex models to play with the VI API; let's begin with a simple mixture model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0.2, 0.8])\n",
    "mu = np.array([-0.3, 0.5])\n",
    "sd = np.array([0.1, 0.1])\n",
    "\n",
    "with pm.Model() as model:\n",
    "    x = pm.NormalMixture(\"x\", w=w, mu=mu, sigma=sd)\n",
    "    x2 = x**2\n",
    "    sin_x = pm.math.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't compute analytical expectations for this model. However, we can obtain an approximation using Markov chain Monte Carlo methods; let's use NUTS first. \n",
    "\n",
    "To allow samples of the expressions to be saved, we need to wrap them in `Deterministic` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    pm.Deterministic(\"x2\", x2)\n",
    "    pm.Deterministic(\"sin_x\", sin_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are traces for $x^2$ and $sin(x)$. We can see there is clear multi-modality in this model. One drawback, is that you need to know in advance what exactly you want to see in trace and wrap it with `Deterministic`.\n",
    "\n",
    "The VI API takes an alternate approach: You obtain inference from model, then calculate expressions based on this model afterwards. \n",
    "\n",
    "Let's use the same model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    x = pm.NormalMixture(\"x\", w=w, mu=mu, sigma=sd)\n",
    "    x2 = x**2\n",
    "    sin_x = pm.math.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use automatic differentiation variational inference (ADVI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    mean_field = pm.fit(method=\"advi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(mean_field.sample(1000), color=\"LightSeaGreen\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that ADVI has failed to approximate the multimodal distribution, since it uses a Gaussian distribution that has a single mode.\n",
    "\n",
    "## Checking convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the default arguments for `CheckParametersConvergence` as they seem to be reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc.variational.callbacks import CheckParametersConvergence\n",
    "\n",
    "with model:\n",
    "    mean_field = pm.fit(method=\"advi\", callbacks=[CheckParametersConvergence()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access inference history via `.hist` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_field.hist);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a good convergence plot, despite the fact that we ran many iterations. The reason is that the mean of the ADVI approximation is close to zero, and therefore taking the relative difference (the default method) is unstable for checking convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    mean_field = pm.fit(\n",
    "        method=\"advi\", callbacks=[pm.callbacks.CheckParametersConvergence(diff=\"absolute\")]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_field.hist);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better! We've reached convergence after less than 5000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful callback allows users to track parameters. It allows for the tracking of arbitrary statistics during inference, though it can be memory-hungry. Using the `fit` function, we do not have direct access to the approximation before inference. However, tracking parameters requires access to the approximation. We can get around this constraint by using the object-oriented (OO) API for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    advi = pm.ADVI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advi.approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different approximations have different hyperparameters. In mean-field ADVI, we have $\\rho$ and $\\mu$ (inspired by [Bayes by BackProp](https://arxiv.org/abs/1505.05424))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advi.approx.shared_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are convenient shortcuts to relevant statistics associated with the approximation. This can be useful, for example, when specifying a mass matrix for NUTS sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advi.approx.mean.eval(), advi.approx.std.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can roll these statistics into the `Tracker` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = pm.callbacks.Tracker(\n",
    "    mean=advi.approx.mean.eval,  # callable that returns mean\n",
    "    std=advi.approx.std.eval,  # callable that returns std\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calling `advi.fit` will record the mean and standard deviation of the approximation as it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx = advi.fit(20000, callbacks=[tracker])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot both the evidence lower bound and parameter traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9))\n",
    "mu_ax = fig.add_subplot(221)\n",
    "std_ax = fig.add_subplot(222)\n",
    "hist_ax = fig.add_subplot(212)\n",
    "mu_ax.plot(tracker[\"mean\"])\n",
    "mu_ax.set_title(\"Mean track\")\n",
    "std_ax.plot(tracker[\"std\"])\n",
    "std_ax.set_title(\"Std track\")\n",
    "hist_ax.plot(advi.hist)\n",
    "hist_ax.set_title(\"Negative ELBO track\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are convergence issues with the mean, and that lack of convergence does not seem to change the ELBO trajectory significantly. As we are using the OO API, we can run the approximation longer until convergence is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advi.refine(100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9))\n",
    "mu_ax = fig.add_subplot(221)\n",
    "std_ax = fig.add_subplot(222)\n",
    "hist_ax = fig.add_subplot(212)\n",
    "mu_ax.plot(tracker[\"mean\"])\n",
    "mu_ax.set_title(\"Mean track\")\n",
    "std_ax.plot(tracker[\"std\"])\n",
    "std_ax.set_title(\"Std track\")\n",
    "hist_ax.plot(advi.hist)\n",
    "hist_ax.set_title(\"Negative ELBO track\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still see evidence for lack of convergence, as the mean has devolved into a random walk. This could be the result of choosing a poor algorithm for inference. At any rate, it is unstable and can produce very different results even using different random seeds.\n",
    "\n",
    "Let's compare results with the NUTS output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(trace.posterior[\"x\"].values.flatten(), label=\"NUTS\")\n",
    "sns.kdeplot(approx.sample(20000).posterior[\"x\"].values.flatten(), label=\"ADVI\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that ADVI is not able to cope with multimodality; we can instead use SVGD, which generates an approximation based on a large number of particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    svgd_approx = pm.fit(\n",
    "        300,\n",
    "        method=\"svgd\",\n",
    "        inf_kwargs=dict(n_particles=1000),\n",
    "        obj_optimizer=pm.sgd(learning_rate=0.01),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(trace.posterior[\"x\"].values.flatten(), label=\"NUTS\")\n",
    "sns.kdeplot(approx.sample(10000).posterior[\"x\"].values.flatten(), label=\"ADVI\")\n",
    "sns.kdeplot(svgd_approx.sample(2000).posterior[\"x\"].values.flatten(), label=\"SVGD\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did the trick, as we now have a multimodal approximation using SVGD. \n",
    "\n",
    "With this, it is possible to calculate arbitrary functions of the parameters with this variational approximation. For example we can calculate $x^2$ and $sin(x)$, as with the NUTS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall x ~ NormalMixture\n",
    "a = x**2\n",
    "b = pm.math.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate these expressions with the approximation, we need `approx.sample_node`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sample = svgd_approx.sample_node(a)\n",
    "a_sample.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sample.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sample.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every call yields a different value from the same node. This is because it is **stochastic**. \n",
    "\n",
    "By applying replacements, we are now free of the dependence on the PyMC model; instead, we now depend on the approximation. Changing it will change the distribution for stochastic nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(np.array([a_sample.eval() for _ in range(2000)]))\n",
    "plt.title(\"$x^2$ distribution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a more convenient way to get lots of samples at once: `sample_node`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_samples = svgd_approx.sample_node(a, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(a_samples.eval())\n",
    "plt.title(\"$x^2$ distribution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sample_node` function includes an additional dimension, so taking expectations or calculating variance is specified by `axis=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_samples.var(0).eval()  # variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_samples.mean(0).eval()  # mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A symbolic sample size can also be specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytensor.tensor as pt\n",
    "\n",
    "i = pt.iscalar(\"i\")\n",
    "i.tag.test_value = 1\n",
    "a_samples_i = svgd_approx.sample_node(a, size=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_samples_i.eval({i: 100}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_samples_i.eval({i: 10000}).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the size must be a scalar value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel logistic regression\n",
    "\n",
    "Let's illustrate the use of `Tracker` with the famous Iris dataset. We'll attempy multi-label classification and compute the expected accuracy score as a diagnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://5047-presscdn.pagely.netdna-cdn.com/wp-content/uploads/2015/04/iris_petal_sepal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A relatively simple model will be sufficient here because the classes are roughly linearly separable; we are going to fit multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = pytensor.shared(X_train)\n",
    "yt = pytensor.shared(y_train)\n",
    "\n",
    "with pm.Model() as iris_model:\n",
    "    # Coefficients for features\n",
    "    β = pm.Normal(\"β\", 0, sigma=1e2, shape=(4, 3))\n",
    "    # Transoform to unit interval\n",
    "    a = pm.Normal(\"a\", sigma=1e4, shape=(3,))\n",
    "    p = pt.special.softmax(Xt.dot(β) + a, axis=-1)\n",
    "\n",
    "    observed = pm.Categorical(\"obs\", p=p, observed=yt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying replacements in practice\n",
    "PyMC models have symbolic inputs for latent variables. To evaluate an expression that requires knowledge of latent variables, one needs to provide fixed values. We can use values approximated by VI for this purpose. The function `sample_node` removes the symbolic dependencies. \n",
    "\n",
    "`sample_node` will use the whole distribution at each step, so we will use it here. We can apply more replacements in single function call using the `more_replacements` keyword argument in both replacement functions.\n",
    "\n",
    "> **HINT:** You can use `more_replacements` argument when calling `fit` too:\n",
    ">   * `pm.fit(more_replacements={full_data: minibatch_data})`\n",
    ">   * `inference.fit(more_replacements={full_data: minibatch_data})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with iris_model:\n",
    "    # We'll use SVGD\n",
    "    inference = pm.SVGD(n_particles=500, jitter=1)\n",
    "\n",
    "    # Local reference to approximation\n",
    "    approx = inference.approx\n",
    "\n",
    "    # Here we need `more_replacements` to change train_set to test_set\n",
    "    test_probs = approx.sample_node(p, more_replacements={Xt: X_test}, size=100)\n",
    "\n",
    "    # For train set no more replacements needed\n",
    "    train_probs = approx.sample_node(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying the code above, we now have 100 sampled probabilities (default number for `sample_node` is `None`) for each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create symbolic expressions for sampled accuracy scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ok = pt.eq(test_probs.argmax(-1), y_test)\n",
    "train_ok = pt.eq(train_probs.argmax(-1), y_train)\n",
    "test_accuracy = test_ok.mean(-1)\n",
    "train_accuracy = train_ok.mean(-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracker expects callables so we can pass `.eval` method of PyTensor node that is function itself. \n",
    "\n",
    "Calls to this function are cached so they can be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tracker = pm.callbacks.Tracker(\n",
    "    test_accuracy=test_accuracy.eval, train_accuracy=train_accuracy.eval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.fit(100, callbacks=[eval_tracker]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1)\n",
    "df = pd.DataFrame(eval_tracker[\"test_accuracy\"]).T.melt()\n",
    "sns.lineplot(x=\"variable\", y=\"value\", data=df, color=\"red\", ax=ax)\n",
    "ax.plot(eval_tracker[\"train_accuracy\"], color=\"blue\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "plt.legend([\"test_accuracy\", \"train_accuracy\"])\n",
    "plt.title(\"Training Progress\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training does not seem to be working here. Let's use a different optimizer and boost the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.fit(400, obj_optimizer=pm.adamax(learning_rate=0.1), callbacks=[eval_tracker]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1)\n",
    "df = pd.DataFrame(np.asarray(eval_tracker[\"test_accuracy\"])).T.melt()\n",
    "sns.lineplot(x=\"variable\", y=\"value\", data=df, color=\"red\", ax=ax)\n",
    "ax.plot(eval_tracker[\"train_accuracy\"], color=\"blue\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "plt.legend([\"test_accuracy\", \"train_accuracy\"])\n",
    "plt.title(\"Training Progress\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better! \n",
    "\n",
    "So, `Tracker` allows us to monitor our approximation and choose good training schedule."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatches\n",
    "When dealing with large datasets, using minibatch training can drastically speed up and improve approximation performance. Large datasets impose a hefty cost on the computation of gradients. \n",
    "\n",
    "There is a nice API in PyMC to handle these cases, which is available through the `pm.Minibatch` class. The minibatch is just a highly specialized PyTensor tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate, let's simulate a large quantity of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw values\n",
    "data = np.random.rand(40000, 100)\n",
    "# Scaled values\n",
    "data *= np.random.randint(1, 10, size=(100,))\n",
    "# Shifted values\n",
    "data += np.random.rand(100) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's fit a model without minibatch processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Flat(\"mu\", shape=(100,))\n",
    "    sd = pm.HalfNormal(\"sd\", shape=(100,))\n",
    "    lik = pm.Normal(\"lik\", mu, sigma=sd, observed=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's create a custom special purpose callback to halt slow optimization. Here we define a callback that causes a hard stop when approximation runs too slowly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_after_10(approx, loss_history, i):\n",
    "    if (i > 0) and (i % 10) == 0:\n",
    "        raise StopIteration(\"I was slow, sorry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    advifit = pm.fit(callbacks=[stop_after_10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference is too slow, taking several seconds per iteration; fitting the approximation would have taken hours!\n",
    "\n",
    "Now let's use minibatches. At every iteration, we will draw 500 random values:\n",
    "\n",
    "> Remember to set `total_size` in observed\n",
    "\n",
    "**total_size** is an important parameter that allows PyMC to infer the right way of rescaling densities. If it is not set, you are likely to get completely wrong results. For more information please refer to the comprehensive documentation of `pm.Minibatch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pm.Minibatch(data, batch_size=500)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    mu = pm.Normal(\"mu\", 0, sigma=1e5, shape=(100,))\n",
    "    sd = pm.HalfNormal(\"sd\", shape=(100,))\n",
    "    likelihood = pm.Normal(\"likelihood\", mu, sigma=sd, observed=X, total_size=data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    advifit = pm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(advifit.hist);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch inference is dramatically faster. Multidimensional minibatches may be needed for some corner cases where you do matrix factorization or model is very wide.\n",
    "\n",
    "Here is the docstring for `Minibatch` to illustrate how it can be customized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pm.Minibatch.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "* Authored by Maxim Kochurov\n",
    "* Updated by Chris Fonnesbeck ([pymc-examples#429](https://github.com/pymc-devs/pymc-examples/pull/497))\n",
    "\n",
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{include} ../page_footer.md\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
