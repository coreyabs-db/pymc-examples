{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b22c91b-aa50-4807-a2c4-5d4ec91b7747",
   "metadata": {
    "tags": []
   },
   "source": [
    "(copula-estimation)=\n",
    "# Bayesian copula estimation: Describing correlated joint distributions\n",
    "\n",
    ":::{post} December 2023\n",
    ":tags: copula, parameter estimation\n",
    ":category: intermediate \n",
    ":author: Eric Ma, Benjamin T. Vincent\n",
    ":::\n",
    "\n",
    "## The problem\n",
    "When we deal with multiple variables (e.g. $a$ and $b$) we often want to describe the joint distribution $P(a, b)$ parametrically. If we are lucky, then this joint distribution might be 'simple' in some way. For example, it could be that $a$ and $b$ are statistically independent, in which case we can break down the joint distribution into $P(a, b) = P(a) P(b)$ and so we just need to find appropriate parametric descriptions for $P(a)$ and $P(b)$. Even if this is not appropriate, it may be that $P(a, b)$ could be described well by a simple multivariate distribution, such as a multivariate normal distribution for example.\n",
    "\n",
    "However, very often when we deal with real datasets, there is complex correlational structure in $P(a, b)$ meaning that these two previous approaches are not available to us. So alternative methods are required.\n",
    "\n",
    "## Copulas to the rescue\n",
    "This is where [copulas](https://en.wikipedia.org/wiki/Copula_(probability_theory)) come in. These allow you do describe a complex distribution $P(a, b)$ with correlational structure by a simple multivariate distribution (such as a Multivariate Gaussian), two marginal distributions, and some transformations. For a very accessible introduction to copulas, we recommend reading through [this](https://twiecki.io/blog/2018/05/03/copulas/) blog post by Thomas Wiecki.\n",
    "\n",
    "This notebook covers how we can describe a distribution $P(a, b)$ with correlational structure using Bayesian methods to infer the parameters of the copula. The general approach we will take is shown in the schematic below.\n",
    "- At the bottom, we have our **observation space** where the data lives.\n",
    "- We can assume that this data was generated through the process from top to bottom - we have a multivariate normal distribution in **multivariate normal space** which is transformed in two stages to result in our data in observation space. \n",
    "- We have access to data in **observation space** but we can make inferences about the parameters in **multivariate normal space** by transforming from one to the other.\n",
    "\n",
    ":::{figure-md} copula-fig-target\n",
    "\n",
    "<img src=\"copula_schematic.png\" alt=\"copula schematic\" width=\"100%\">\n",
    "\n",
    "Schematic of a 2D Gaussian copula. Our complex distribution P(a, b) in observation space (bottom) is modelled as being generated by a 2D Gaussian copula in multivariate normal space (top). Mapping from multivariate normal space to observation space (downwards) is done by the normal cumulative density function and then the inverse cumulative density function of the marginal distributions. The reverse, inference, process (upwards) can be done through the cumulative density function of the marginal distributions followed by an inverse cumulative density function of the normal distribution.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a604dba9",
   "metadata": {},
   "source": [
    "This notebook will describe how to make inferences about copulas based on bivariate data with rich correlational structure. We at [PyMC Labs](https://www.pymc-labs.com) completed this work as part of a larger project with the [Gates Foundation](https://www.gatesfoundation.org), some of which has also been outlined {ref}`here <binning>`.\n",
    "\n",
    "![](gates_labs_logos.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ef48d-cbb5-4a30-ad76-c64251acdbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import expon, multivariate_normal, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3308f4f-a8ef-4b28-a0ed-0bc93a41c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "plt.rcParams.update({\"font.size\": 14, \"figure.constrained_layout.use\": False})\n",
    "SEED = 43\n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19818c84-f770-4847-a76d-dffd92376b51",
   "metadata": {},
   "source": [
    "## Data generating process\n",
    "Before diving in to inference, we will spend some time fleshing out the steps in the schematic figure above. First, we will demonstrate the generative model by describing a multivariate normal copula and transform that into observation space. Second, we show how the inverse transformations can allow use to move back from observation space to multivariate normal space. Once we have these details pinned down, we proceed to the inference process in a later section.\n",
    "\n",
    "Now we will define the properties of our Gaussian copula with a nested dictionary. At the top level we have keys `a` and `b` and `rho`. \n",
    "\n",
    "- `rho` describes the correlation coefficient of the multivariate normal copula. \n",
    "- `a` and `b` are also dictionaries, each of which contains the marginal distribution (as a scipy distribution object) and their parameters.\n",
    "\n",
    "Note that we implicitly define the multivariate normal to have zero mean and unit variance. This is because these moments do not survive the transformation through 'uniform space', the second step in our copula schematic above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e665ed-71ec-449a-bba6-25dc7cc2cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define properties of our copula\n",
    "b_scale = 2\n",
    "θ = {\"a_dist\": norm(), \"b_dist\": expon(scale=1 / b_scale), \"rho\": 0.9}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee7e969-01c6-4ba0-aff9-b31ac1d0568a",
   "metadata": {},
   "source": [
    "First, we define the true multivariate normal and draw some samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f68f3-5cb4-411b-ac2b-57733acb06e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5000\n",
    "\n",
    "# draw random samples in multivariate normal space\n",
    "mu = [0, 0]\n",
    "cov = [[1, θ[\"rho\"]], [θ[\"rho\"], 1]]\n",
    "x = multivariate_normal(mu, cov).rvs(n_samples, random_state=rng)\n",
    "a_norm = x[:, 0]\n",
    "b_norm = x[:, 1]\n",
    "\n",
    "sns.jointplot(x=a_norm, y=b_norm, height=6, kind=\"hex\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e201b3-4579-47b1-a555-b263b845f4ae",
   "metadata": {},
   "source": [
    "Our first transformation (normal cdf) transforms data from multivariate normal space into uniform space. Note how the marginal distributions are uniform, but the correlational structure from the multivariate normal space remains in the interesting joint density below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b4ab3-7b97-4fcb-8fbd-61e0e2a6f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make marginals uniform\n",
    "a_unif = norm(loc=0, scale=1).cdf(a_norm)\n",
    "b_unif = norm(loc=0, scale=1).cdf(b_norm)\n",
    "sns.jointplot(x=a_unif, y=b_unif, height=6, kind=\"hex\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6672e6-bc20-4a12-aeed-2e97662338d9",
   "metadata": {},
   "source": [
    "Our final transformation (the inverse CDF of the marginal distributions) gives rise to $a$ and $b$ in observation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7efe6d-d9ae-497c-820d-b2641418cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to observation space\n",
    "a = θ[\"a_dist\"].ppf(a_unif)\n",
    "b = θ[\"b_dist\"].ppf(b_unif)\n",
    "sns.jointplot(x=a, y=b, height=6, kind=\"hex\", xlim=(-4, 4), ylim=(0, 3));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52243f9-ddc0-4596-bc13-96bf2d28b438",
   "metadata": {},
   "source": [
    "## Copula inference process\n",
    "To understand the approach taken, we will walk through the inverse process, going from observation space to multivariate normal space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54d238-c742-461e-b5c9-ce5b0510418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation -> uniform space\n",
    "a1 = θ[\"a_dist\"].cdf(a)\n",
    "b1 = θ[\"b_dist\"].cdf(b)\n",
    "sns.jointplot(x=a1, y=b1, kind=\"hex\", height=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae6173-5510-43d0-8629-a69af1f4211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform -> MvNormal space\n",
    "a2 = norm(loc=0, scale=1).ppf(a1)\n",
    "b2 = norm(loc=0, scale=1).ppf(b1)\n",
    "sns.jointplot(x=a2, y=b2, kind=\"hex\", height=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57462efe-3ba9-4fcc-a2a2-b9aa4fdb51aa",
   "metadata": {},
   "source": [
    "So now we have worked through what we outlined in Figure 1. We have stepped through in detail the data generating process going from multivariate normal to observation space. We then saw how to do the inverse (inference) process going from observation space to multivariate normal space. This is the approach we use in the PyMC model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d656356e-dc47-4b40-8411-ca5fa44d72c8",
   "metadata": {},
   "source": [
    "## PyMC models for copula and marginal estimation\n",
    "\n",
    "We will conduct inferences about parameters in multivariate normal space, constraining plausible parameter values by the data in observation space. However, we also use our observations of $a$ and $b$ to constrain estimates of the parameters of the marginal distributions.\n",
    "\n",
    "In our experimentation, we explored models which conducted simultaneous estimation of the parameters of the marginals, and the covariance parameter of the copula, but we found this unstable. The solution we used below was found to be more robust, and involves a 2-step process. \n",
    "\n",
    "1. Estimate the parameters of the marginal distributions. \n",
    "2. Estimate the covariance parameter of the copula, using point estimates of the marginal distribution parameters from step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84eb18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\"obs_id\": np.arange(len(a))}\n",
    "with pm.Model(coords=coords) as marginal_model:\n",
    "    \"\"\"\n",
    "    Assumes observed data in variables `a` and `b`\n",
    "    \"\"\"\n",
    "    # marginal estimation\n",
    "    a_mu = pm.Normal(\"a_mu\", mu=0, sigma=1)\n",
    "    a_sigma = pm.Exponential(\"a_sigma\", lam=0.5)\n",
    "    pm.Normal(\"a\", mu=a_mu, sigma=a_sigma, observed=a, dims=\"obs_id\")\n",
    "\n",
    "    b_scale = pm.Exponential(\"b_scale\", lam=0.5)\n",
    "    pm.Exponential(\"b\", lam=1 / b_scale, observed=b, dims=\"obs_id\")\n",
    "\n",
    "pm.model_graph.model_to_graphviz(marginal_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "with marginal_model:\n",
    "    marginal_idata = pm.sample(random_seed=rng)\n",
    "\n",
    "az.plot_posterior(\n",
    "    marginal_idata, var_names=[\"a_mu\", \"a_sigma\", \"b_scale\"], ref_val=[0, 1.0, 1 / 2.0]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447d904",
   "metadata": {},
   "source": [
    "In the copula model below you can see that we set up a prior over the covariance parameter. The posterior distribution over this parameter is constrained by the data in multivariate normal space. But in order to do that we need to transform the observations `[a, b]` in observation space, to multivariate normal space, which we store in `data`.\n",
    "\n",
    "On using point estimates: as you'll see in the code below we have opted to use point estimates from Step 1 rather than the full posterior from Step 1. This is a simplification that we opted for due to complexities in tensor shape handling when passing in posterior distributions as parameters to a distribution.\n",
    "\n",
    "During notebook review, however, [@OriolAbril](https://github.com/OriolAbril) (one of the maintainers of the PyMC Examples repository) correctly pointed out that exponentiating the logcdf of a data point evaluated under a distribution using point estimates _will not necessarily_ return an value equal to the expectation of exponentiating the logcdf of a data point evaluated under many possible distributions (constructed from a full posterior). To ensure timely progress on the notebook, we have opted to show the code as-is, but also leave this note for both our future selves to update the notebook later while also providing an opportunity for future readers to contribute through modifying the example to address this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(marginal_idata):\n",
    "    # point estimates\n",
    "    a_mu = marginal_idata.posterior[\"a_mu\"].mean().item()\n",
    "    a_sigma = marginal_idata.posterior[\"a_sigma\"].mean().item()\n",
    "    b_scale = marginal_idata.posterior[\"b_scale\"].mean().item()\n",
    "    # transformations from observation space -> uniform space\n",
    "    __a = pt.exp(pm.logcdf(pm.Normal.dist(mu=a_mu, sigma=a_sigma), a))\n",
    "    __b = pt.exp(pm.logcdf(pm.Exponential.dist(lam=1 / b_scale), b))\n",
    "    # uniform space -> multivariate normal space\n",
    "    _a = pm.math.probit(__a)\n",
    "    _b = pm.math.probit(__b)\n",
    "    # join into an Nx2 matrix\n",
    "    data = pt.math.stack([_a, _b], axis=1).eval()\n",
    "    return data, a_mu, a_sigma, b_scale\n",
    "\n",
    "\n",
    "data, a_mu, a_sigma, b_scale = transform_data(marginal_idata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbda1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords.update({\"param\": [\"a\", \"b\"], \"param_bis\": [\"a\", \"b\"]})\n",
    "with pm.Model(coords=coords) as copula_model:\n",
    "    # Prior on covariance of the multivariate normal\n",
    "    chol, corr, stds = pm.LKJCholeskyCov(\n",
    "        \"chol\",\n",
    "        n=2,\n",
    "        eta=2.0,\n",
    "        sd_dist=pm.Exponential.dist(1.0),\n",
    "        compute_corr=True,\n",
    "    )\n",
    "    cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=(\"param\", \"param_bis\"))\n",
    "\n",
    "    # Likelihood function\n",
    "    pm.MvNormal(\"N\", mu=0.0, cov=cov, observed=data, dims=(\"obs_id\", \"param\"))\n",
    "\n",
    "pm.model_graph.model_to_graphviz(copula_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7138cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with copula_model:\n",
    "    copula_idata = pm.sample(random_seed=rng, tune=2000, cores=1)\n",
    "\n",
    "az.plot_posterior(copula_idata, var_names=[\"cov\"], ref_val=[1.0, θ[\"rho\"], θ[\"rho\"], 1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0289074",
   "metadata": {},
   "source": [
    "You can see that we have successfully recovered the covariance matrix of the multivariate normal copula which was used to generate the sample data.\n",
    "\n",
    "In the section below, we will use this information in order to sample from our parametric description of $P(a, b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ed0db-a6f9-4f29-bf21-6d584bf0320c",
   "metadata": {},
   "source": [
    "## Comparing inferences against the true data\n",
    "Finally, we can do a visual check to see whether our inferences (red) match up with our original observed data (black)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e5388-43b1-4857-9efd-5322487cfa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data munging to extract covariance estimates from copula_idata in useful shape\n",
    "d = {k: v.values.reshape((-1, *v.shape[2:])) for k, v in copula_idata.posterior[[\"cov\"]].items()}\n",
    "\n",
    "# generate (a, b) samples\n",
    "ab = np.vstack([multivariate_normal([0, 0], cov).rvs() for cov in d[\"cov\"]])\n",
    "\n",
    "# transform to uniform space\n",
    "uniform_a = norm().cdf(ab[:, 0])\n",
    "uniform_b = norm().cdf(ab[:, 1])\n",
    "\n",
    "# transform to observation space\n",
    "# estimated marginal parameters a_mu, a_sigma, b_scale are point estimates from marginal estimation.\n",
    "ppc_a = norm(loc=a_mu, scale=a_sigma).ppf(uniform_a)\n",
    "ppc_b = expon(scale=b_scale).ppf(uniform_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508da5e-8871-4600-b590-285efaa4f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data in black\n",
    "ax = az.plot_pair(\n",
    "    {\"a\": a, \"b\": b},\n",
    "    marginals=True,\n",
    "    # kind=[\"kde\", \"scatter\"],\n",
    "    kind=\"kde\",\n",
    "    scatter_kwargs={\"alpha\": 0.1},\n",
    "    kde_kwargs=dict(\n",
    "        contour_kwargs=dict(colors=\"k\", linestyles=\"--\"), contourf_kwargs=dict(alpha=0)\n",
    "    ),\n",
    "    marginal_kwargs=dict(color=\"k\", plot_kwargs=dict(ls=\"--\")),\n",
    ")\n",
    "\n",
    "# plot inferences in red\n",
    "axs = az.plot_pair(\n",
    "    {\"a\": ppc_a, \"b\": ppc_b},\n",
    "    marginals=True,\n",
    "    # kind=[\"kde\", \"scatter\"],\n",
    "    kind=\"kde\",\n",
    "    scatter_kwargs={\"alpha\": 0.01},\n",
    "    kde_kwargs=dict(contour_kwargs=dict(colors=\"r\", linestyles=\"-\"), contourf_kwargs=dict(alpha=0)),\n",
    "    marginal_kwargs=dict(color=\"r\"),\n",
    "    ax=ax,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcb648b-09b2-4cc4-8d3b-978a219fb784",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "We would like to acknowledge [Jonathan Sedar](https://github.com/jonsedar), [Junpeng Lao](https://github.com/junpenglao), and [Oriol Abril](https://github.com/OriolAbril) for useful advice during the development of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e2a54",
   "metadata": {},
   "source": [
    "## Authors\n",
    "* Authored by [Eric Ma](https://www.pymc-labs.com/team) & [Benjamin T. Vincent](https://github.com/drbenvincent) in November, 2023 ([pymc-examples#257](https://github.com/pymc-devs/pymc-examples/pull/257))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aec5c13",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25d6b6-f0a1-4ca3-96f5-9a5a858bb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p pytensor,xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c2750-89d5-4310-8df2-91b1e531afe7",
   "metadata": {},
   "source": [
    ":::{include} ../page_footer.md\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58b944638401d33c3d09208b59b4291a3f95aa998af834645f3a14808299b080"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
