{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(blackbox_external_likelihood_numpy)=\n",
    "# Using a \"black box\" likelihood function\n",
    "\n",
    ":::{post} Dec 16, 2021\n",
    ":tags: PyTensor \n",
    ":category: advanced, how-to\n",
    ":author: Matt Pitkin, Jørgen Midtbø, Oriol Abril, Ricardo Vieira\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "There is a {ref}`related example <wrapping_jax_function>` that discusses how to use a likelihood implemented in JAX\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:35.009919Z",
     "start_time": "2024-03-13T11:56:32.211412Z"
    }
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "from pytensor.graph import Apply, Op\n",
    "from scipy.optimize import approx_fprime\n",
    "\n",
    "print(f\"Running on PyMC v{pm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:35.015994Z",
     "start_time": "2024-03-13T11:56:35.011286Z"
    }
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "PyMC is a great tool for doing Bayesian inference and parameter estimation. It has many {doc}`in-built probability distributions <pymc:api/distributions>` that you can use to set up priors and likelihood functions for your particular model. You can even create your own {class}`Custom Distribution <pymc.CustomDist>` with a custom logp defined by PyTensor operations or automatically inferred from the generative graph.\n",
    "\n",
    "Despite all these \"batteries included\", you may still find yourself dealing with a model function or probability distribution that relies on complex external code that you cannot avoid but to use. This code is unlikely to work with the kind of abstract PyTensor variables that PyMC uses: {ref}`pymc:pymc_pytensor`.\n",
    "\n",
    "```python\n",
    "import pymc as pm\n",
    "from external_module import my_external_func  # your external function!\n",
    "\n",
    "# set up your model\n",
    "with pm.Model():\n",
    "    # your external function takes two parameters, a and b, with Uniform priors\n",
    "    a = pm.Uniform('a', lower=0., upper=1.)\n",
    "    b = pm.Uniform('b', lower=0., upper=1.)\n",
    "    \n",
    "    m = my_external_func(a, b)  # <--- this is not going to work!\n",
    "```\n",
    "\n",
    "Another issue is that if you want to be able to use the gradient-based step samplers like {term}`NUTS` and {term}`Hamiltonian Monte Carlo`, then your model/likelihood needs a gradient to be defined. If you have a model that is defined as a set of PyTensor operators then this is no problem - internally it will be able to do automatic differentiation - but if your model is essentially a \"black box\" then you won't necessarily know what the gradients are.\n",
    "\n",
    "Defining a model/likelihood that PyMC can use and that calls your \"black box\" function is possible, but it relies on creating a custom PyTensor Op. This is, hopefully, a clear description of how to do this, including one way of writing a gradient function that could be generally applicable.\n",
    "\n",
    "In the examples below, we create a very simple linear model and log-likelihood function in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:35.023748Z",
     "start_time": "2024-03-13T11:56:35.017610Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_model(m, c, x):\n",
    "    return m * x + c\n",
    "\n",
    "\n",
    "def my_loglike(m, c, sigma, x, data):\n",
    "    # We fail explicitly if inputs are not numerical types for the sake of this tutorial\n",
    "    # As defined, my_loglike would actually work fine with PyTensor variables!\n",
    "    for param in (m, c, sigma, x, data):\n",
    "        if not isinstance(param, (float, np.ndarray)):\n",
    "            raise TypeError(f\"Invalid input type to loglike: {type(param)}\")\n",
    "    model = my_model(m, c, x)\n",
    "    return -0.5 * ((data - model) / sigma) ** 2 - np.log(np.sqrt(2 * np.pi)) - np.log(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as things are, if we wanted to sample from this log-likelihood function, using certain prior distributions for the model parameters (gradient and y-intercept) using PyMC, we might try something like this (using a {class}`pymc.CustomDist` or {class}`pymc.Potential`):\n",
    "\n",
    "```python\n",
    "import pymc as pm\n",
    "\n",
    "# create/read in our \"data\" (I'll show this in the real example below)\n",
    "x = ...\n",
    "sigma = ...\n",
    "data = ...\n",
    "\n",
    "with pm.Model():\n",
    "    # set priors on model gradient and y-intercept\n",
    "    m = pm.Uniform('m', lower=-10., upper=10.)\n",
    "    c = pm.Uniform('c', lower=-10., upper=10.)\n",
    "\n",
    "    # create custom distribution \n",
    "    pm.CustomDist('likelihood', m, c, sigma, x, observed=data, logp=my_loglike)\n",
    "    \n",
    "    # sample from the distribution\n",
    "    trace = pm.sample(1000)\n",
    "```\n",
    "\n",
    "But, this will likely give an error when the black-box function does not accept PyTensor tensor objects as inputs.\n",
    "\n",
    "So, what we actually need to do is create a {ref}`PyTensor Op <pytensor:creating_an_op>`. This will be a new class that wraps our log-likelihood function while obeying the PyTensor API contract. We will do this below, initially without defining a {func}`grad` for the Op.\n",
    "\n",
    ":::{tip}\n",
    "Depending on your application you may only need to wrap a custom log-likelihood or a subset of the whole model (such as a function that computes an infinite series summation using an advanced library like mpmath), which can then be chained with other PyMC distributions and PyTensor operations to define your whole model. There is a trade-off here, usually the more you leave out of a black-box the more you may benefit from PyTensor rewrites and optimizations. We suggest you always try to define the whole model in PyMC and PyTensor, and only use black-boxes where strictly necessary.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTensor Op without gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Op definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:35.033798Z",
     "start_time": "2024-03-13T11:56:35.026763Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a pytensor Op for our likelihood function\n",
    "\n",
    "\n",
    "class LogLike(Op):\n",
    "    def make_node(self, m, c, sigma, x, data) -> Apply:\n",
    "        # Convert inputs to tensor variables\n",
    "        m = pt.as_tensor(m)\n",
    "        c = pt.as_tensor(c)\n",
    "        sigma = pt.as_tensor(sigma)\n",
    "        x = pt.as_tensor(x)\n",
    "        data = pt.as_tensor(data)\n",
    "\n",
    "        inputs = [m, c, sigma, x, data]\n",
    "        # Define output type, in our case a vector of likelihoods\n",
    "        # with the same dimensions and same data type as data\n",
    "        # If data must always be a vector, we could have hard-coded\n",
    "        # outputs = [pt.vector()]\n",
    "        outputs = [data.type()]\n",
    "\n",
    "        # Apply is an object that combines inputs, outputs and an Op (self)\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node: Apply, inputs: list[np.ndarray], outputs: list[list[None]]) -> None:\n",
    "        # This is the method that compute numerical output\n",
    "        # given numerical inputs. Everything here is numpy arrays\n",
    "        m, c, sigma, x, data = inputs  # this will contain my variables\n",
    "\n",
    "        # call our numpy log-likelihood function\n",
    "        loglike_eval = my_loglike(m, c, sigma, x, data)\n",
    "\n",
    "        # Save the result in the outputs list provided by PyTensor\n",
    "        # There is one list per output, each containing another list\n",
    "        # pre-populated with a `None` where the result should be saved.\n",
    "        outputs[0][0] = np.asarray(loglike_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:35.040091Z",
     "start_time": "2024-03-13T11:56:35.035560Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up our data\n",
    "N = 10  # number of data points\n",
    "sigma = 1.0  # standard deviation of noise\n",
    "x = np.linspace(0.0, 9.0, N)\n",
    "\n",
    "mtrue = 0.4  # true gradient\n",
    "ctrue = 3.0  # true y-intercept\n",
    "\n",
    "truemodel = my_model(mtrue, ctrue, x)\n",
    "\n",
    "# make data\n",
    "rng = np.random.default_rng(716743)\n",
    "data = sigma * rng.normal(size=N) + truemodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some data we initialize the actual Op and try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:35.046598Z",
     "start_time": "2024-03-13T11:56:35.041624Z"
    }
   },
   "outputs": [],
   "source": [
    "# create our Op\n",
    "loglike_op = LogLike()\n",
    "\n",
    "test_out = loglike_op(mtrue, ctrue, sigma, x, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pytensor.dprint` prints a textual representation of a PyTensor graph.\n",
    "We can see our variable is the output of the `LogLike` Op and has a type of `pt.vector(float64, shape=(10,))`.\n",
    "We can also see the five constant inputs and their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:35.056466Z",
     "start_time": "2024-03-13T11:56:35.048024Z"
    }
   },
   "outputs": [],
   "source": [
    "pytensor.dprint(test_out, print_type=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because all inputs are constant, we can use the handy `eval` method to evaluate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:35.104559Z",
     "start_time": "2024-03-13T11:56:35.057492Z"
    }
   },
   "outputs": [],
   "source": [
    "test_out.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm this returns what we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:35.111635Z",
     "start_time": "2024-03-13T11:56:35.106133Z"
    }
   },
   "outputs": [],
   "source": [
    "my_loglike(mtrue, ctrue, sigma, x, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use this Op to repeat the example shown above. To do this let's create some data containing a straight line with additive Gaussian noise (with a mean of zero and a standard deviation of `sigma`). For simplicity we set {class}`~pymc.Uniform` prior distributions on the gradient and y-intercept. As we've not set the `grad()` method of the Op PyMC will not be able to use the gradient-based samplers, so will fall back to using the {class}`pymc.Slice` sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:35.146503Z",
     "start_time": "2024-03-13T11:56:35.115496Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_dist_loglike(data, m, c, sigma, x):\n",
    "    # data, or observed is always passed as the first input of CustomDist\n",
    "    return loglike_op(m, c, sigma, x, data)\n",
    "\n",
    "\n",
    "# use PyMC to sampler from log-likelihood\n",
    "with pm.Model() as no_grad_model:\n",
    "    # uniform priors on m and c\n",
    "    m = pm.Uniform(\"m\", lower=-10.0, upper=10.0, initval=mtrue)\n",
    "    c = pm.Uniform(\"c\", lower=-10.0, upper=10.0, initval=ctrue)\n",
    "\n",
    "    # use a CustomDist with a custom logp function\n",
    "    likelihood = pm.CustomDist(\n",
    "        \"likelihood\", m, c, sigma, x, observed=data, logp=custom_dist_loglike\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we even sample, we can check if the model logp is correct (and no errors are raised). \n",
    "\n",
    "We need a point to evaluate the logp, which we can get with `initial_point` method. \n",
    "This will be the transformed initvals we defined in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:35.215006Z",
     "start_time": "2024-03-13T11:56:35.147864Z"
    }
   },
   "outputs": [],
   "source": [
    "ip = no_grad_model.initial_point()\n",
    "ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sholud get exactly the same values as when we tested manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:35.426478Z",
     "start_time": "2024-03-13T11:56:35.216367Z"
    }
   },
   "outputs": [],
   "source": [
    "no_grad_model.compile_logp(vars=[likelihood], sum=False)(ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also double-check that PyMC will error out if we try to extract the model gradients with respect to the logp (which we call `dlogp`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:35.550890Z",
     "start_time": "2024-03-13T11:56:35.427550Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    no_grad_model.compile_dlogp()\n",
    "except Exception as exc:\n",
    "    print(type(exc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:43.212635Z",
     "start_time": "2024-03-13T11:56:35.551871Z"
    }
   },
   "outputs": [],
   "source": [
    "with no_grad_model:\n",
    "    # Use custom number of draws to replace the HMC based defaults\n",
    "    idata_no_grad = pm.sample(3000, tune=1000)\n",
    "\n",
    "# plot the traces\n",
    "az.plot_trace(idata_no_grad, lines=[(\"m\", {}, mtrue), (\"c\", {}, ctrue)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTensor Op with gradients\n",
    "\n",
    "What if we wanted to use NUTS or HMC? If we knew the analytical derivatives of the model/likelihood function then we could add a {func}`grad` to the Op using existing PyTensor operations.\n",
    "\n",
    "But, what if we don't know the analytical form. If our model/likelihood, is implemented in a framework that provides automatic differentiation (just like PyTensor does), it's possible to reuse their functionality. This {ref}`related example <wrapping_jax_function>` shows how to do this when working with JAX functions.\n",
    "\n",
    "If our model/likelihood truly is a \"black box\" then we can try to use approximation methods like [finite difference](https://en.wikipedia.org/wiki/Finite_difference) to find the gradients. We illustrate this approach with the handy SciPy {func}`~scipy.optimize.approx_fprime` function to achieve this.\n",
    "\n",
    ":::{caution}\n",
    "Finite differences are rarely recommended as a way to compute gradients. They can be too slow or unstable for practical uses. We suggest you use them only as a last resort. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Op definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_differences_loglike(m, c, sigma, x, data, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Calculate the partial derivatives of a function at a set of values. The\n",
    "    derivatives are calculated using scipy approx_fprime function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m, c: array_like\n",
    "        The parameters of the function for which we wish to define partial derivatives\n",
    "    x, data, sigma:\n",
    "        Observed variables as we have been using so far\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad_wrt_m: array_like\n",
    "        Partial derivative wrt to the m parameter\n",
    "    grad_wrt_c: array_like\n",
    "        Partial derivative wrt to the c parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def inner_func(mc, sigma, x, data):\n",
    "        return my_loglike(*mc, sigma, x, data)\n",
    "\n",
    "    grad_wrt_mc = approx_fprime([m, c], inner_func, [eps, eps], sigma, x, data)\n",
    "\n",
    "    return grad_wrt_mc[:, 0], grad_wrt_mc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:43.224985Z",
     "start_time": "2024-03-13T11:56:43.220Z"
    }
   },
   "outputs": [],
   "source": [
    "finite_differences_loglike(mtrue, ctrue, sigma, x, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we can just redefine our Op with a `grad()` method, right?\n",
    "\n",
    "It's not quite so simple! The `grad()` method itself requires that its inputs are PyTensor tensor variables, whereas our `gradients` function above, like our `my_loglike` function, wants a list of floating point values. So, we need to define another Op that calculates the gradients. Below, I define a new version of the `LogLike` Op, called `LogLikeWithGrad` this time, that has a `grad()` method. This is followed by anothor Op called `LogLikeGrad` that, when called with a vector of PyTensor tensor variables, returns another vector of values that are the gradients (i.e., the [Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)) of our log-likelihood function at those values. Note that the `grad()` method itself does not return the gradients directly, but instead returns the [Jacobian-vector product](https://en.wikipedia.org/wiki/Pushforward_(differential))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:43.237603Z",
     "start_time": "2024-03-13T11:56:43.226264Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a pytensor Op for our likelihood function\n",
    "\n",
    "\n",
    "class LogLikeWithGrad(Op):\n",
    "    def make_node(self, m, c, sigma, x, data) -> Apply:\n",
    "        # Same as before\n",
    "        m = pt.as_tensor(m)\n",
    "        c = pt.as_tensor(c)\n",
    "        sigma = pt.as_tensor(sigma)\n",
    "        x = pt.as_tensor(x)\n",
    "        data = pt.as_tensor(data)\n",
    "\n",
    "        inputs = [m, c, sigma, x, data]\n",
    "        outputs = [data.type()]\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node: Apply, inputs: list[np.ndarray], outputs: list[list[None]]) -> None:\n",
    "        # Same as before\n",
    "        m, c, sigma, x, data = inputs  # this will contain my variables\n",
    "        loglike_eval = my_loglike(m, c, sigma, x, data)\n",
    "        outputs[0][0] = np.asarray(loglike_eval)\n",
    "\n",
    "    def grad(\n",
    "        self, inputs: list[pt.TensorVariable], g: list[pt.TensorVariable]\n",
    "    ) -> list[pt.TensorVariable]:\n",
    "        # NEW!\n",
    "        # the method that calculates the gradients - it actually returns the vector-Jacobian product\n",
    "        m, c, sigma, x, data = inputs\n",
    "\n",
    "        # Our gradient expression assumes these are scalar parameters\n",
    "        if m.type.ndim != 0 or c.type.ndim != 0:\n",
    "            raise NotImplementedError(\"Gradient only implemented for scalar m and c\")\n",
    "\n",
    "        grad_wrt_m, grad_wrt_c = loglikegrad_op(m, c, sigma, x, data)\n",
    "\n",
    "        # out_grad is a tensor of gradients of the Op outputs wrt to the function cost\n",
    "        [out_grad] = g\n",
    "        return [\n",
    "            pt.sum(out_grad * grad_wrt_m),\n",
    "            pt.sum(out_grad * grad_wrt_c),\n",
    "            # We did not implement gradients wrt to the last 3 inputs\n",
    "            # This won't be a problem for sampling, as those are constants in our model\n",
    "            pytensor.gradient.grad_not_implemented(self, 2, sigma),\n",
    "            pytensor.gradient.grad_not_implemented(self, 3, x),\n",
    "            pytensor.gradient.grad_not_implemented(self, 4, data),\n",
    "        ]\n",
    "\n",
    "\n",
    "class LogLikeGrad(Op):\n",
    "    def make_node(self, m, c, sigma, x, data) -> Apply:\n",
    "        m = pt.as_tensor(m)\n",
    "        c = pt.as_tensor(c)\n",
    "        sigma = pt.as_tensor(sigma)\n",
    "        x = pt.as_tensor(x)\n",
    "        data = pt.as_tensor(data)\n",
    "\n",
    "        inputs = [m, c, sigma, x, data]\n",
    "        # There are two outputs with the same type as data,\n",
    "        # for the partial derivatives wrt to m, c\n",
    "        outputs = [data.type(), data.type()]\n",
    "\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node: Apply, inputs: list[np.ndarray], outputs: list[list[None]]) -> None:\n",
    "        m, c, sigma, x, data = inputs\n",
    "\n",
    "        # calculate gradients\n",
    "        grad_wrt_m, grad_wrt_c = finite_differences_loglike(m, c, sigma, x, data)\n",
    "\n",
    "        outputs[0][0] = grad_wrt_m\n",
    "        outputs[1][0] = grad_wrt_c\n",
    "\n",
    "\n",
    "# Initalize the Ops\n",
    "loglikewithgrad_op = LogLikeWithGrad()\n",
    "loglikegrad_op = LogLikeGrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should test the gradient is working before we jump back to the model.\n",
    "\n",
    "Instead of evaluating the loglikegrad_op directly, we will use the same PyTensor grad machinery that PyMC will ultimately use, to make sure there are no surprises.\n",
    "\n",
    "For this we will provide symbolic inputs for `m` and `c` (which in the PyMC model will be RandomVariables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:43.243628Z",
     "start_time": "2024-03-13T11:56:43.239208Z"
    }
   },
   "outputs": [],
   "source": [
    "m = pt.scalar(\"m\")\n",
    "c = pt.scalar(\"c\")\n",
    "out = loglikewithgrad_op(m, c, sigma, x, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our loglike Op is also new, let's make sure it's output is still correct. We can still use `eval` but because we have two non-constant inputs we need to provide values for those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:43.255991Z",
     "start_time": "2024-03-13T11:56:43.244799Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_out = out.eval({m: mtrue, c: ctrue})\n",
    "print(eval_out)\n",
    "assert np.allclose(eval_out, my_loglike(mtrue, ctrue, sigma, x, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested you can see how the gradient computational graph looks like, but it's a bit messy. \n",
    "\n",
    "You can see that both `LogLikeWithGrad` and `LogLikeGrad` show up as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:43.267749Z",
     "start_time": "2024-03-13T11:56:43.257371Z"
    }
   },
   "outputs": [],
   "source": [
    "grad_wrt_m, grad_wrt_c = pytensor.grad(out.sum(), wrt=[m, c])\n",
    "pytensor.dprint([grad_wrt_m], print_type=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to confirm we implemented the gradient correctly is to use PyTensor's `verify_grad` utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:43.326943Z",
     "start_time": "2024-03-13T11:56:43.269016Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_fn(m, c):\n",
    "    return loglikewithgrad_op(m, c, sigma, x, data)\n",
    "\n",
    "\n",
    "# This raises an error if the gradient output is not within a given tolerance\n",
    "pytensor.gradient.verify_grad(test_fn, pt=[mtrue, ctrue], rng=np.random.default_rng(123))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's re-run PyMC with our new \"grad\"-ed Op. This time it will be able to automatically use NUTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:43.349754Z",
     "start_time": "2024-03-13T11:56:43.328080Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_dist_loglike(data, m, c, sigma, x):\n",
    "    # data, or observed is always passed as the first input of CustomDist\n",
    "    return loglikewithgrad_op(m, c, sigma, x, data)\n",
    "\n",
    "\n",
    "# use PyMC to sampler from log-likelihood\n",
    "with pm.Model() as grad_model:\n",
    "    # uniform priors on m and c\n",
    "    m = pm.Uniform(\"m\", lower=-10.0, upper=10.0)\n",
    "    c = pm.Uniform(\"c\", lower=-10.0, upper=10.0)\n",
    "\n",
    "    # use a CustomDist with a custom logp function\n",
    "    likelihood = pm.CustomDist(\n",
    "        \"likelihood\", m, c, sigma, x, observed=data, logp=custom_dist_loglike\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loglikelihood should not have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:43.441324Z",
     "start_time": "2024-03-13T11:56:43.351120Z"
    }
   },
   "outputs": [],
   "source": [
    "grad_model.compile_logp(vars=[likelihood], sum=False)(ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this time we should be able to evaluate the dlogp (wrt to m and c) as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:43.874428Z",
     "start_time": "2024-03-13T11:56:43.442562Z"
    }
   },
   "outputs": [],
   "source": [
    "grad_model.compile_dlogp()(ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, accordingly, NUTS will now be selected by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:57.860869Z",
     "start_time": "2024-03-13T11:56:43.875592Z"
    }
   },
   "outputs": [],
   "source": [
    "with grad_model:\n",
    "    # Use custom number of draws to replace the HMC based defaults\n",
    "    idata_grad = pm.sample()\n",
    "\n",
    "# plot the traces\n",
    "az.plot_trace(idata_grad, lines=[(\"m\", {}, mtrue), (\"c\", {}, ctrue)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to equivalent PyMC distributions\n",
    "Finally, just to check things actually worked as we might expect, let's do the same thing purely using PyMC distributions (because in this simple example we can!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:57.890280Z",
     "start_time": "2024-03-13T11:56:57.862637Z"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as pure_model:\n",
    "    # uniform priors on m and c\n",
    "    m = pm.Uniform(\"m\", lower=-10.0, upper=10.0)\n",
    "    c = pm.Uniform(\"c\", lower=-10.0, upper=10.0)\n",
    "\n",
    "    # use a Normal distribution\n",
    "    likelihood = pm.Normal(\"likelihood\", mu=(m * x + c), sigma=sigma, observed=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:58.160497Z",
     "start_time": "2024-03-13T11:56:57.891645Z"
    }
   },
   "outputs": [],
   "source": [
    "pure_model.compile_logp(vars=[likelihood], sum=False)(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:56:58.800813Z",
     "start_time": "2024-03-13T11:56:58.165396Z"
    }
   },
   "outputs": [],
   "source": [
    "pure_model.compile_dlogp()(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:57:09.067126Z",
     "start_time": "2024-03-13T11:56:58.801859Z"
    }
   },
   "outputs": [],
   "source": [
    "with pure_model:\n",
    "    idata_pure = pm.sample()\n",
    "\n",
    "# plot the traces\n",
    "az.plot_trace(idata_pure, lines=[(\"m\", {}, mtrue), (\"c\", {}, ctrue)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that they match let's plot all the examples together and also find the autocorrelation lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:57:10.323876Z",
     "start_time": "2024-03-13T11:57:09.068366Z"
    }
   },
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(3, 2, sharex=True, sharey=True)\n",
    "az.plot_autocorr(idata_no_grad, combined=True, ax=axes[0, :])\n",
    "az.plot_autocorr(idata_grad, combined=True, ax=axes[1, :])\n",
    "az.plot_autocorr(idata_pure, combined=True, ax=axes[2, :])\n",
    "axes[2, 0].set_xlim(right=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:57:11.170160Z",
     "start_time": "2024-03-13T11:57:10.325223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot no grad result (blue)\n",
    "pair_kwargs = dict(\n",
    "    kind=\"kde\",\n",
    "    marginals=True,\n",
    "    reference_values={\"m\": mtrue, \"c\": ctrue},\n",
    "    kde_kwargs={\"contourf_kwargs\": {\"alpha\": 0}, \"contour_kwargs\": {\"colors\": \"C0\"}},\n",
    "    reference_values_kwargs={\"color\": \"k\", \"ms\": 15, \"marker\": \"d\"},\n",
    "    marginal_kwargs={\"color\": \"C0\"},\n",
    ")\n",
    "ax = az.plot_pair(idata_no_grad, **pair_kwargs)\n",
    "\n",
    "# Plot nuts+blackbox fit (orange)\n",
    "pair_kwargs[\"kde_kwargs\"][\"contour_kwargs\"][\"colors\"] = \"C1\"\n",
    "pair_kwargs[\"marginal_kwargs\"][\"color\"] = \"C1\"\n",
    "az.plot_pair(idata_grad, **pair_kwargs, ax=ax)\n",
    "\n",
    "# Plot pure pymc+nuts fit (green)\n",
    "pair_kwargs[\"kde_kwargs\"][\"contour_kwargs\"][\"colors\"] = \"C2\"\n",
    "pair_kwargs[\"marginal_kwargs\"][\"color\"] = \"C2\"\n",
    "az.plot_pair(idata_pure, **pair_kwargs, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Potential instead of CustomDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we show how a {func}`pymc.Potential` can be used to implement a black-box likelihood a bit more directly than when using a {class}`pymc.CustomDist`. \n",
    "\n",
    "The simpler interface comes at the cost of making other parts of the Bayesian workflow more cumbersome. For instance, as {func}`pymc.Potential` cannot be used for model comparison, as PyMC does not know whether a Potential term corresponds to a prior, a likelihood or even a mix of both. Potentials also have no forward sampling counterparts, which are needed for prior and posterior predictive sampling, while {class}`pymc.CustomDist` accepts `random` or `dist` functions for such occasions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:57:26.595854Z",
     "start_time": "2024-03-13T11:57:11.171574Z"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as potential_model:\n",
    "    # uniform priors on m and c\n",
    "    m = pm.Uniform(\"m\", lower=-10.0, upper=10.0)\n",
    "    c = pm.Uniform(\"c\", lower=-10.0, upper=10.0)\n",
    "\n",
    "    # use a Potential instead of a CustomDist\n",
    "    pm.Potential(\"likelihood\", loglikewithgrad_op(m, c, sigma, x, data))\n",
    "\n",
    "    idata_potential = pm.sample()\n",
    "\n",
    "# plot the traces\n",
    "az.plot_trace(idata_potential, lines=[(\"m\", {}, mtrue), (\"c\", {}, ctrue)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Authors\n",
    "\n",
    "* Adapted from [Jørgen Midtbø](https://github.com/jorgenem/)'s [example](https://discourse.pymc.io/t/connecting-pymc-to-external-code-help-with-understanding-pytensor-custom-ops/670) by Matt Pitkin both as a [blogpost](http://mattpitkin.github.io/samplers-demo/pages/pymc-blackbox-likelihood/) and as an example notebook to this gallery in August, 2018 ([pymc#3169](https://github.com/pymc-devs/pymc/pull/3169) and [pymc#3177](https://github.com/pymc-devs/pymc/pull/3177))\n",
    "* Updated by [Oriol Abril](https://github.com/OriolAbril) on December 2021 to drop the Cython dependency from the original notebook and use numpy instead ([pymc-examples#28](https://github.com/pymc-devs/pymc-examples/pull/28))\n",
    "* Re-executed by Oriol Abril with pymc 5.0.0 ([pymc-examples#496](https://github.com/pymc-devs/pymc-examples/pull/496))\n",
    "* Updated by [Ricardo Vieira](https://github.com/ricardoV94/) on February 2024 to show an incremental implementation and debugging strategies, as well as to use CustomDist instead of Potential ([pymc-examples#631](https://github.com/pymc-devs/pymc-examples/pull/631))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:57:26.609484Z",
     "start_time": "2024-03-13T11:57:26.596902Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{include} ../page_footer.md\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc-examples",
   "language": "python",
   "name": "pymc-examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
