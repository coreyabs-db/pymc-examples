{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7522bf72",
   "metadata": {},
   "source": [
    "(updating_priors)=\n",
    "# Updating Priors\n",
    "\n",
    ":::{post} January, 2017\n",
    ":tags: priors\n",
    ":category: intermediate, how-to\n",
    ":author: [David Brochart](https://github.com/davidbrochart)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557bc1b",
   "metadata": {},
   "source": [
    "In this notebook, we will show how, in principle, it is possible to update the priors as new data becomes available.\n",
    "\n",
    "`````{admonition} Words of Caution\n",
    ":class: warning\n",
    "This example provides a very nice usage example for the {class}`~pymc.Interpolated` class, as we will see below. However, this might not be a good idea to do in practice, not only because KDEs are being used to compute pdf values for the posterior, but mostly because Interpolated distributions used as priors are **unidimensional** and **uncorrelated**.  So even if a perfect fit *marginally* they don't really incorporate all the information we have from the previous posterior into the model, especially when posterior variables are correlated. See a nice discussion about the subject in the blog post [Some dimensionality devils](https://oriolabrilpla.cat/en/blog/posts/2022/too-eager-reduction.html#univariate-priors) by [Oriol Abril](https://oriolabrilpla.cat/en/).\n",
    "``````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "from scipy import stats\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "az.style.use(\"arviz-white\")\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303568ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng: np.random.Generator = np.random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c4da6",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec6c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameter values\n",
    "alpha_true = 5\n",
    "beta0_true = 7\n",
    "beta1_true = 13\n",
    "sigma_true = 2\n",
    "\n",
    "# Size of dataset\n",
    "size = 100\n",
    "\n",
    "# Predictor variable\n",
    "X1 = rng.normal(size=size)\n",
    "X2 = rng.normal(size=size) * 0.2\n",
    "\n",
    "# Simulate outcome variable\n",
    "Y = alpha_true + beta0_true * X1 + beta1_true * X2 + rng.normal(size=size, scale=sigma_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e7929",
   "metadata": {},
   "source": [
    "## Model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e811e3",
   "metadata": {},
   "source": [
    "Our initial beliefs about the parameters are quite informative (sigma=1) and a bit off the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Priors for unknown model parameters\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=5)\n",
    "    beta0 = pm.Normal(\"beta0\", mu=0, sigma=5)\n",
    "    beta1 = pm.Normal(\"beta1\", mu=0, sigma=5)\n",
    "\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n",
    "\n",
    "    # Expected value of outcome\n",
    "    mu = alpha + beta0 * X1 + beta1 * X2\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    Y_obs = pm.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=Y)\n",
    "\n",
    "    # draw 2_000 posterior samples\n",
    "    trace = pm.sample(\n",
    "        tune=1_500, draws=2_000, target_accept=0.9, progressbar=False, random_seed=rng\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b90f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_trace(\n",
    "    data=trace,\n",
    "    compact=True,\n",
    "    lines=[\n",
    "        (\"alpha\", {}, alpha_true),\n",
    "        (\"beta0\", {}, beta0_true),\n",
    "        (\"beta1\", {}, beta1_true),\n",
    "        (\"sigma\", {}, sigma_true),\n",
    "    ],\n",
    "    backend_kwargs={\"figsize\": (12, 9), \"layout\": \"constrained\"},\n",
    ")\n",
    "plt.gcf().suptitle(\"Trace\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb212e4a",
   "metadata": {},
   "source": [
    "In order to update our beliefs about the parameters, we use the posterior distributions, which will be used as the prior distributions for the next inference. The data used for each inference iteration has to be independent from the previous iterations, otherwise the same (possibly wrong) belief is injected over and over in the system, amplifying the errors and misleading the inference. By ensuring the data is independent, the system should converge to the true parameter values.\n",
    "\n",
    "Because we draw samples from the posterior distribution (shown on the right in the figure above), we need to estimate their probability density (shown on the left in the figure above). [Kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) is a way to achieve this, and we will use this technique here. In any case, it is an empirical distribution that cannot be expressed analytically. Fortunately PyMC provides a way to use custom distributions, via {class}`~pymc.Interpolated` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5392bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_posterior(param, samples):\n",
    "    smin, smax = samples.min().item(), samples.max().item()\n",
    "    width = smax - smin\n",
    "    x = np.linspace(smin, smax, 100)\n",
    "    y = stats.gaussian_kde(samples)(x)\n",
    "\n",
    "    # what was never sampled should have a small probability but not 0,\n",
    "    # so we'll extend the domain and use linear approximation of density on it\n",
    "    x = np.concatenate([[x[0] - 3 * width], x, [x[-1] + 3 * width]])\n",
    "    y = np.concatenate([[0], y, [0]])\n",
    "    return pm.Interpolated(param, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77750fbc",
   "metadata": {},
   "source": [
    "Now we just need to generate more data and build our Bayesian model so that the prior distributions for the current iteration are the posterior distributions from the previous iteration. It is still possible to continue using NUTS sampling method because `Interpolated` class implements calculation of gradients that are necessary for Hamiltonian Monte Carlo samplers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38eb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [trace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a9124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 10\n",
    "\n",
    "for _ in trange(n_iterations):\n",
    "    # generate more data\n",
    "    X1 = rng.normal(size=size)\n",
    "    X2 = rng.normal(size=size) * 0.2\n",
    "    Y = alpha_true + beta0_true * X1 + beta1_true * X2 + rng.normal(size=size, scale=sigma_true)\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # Priors are posteriors from previous iteration\n",
    "        alpha = from_posterior(\"alpha\", az.extract(trace, group=\"posterior\", var_names=[\"alpha\"]))\n",
    "        beta0 = from_posterior(\"beta0\", az.extract(trace, group=\"posterior\", var_names=[\"beta0\"]))\n",
    "        beta1 = from_posterior(\"beta1\", az.extract(trace, group=\"posterior\", var_names=[\"beta1\"]))\n",
    "        sigma = from_posterior(\"sigma\", az.extract(trace, group=\"posterior\", var_names=[\"sigma\"]))\n",
    "\n",
    "        # Expected value of outcome\n",
    "        mu = alpha + beta0 * X1 + beta1 * X2\n",
    "\n",
    "        # Likelihood (sampling distribution) of observations\n",
    "        Y_obs = pm.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=Y)\n",
    "\n",
    "        # draw 2_000 posterior samples\n",
    "        trace = pm.sample(\n",
    "            tune=1_500, draws=2_000, target_accept=0.9, progressbar=False, random_seed=rng\n",
    "        )\n",
    "        traces.append(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67961ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, ncols=1, figsize=(12, 12), sharex=False, sharey=False)\n",
    "\n",
    "cmap = mpl.cm.viridis\n",
    "\n",
    "for i, (param, true_value) in enumerate(\n",
    "    zip([\"alpha\", \"beta0\", \"beta1\", \"sigma\"], [alpha_true, beta0_true, beta1_true, sigma_true])\n",
    "):\n",
    "    for update_i, trace in enumerate(traces):\n",
    "        samples = az.extract(trace, group=\"posterior\", var_names=param)\n",
    "        smin, smax = np.min(samples), np.max(samples)\n",
    "        x = np.linspace(smin, smax, 100)\n",
    "        y = stats.gaussian_kde(samples)(x)\n",
    "        ax[i].plot(x, y, color=cmap(1 - update_i / len(traces)))\n",
    "        ax[i].axvline(true_value, c=\"k\")\n",
    "        ax[i].set(title=param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1babff4c",
   "metadata": {},
   "source": [
    "You can re-execute the last two cells to generate more updates.\n",
    "\n",
    "What is interesting to note is that the posterior distributions for our parameters tend to get centered on their true value (vertical lines), and the distribution gets thiner and thiner. This means that we get more confident each time, and the (false) belief we had at the beginning gets flushed away by the new data we incorporate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b7bdc",
   "metadata": {},
   "source": [
    "``````{admonition} Not a silver bullet\n",
    ":class: warning\n",
    "Observe that, despite the fact that the iterations seems improving, some of them don't look so good, even sometimes it seems it regresses. In addition to reasons noted at the beginning of the notebook, there are a couple key steps in the process where randomness is involved. Thus, things should be expected to improve on average.\n",
    "\n",
    "1. New observations are random. If in the initial iterations we get values closer to the bulk of the distribuion and then we get several values in a row from the positive tail, the iterations where we have accumulated a couple draws from the tail will probably be biased and \"look worse\" than previous ones.\n",
    "2. MCMC is random. Even when it converges, MCMC is a random process, so different calls to `pymc.sample` will return values centered around the exact posterior but not always the same; how large a variation we should expect can be checked with {func}`arviz.mcse`. KDEs also incorporate this often negligible yet present source of uncertainty in the posterior estimates, and so will the generated Interpolated distributions.\n",
    "\n",
    "+++\n",
    "\n",
    "``````{admonition} An alternative approach\n",
    ":class: tip\n",
    "There is an alternative way in `pymc-experimental` trough the function {func}`~pymc_experimental.utils.prior.prior_from_idata` that does something similar. This function:\n",
    "> Creates a prior from posterior using MvNormal approximation.\n",
    "> The approximation uses MvNormal distribution. Keep in mind that this function will only work well for unimodal\n",
    "> posteriors and will fail when complicated interactions happen. Moreover, if a retrieved variable is constrained, you\n",
    "> should specify a transform for the variable, e.g.\n",
    "> {func}`~pymc.distributions.transforms.log` for standard deviation posterior.\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca92cc3",
   "metadata": {},
   "source": [
    "## Authors\n",
    "- Created by [David Brochart](https://github.com/davidbrochart) ([pymc#1878](https://github.com/pymc-devs/pymc/pull/1878)) on May 2017.\n",
    "- Updated by [Juan Orduz](https://github.com/juanitorduz) on August 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd07409",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c68185",
   "metadata": {},
   "source": [
    ":::{include} ../page_footer.md\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
