{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241ec99a-6825-4d61-b90b-5d255a9b1764",
   "metadata": {},
   "source": [
    "(marginalizing-models)=\n",
    "# Automatic marginalization of discrete variables\n",
    "\n",
    ":::{post} Jan 20, 2024\n",
    ":tags: mixture model\n",
    ":category: intermediate, how-to\n",
    ":author: Rob Zinkov\n",
    ":::\n",
    "\n",
    "PyMC is very amendable to sampling models with discrete latent variables. But if you insist on using the NUTS sampler exclusively, you will need to get rid of your discrete variables somehow. The best way to do this is by marginalizing them out, as then you benefit from Rao-Blackwell's theorem and get a lower variance estimate of your parameters.\n",
    "\n",
    "Formally the argument goes like this, samplers can be understood as approximating the expectation $\\mathbb{E}_{p(x, z)}[f(x, z)]$ for some function $f$ with respect to a distribution $p(x, z)$. By [law of total expectation](https://en.wikipedia.org/wiki/Law_of_total_expectation) we know that\n",
    "\n",
    "$$ \\mathbb{E}_{p(x, z)}[f(x, z)] =  \\mathbb{E}_{p(z)}\\left[\\mathbb{E}_{p(x \\mid z)}\\left[f(x, z)\\right]\\right] $$\n",
    "\n",
    "Letting $g(z) = \\mathbb{E}_{p(x \\mid z)}\\left[f(x, z)\\right]$, we know by [law of total variance](https://en.wikipedia.org/wiki/Law_of_total_variance) that\n",
    "\n",
    "$$ \\mathbb{V}_{p(x, z)}[f(x, z)] = \\mathbb{V}_{p(z)}[g(z)] + \\mathbb{E}_{p(z)}\\left[\\mathbb{V}_{p(x \\mid z)}\\left[f(x, z)\\right]\\right] $$\n",
    "\n",
    "Because the expectation is over a variance it must always be positive, and thus we know\n",
    "\n",
    "$$ \\mathbb{V}_{p(x, z)}[f(x, z)] \\geq \\mathbb{V}_{p(z)}[g(z)] $$\n",
    "\n",
    "Intuitively, marginalizing variables in your model lets you use $g$ instead of $f$. This lower variance manifests most directly in lower Monte-Carlo standard error (mcse), and indirectly in a generally higher effective sample size (ESS).\n",
    "\n",
    "Unfortunately, the computation to do this is often tedious and unintuitive. Luckily, `pymc-experimental` now supports a way to do this work automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e8a9d-7516-4ad2-af1e-09fb85f77639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495efc5b-a0c0-45f0-a723-3278495e1ace",
   "metadata": {},
   "source": [
    ":::{include} ../extra_installs.md\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d802429-a250-4c22-9ecd-0dcb6778d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc_experimental as pmx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686f41b-d55c-417d-8ef4-772c421a47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'  # high resolution figures\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "rng = np.random.default_rng(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f646c49f-41af-4004-a2c4-63d6ead8e007",
   "metadata": {},
   "source": [
    "As a motivating example, consider a gaussian mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c7fb7-3339-4e82-abe2-1d0aebf85242",
   "metadata": {},
   "source": [
    "## Gaussian Mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eecdf9b-4527-45fe-84d5-8a776086cb0c",
   "metadata": {},
   "source": [
    "There are two ways to specify the same model. One where the choice of mixture is explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b84e4-1323-4508-93e6-1f00fe21f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = pt.as_tensor([-2.0, 2.0])\n",
    "\n",
    "with pmx.MarginalModel() as explicit_mixture:\n",
    "    idx = pm.Bernoulli(\"idx\", 0.7)\n",
    "    y = pm.Normal(\"y\", mu=mu[idx], sigma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c63f01-8a34-4ef1-a316-384c721a3966",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pm.draw(y, draws=2000, random_seed=rng), bins=30, rwidth=0.9);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1b1cab-56ce-4ddd-95d3-6454c8d0aae0",
   "metadata": {},
   "source": [
    "The other way is where we use the built-in {class}`NormalMixture <pymc.NormalMixture>` distribution. Here the mixture assignment is not an explicit variable in our model. There is nothing unique about the first model other than we initialize it with {class}`pmx.MarginalModel <pymc_experimental.MarginalModel>` instead of {class}`pm.Model <pymc.model.core.Model>`. This different class is what will allow us to marginalize out variables later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27852bef-f23b-4151-bc41-1af26f934e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as prebuilt_mixture:\n",
    "    y = pm.NormalMixture(\"y\", w=[0.3, 0.7], mu=[-2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318f820-9a2c-4b7d-bdfd-34cb1a9eecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pm.draw(y, draws=2000, random_seed=rng), bins=30, rwidth=0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b907c-8146-4694-a821-7a2ebacbcab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with prebuilt_mixture:\n",
    "    idata = pm.sample(draws=2000, chains=4, random_seed=rng)\n",
    "\n",
    "az.summary(idata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9a596-af22-4074-bc96-85a91cd35a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with explicit_mixture:\n",
    "    idata = pm.sample(draws=2000, chains=4, random_seed=rng)\n",
    "\n",
    "az.summary(idata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b1591-ff13-4dde-880a-aee4572a0b19",
   "metadata": {},
   "source": [
    "We can immediately see that the marginalized model has a higher ESS. Let's now marginalize out the choice and see what it changes in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a84902-73af-4485-a40b-72200411a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "explicit_mixture.marginalize([\"idx\"])\n",
    "with explicit_mixture:\n",
    "    idata = pm.sample(draws=2000, chains=4, random_seed=rng)\n",
    "\n",
    "az.summary(idata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034eb4d-83f9-4543-992f-0f68bf47fb68",
   "metadata": {},
   "source": [
    "As we can see, the `idx` variable is gone now. We also were able to use the NUTS sampler, and the ESS has improved.\n",
    "\n",
    "But {class}`MarginalModel <pymc_experimental.MarginalModel>` has a distinct advantage. It still knows about the discrete variables that were marginalized out, and we can obtain estimates for the posterior of `idx` given the other variables. We do this using the {meth}`recover_marginals <pymc_experimental.MarginalModel.recover_marginals>` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4457a-0af5-4ba8-89c9-e2c8267f0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "explicit_mixture.recover_marginals(idata, random_seed=rng);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f23bf-c871-4b81-bbf7-14f411604eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(idata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d687f4b-ef2a-4512-8e81-0c0b1bc0d0bc",
   "metadata": {},
   "source": [
    "This `idx` variable lets us recover the mixture assignment variable after running the NUTS sampler! We can split out the samples of `y` by reading off the mixture label from the associated `idx` for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d23a58-3ebd-4b67-80f5-42dd495dfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "post = idata.posterior\n",
    "plt.hist(\n",
    "    post.where(post.idx == 0).y.values.reshape(-1),\n",
    "    bins=30,\n",
    "    rwidth=0.9,\n",
    "    alpha=0.75,\n",
    "    label='idx = 0',\n",
    ")\n",
    "plt.hist(\n",
    "    post.where(post.idx == 1).y.values.reshape(-1),\n",
    "    bins=30,\n",
    "    rwidth=0.9,\n",
    "    alpha=0.75,\n",
    "    label='idx = 1'\n",
    ")\n",
    "# fmt: on\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe000d6-9e6a-4ae7-9cae-3d0eed952410",
   "metadata": {},
   "source": [
    "One important thing to notice is that this discrete variable has a lower ESS, and particularly so for the tail. This means `idx` might not be estimated well particularly for the tails. If this is important, I recommend using the `lp_idx` instead, which is the log-probability of `idx` given sample values on each iteration. The benefits of working with `lp_idx` will explored further in the next example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b458c9e-3b2d-4ba3-a657-5d7db1c046c5",
   "metadata": {},
   "source": [
    "## Coal mining model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd6e73-6d3b-4ee0-9bff-eb0a581399af",
   "metadata": {},
   "source": [
    "The same methods work for the {ref}`Coal mining <pymc:pymc_overview#case-study-2-coal-mining-disasters>` switchpoint model as well. The coal mining dataset records the number of coal mining disasters in the UK between 1851 and 1962. The time series dataset captures a time when mining safety regulations are being introduced, we try to estimate when this occurred using a discrete `switchpoint` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086c01b-5da7-4744-96ba-8d0b52e088c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "disaster_data = pd.Series(\n",
    "    [4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n",
    "    3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n",
    "    2, 2, 3, 4, 2, 1, 3, np.nan, 2, 1, 1, 1, 1, 3, 0, 0,\n",
    "    1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n",
    "    0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n",
    "    3, 3, 1, np.nan, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n",
    "    0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]\n",
    ")\n",
    "\n",
    "# fmt: on\n",
    "years = np.arange(1851, 1962)\n",
    "\n",
    "with pmx.MarginalModel() as disaster_model:\n",
    "    switchpoint = pm.DiscreteUniform(\"switchpoint\", lower=years.min(), upper=years.max())\n",
    "    early_rate = pm.Exponential(\"early_rate\", 1.0, initval=3)\n",
    "    late_rate = pm.Exponential(\"late_rate\", 1.0, initval=1)\n",
    "    rate = pm.math.switch(switchpoint >= years, early_rate, late_rate)\n",
    "    disasters = pm.Poisson(\"disasters\", rate, observed=disaster_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d95bc6-ac70-427f-9bf4-c5b42cdf09fe",
   "metadata": {},
   "source": [
    "We will sample the model both before and after we marginalize out the `switchpoint` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b71716-a585-49b8-b31a-43e54211a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "with disaster_model:\n",
    "    before_marg = pm.sample(chains=2, random_seed=rng)\n",
    "\n",
    "disaster_model.marginalize([\"switchpoint\"])\n",
    "\n",
    "with disaster_model:\n",
    "    after_marg = pm.sample(chains=2, random_seed=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b108e7-c49a-40f1-afd7-c3890587a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(before_marg, var_names=[\"~disasters\"], filter_vars=\"like\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb62001a-3b80-4923-96e2-064d411ff523",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(after_marg, var_names=[\"~disasters\"], filter_vars=\"like\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66532abc-38a6-4796-ab4d-9252159663fc",
   "metadata": {},
   "source": [
    "As before, the ESS improved massively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e058dba7-9b6b-4002-8360-2fae6fe71306",
   "metadata": {},
   "source": [
    "Finally, let us recover the `switchpoint` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19459eaa-a781-4baf-8360-77dad3c15217",
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_model.recover_marginals(after_marg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306de49-12b1-44f1-90e7-2d2320567afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(after_marg, var_names=[\"~disasters\", \"~lp\"], filter_vars=\"like\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc7e742-67b4-4152-8ec5-4bd8c4f7c640",
   "metadata": {},
   "source": [
    "While `recover_marginals` is able to sample the discrete variables that were marginalized out. The probabilities associated with each draw often offer a cleaner estimate of the discrete variable. Particularly for lower probability values. This is best illustrated by comparing the histogram of the sampled values with the plot of the log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d9cbf-5eda-4625-8c9b-84995318bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "post = after_marg.posterior.switchpoint.values.reshape(-1)\n",
    "bins = np.arange(post.min(), post.max())\n",
    "plt.hist(post, bins, rwidth=0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3338722f-a0c6-4277-b458-8ff8dcb59434",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_switchpoint = after_marg.posterior.lp_switchpoint.mean(dim=[\"chain\", \"draw\"])\n",
    "x_max = years[lp_switchpoint.argmax()]\n",
    "\n",
    "plt.scatter(years, lp_switchpoint)\n",
    "plt.axvline(x=x_max, c=\"orange\")\n",
    "plt.xlabel(r\"$\\mathrm{year}$\")\n",
    "plt.ylabel(r\"$\\log p(\\mathrm{switchpoint}=\\mathrm{year})$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cc13c-f2e7-4789-aac2-3e3e9dfe58cc",
   "metadata": {},
   "source": [
    "By plotting a histogram of sampled values instead of working with the log-probabilities directly, we are left with noisier and more incomplete exploration of the underlying discrete distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c675ae7f-2c91-4ead-90c2-ab0bd78a02ed",
   "metadata": {},
   "source": [
    "## Authors\n",
    "* Authored by [Rob Zinkov](https://zinkov.com) in January, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073a737-5f30-44bc-ac6c-bc85b8955391",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    ":::{bibliography}\n",
    ":filter: docname in docnames \n",
    ":::\n",
    "\n",
    "* [STAN manual section on marginalization](https://mc-stan.org/docs/stan-users-guide/latent-discrete.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14213a-651e-4271-9a2d-71954e84605c",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd6d30-cfd8-4fc4-85df-1f4361ed7015",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p pytensor,xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47987baa-2f8d-4efd-9c43-12f76e2659e2",
   "metadata": {},
   "source": [
    ":::{include} ../page_footer.md\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc-dev",
   "language": "python",
   "name": "pymc-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "myst": {
   "substitutions": {
    "extra_dependencies": "pymc-experimental"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
