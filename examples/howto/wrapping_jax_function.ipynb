{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(wrapping_jax_function)=\n",
    "# How to wrap a JAX function for use in PyMC\n",
    "\n",
    ":::{post} Mar 24, 2022\n",
    ":tags: PyTensor, hidden markov model, JAX \n",
    ":category: advanced, how-to\n",
    ":author: Ricardo Vieira\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "from pytensor.graph import Apply, Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 104109109\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{include} ../extra_installs.md\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "import pymc.sampling_jax\n",
    "\n",
    "from pytensor.link.jax.dispatch import jax_funcify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro: PyTensor and its backends\n",
    "\n",
    "PyMC uses the {doc}`PyTensor <pytensor:index>` library to create and manipulate probabilistic graphs. PyTensor is backend-agnostic, meaning it can make use of functions written in different languages or frameworks, including pure Python, NumPy, C, Cython, Numba, and [JAX](https://jax.readthedocs.io/en/latest/index.html). \n",
    "\n",
    "All that is needed is to encapsulate such function in a PyTensor {class}`~pytensor.graph.op.Op`, which enforces a specific API regarding how inputs and outputs of pure \"operations\" should be handled. It also implements methods for optional extra functionality like symbolic shape inference and automatic differentiation. This is well covered in the PyTensor {ref}`Op documentation <pytensor:op_contract>` and in our {ref}`blackbox_external_likelihood_numpy` pymc-example.\n",
    "\n",
    "More recently, PyTensor became capable of compiling directly to some of these languages/frameworks, meaning that we can convert a complete PyTensor graph into a JAX or NUMBA jitted function, whereas traditionally they could only be converted to Python or C.\n",
    "\n",
    "This has some interesting uses, such as sampling models defined in PyMC with pure JAX samplers, like those implemented in [NumPyro](https://num.pyro.ai/en/latest/index.html) or [BlackJax](https://github.com/blackjax-devs/blackjax). \n",
    "\n",
    "This notebook illustrates how we can implement a new PyTensor {class}`~pytensor.graph.op.Op` that wraps a JAX function. \n",
    "\n",
    "### Outline\n",
    "\n",
    "1. We start in a similar path as that taken in the {ref}`blackbox_external_likelihood_numpy`, which wraps a NumPy function in a PyTensor {class}`~pytensor.graph.op.Op`, this time wrapping a JAX jitted function instead. \n",
    "2. We then enable PyTensor to \"unwrap\" the just wrapped JAX function, so that the whole graph can be compiled to JAX. We make use of this to sample our PyMC model via the JAX NumPyro NUTS sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A motivating example: marginal HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration purposes, we will simulate data following a simple [Hidden Markov Model](https://en.wikipedia.org/wiki/Hidden_Markov_model) (HMM), with 3 possible latent states $S \\in \\{0, 1, 2\\}$ and normal emission likelihood.\n",
    "\n",
    "$$Y \\sim \\text{Normal}((S + 1) \\cdot \\text{signal}, \\text{noise})$$\n",
    "\n",
    "Our HMM will have a fixed Categorical probability $P$ of switching across states, which depends only on the last state\n",
    "\n",
    "$$S_{t+1} \\sim \\text{Categorical}(P_{S_t})$$\n",
    "\n",
    "To complete our model, we assume a fixed probability $P_{t0}$ for each possible initial state $S_{t0}$,\n",
    "\n",
    "$$S_{t0} \\sim \\text{Categorical}(P_{t0})$$\n",
    "\n",
    "\n",
    "### Simulating data\n",
    "Let's generate data according to this model! The first step is to set some values for the parameters in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emission signal and noise parameters\n",
    "emission_signal_true = 1.15\n",
    "emission_noise_true = 0.15\n",
    "\n",
    "p_initial_state_true = np.array([0.9, 0.09, 0.01])\n",
    "\n",
    "# Probability of switching from state_t to state_t+1\n",
    "p_transition_true = np.array(\n",
    "    [\n",
    "        #    0,   1,   2\n",
    "        [0.9, 0.09, 0.01],  # 0\n",
    "        [0.1, 0.8, 0.1],  # 1\n",
    "        [0.2, 0.1, 0.7],  # 2\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Confirm that we have defined valid probabilities\n",
    "assert np.isclose(np.sum(p_initial_state_true), 1)\n",
    "assert np.allclose(np.sum(p_transition_true, axis=-1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the log of the probalitiy transition matrix for later use\n",
    "with np.errstate(divide=\"ignore\"):\n",
    "    logp_initial_state_true = np.log(p_initial_state_true)\n",
    "    logp_transition_true = np.log(p_transition_true)\n",
    "\n",
    "logp_initial_state_true, logp_transition_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will observe 70 HMM processes, each with a total of 50 steps\n",
    "n_obs = 70\n",
    "n_steps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a helper function to generate a single HMM process and create our simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_hmm(p_initial_state, p_transition, emission_signal, emission_noise, n_steps, rng):\n",
    "    \"\"\"Generate hidden state and emission from our HMM model.\"\"\"\n",
    "\n",
    "    possible_states = np.array([0, 1, 2])\n",
    "\n",
    "    hidden_states = []\n",
    "    initial_state = rng.choice(possible_states, p=p_initial_state)\n",
    "    hidden_states.append(initial_state)\n",
    "    for step in range(n_steps):\n",
    "        new_hidden_state = rng.choice(possible_states, p=p_transition[hidden_states[-1]])\n",
    "        hidden_states.append(new_hidden_state)\n",
    "    hidden_states = np.array(hidden_states)\n",
    "\n",
    "    emissions = rng.normal(\n",
    "        (hidden_states + 1) * emission_signal,\n",
    "        emission_noise,\n",
    "    )\n",
    "\n",
    "    return hidden_states, emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_hmm_hidden_state, single_hmm_emission = simulate_hmm(\n",
    "    p_initial_state_true,\n",
    "    p_transition_true,\n",
    "    emission_signal_true,\n",
    "    emission_noise_true,\n",
    "    n_steps,\n",
    "    rng,\n",
    ")\n",
    "print(single_hmm_hidden_state)\n",
    "print(np.round(single_hmm_emission, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state_true = []\n",
    "emission_observed = []\n",
    "\n",
    "for i in range(n_obs):\n",
    "    hidden_state, emission = simulate_hmm(\n",
    "        p_initial_state_true,\n",
    "        p_transition_true,\n",
    "        emission_signal_true,\n",
    "        emission_noise_true,\n",
    "        n_steps,\n",
    "        rng,\n",
    "    )\n",
    "    hidden_state_true.append(hidden_state)\n",
    "    emission_observed.append(emission)\n",
    "\n",
    "hidden_state = np.array(hidden_state_true)\n",
    "emission_observed = np.array(emission_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n",
    "# Plot first five hmm processes\n",
    "for i in range(4):\n",
    "    ax[0].plot(hidden_state_true[i] + i * 0.02, color=f\"C{i}\", lw=2, alpha=0.4)\n",
    "    ax[1].plot(emission_observed[i], color=f\"C{i}\", lw=2, alpha=0.4)\n",
    "ax[0].set_yticks([0, 1, 2])\n",
    "ax[0].set_ylabel(\"hidden state\")\n",
    "ax[1].set_ylabel(\"observed emmission\")\n",
    "ax[1].set_xlabel(\"step\")\n",
    "fig.suptitle(\"Simulated data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the hidden state and respective observed emission of our simulated data. Later, we will use this data to perform posterior inferences about the true model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the marginal HMM likelihood using JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a JAX function to compute the likelihood of our HMM model, marginalizing over the hidden states. This allows for more efficient sampling of the remaining model parameters. To achieve this, we will use the well known [forward algorithm](https://en.wikipedia.org/wiki/Forward_algorithm), working on the log scale for numerical stability.\n",
    "\n",
    "We will take advantage of JAX [scan](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html) to obtain an efficient and differentiable log-likelihood, and the handy [vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap) to automatically vectorize this log-likelihood across multiple observed processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our core JAX function computes the marginal log-likelihood of a single HMM process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_logp(\n",
    "    emission_observed,\n",
    "    emission_signal,\n",
    "    emission_noise,\n",
    "    logp_initial_state,\n",
    "    logp_transition,\n",
    "):\n",
    "    \"\"\"Compute the marginal log-likelihood of a single HMM process.\"\"\"\n",
    "\n",
    "    hidden_states = np.array([0, 1, 2])\n",
    "\n",
    "    # Compute log-likelihood of observed emissions for each (step x possible hidden state)\n",
    "    logp_emission = jsp.stats.norm.logpdf(\n",
    "        emission_observed[:, None],\n",
    "        (hidden_states + 1) * emission_signal,\n",
    "        emission_noise,\n",
    "    )\n",
    "\n",
    "    # We use the forward_algorithm to compute log_alpha(x_t) = logp(x_t, y_1:t)\n",
    "    log_alpha = logp_initial_state + logp_emission[0]\n",
    "    log_alpha, _ = jax.lax.scan(\n",
    "        f=lambda log_alpha_prev, logp_emission: (\n",
    "            jsp.special.logsumexp(log_alpha_prev + logp_transition.T, axis=-1) + logp_emission,\n",
    "            None,\n",
    "        ),\n",
    "        init=log_alpha,\n",
    "        xs=logp_emission[1:],\n",
    "    )\n",
    "\n",
    "    return jsp.special.logsumexp(log_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with the true parameters and the first simulated HMM process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_logp(\n",
    "    emission_observed[0],\n",
    "    emission_signal_true,\n",
    "    emission_noise_true,\n",
    "    logp_initial_state_true,\n",
    "    logp_transition_true,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use vmap to vectorize the core function across multiple observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_hmm_logp(*args):\n",
    "    vmap = jax.vmap(\n",
    "        hmm_logp,\n",
    "        # Only the first argument, needs to be vectorized\n",
    "        in_axes=(0, None, None, None, None),\n",
    "    )\n",
    "    # For simplicity we sum across all the HMM processes\n",
    "    return jnp.sum(vmap(*args))\n",
    "\n",
    "\n",
    "# We jit it for better performance!\n",
    "jitted_vec_hmm_logp = jax.jit(vec_hmm_logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a row matrix with only the first simulated HMM process should return the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_vec_hmm_logp(\n",
    "    emission_observed[0][None, :],\n",
    "    emission_signal_true,\n",
    "    emission_noise_true,\n",
    "    logp_initial_state_true,\n",
    "    logp_transition_true,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is, however, to compute the joint log-likelihood for all the simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "jitted_vec_hmm_logp(\n",
    "    emission_observed,\n",
    "    emission_signal_true,\n",
    "    emission_noise_true,\n",
    "    logp_initial_state_true,\n",
    "    logp_transition_true,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also ask JAX to give us the function of the gradients with respect to each input. This will come in handy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_vec_hmm_logp_grad = jax.jit(jax.grad(vec_hmm_logp, argnums=list(range(5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the gradient with respect to `emission_signal`. We will check this value is unchanged after we wrap our function in PyTensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_vec_hmm_logp_grad(\n",
    "    emission_observed,\n",
    "    emission_signal_true,\n",
    "    emission_noise_true,\n",
    "    logp_initial_state_true,\n",
    "    logp_transition_true,\n",
    ")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping the JAX function in PyTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to wrap our JAX jitted function in a PyTensor {class}`~pytensor.graph.op.Op`, that we can then use in our PyMC models. We recommend you check PyTensor's official {ref}`Op documentation <pytensor:op_contract>` if you want to understand it in more detail.\n",
    "\n",
    "In brief, we will inherit from {class}`~pytensor.graph.op.Op` and define the following methods:\n",
    "1. `make_node`: Creates an {class}`~pytensor.graph.basic.Apply` node that holds together the symbolic inputs and outputs of our operation\n",
    "2. `perform`: Python code that returns the evaluation of our operation, given concrete input values\n",
    "3. `grad`: Returns a PyTensor symbolic graph that represents the gradient expression of an output cost wrt to its inputs\n",
    "\n",
    "For the `grad` we will create a second {class}`~pytensor.graph.op.Op` that wraps our jitted grad version from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMMLogpOp(Op):\n",
    "    def make_node(\n",
    "        self,\n",
    "        emission_observed,\n",
    "        emission_signal,\n",
    "        emission_noise,\n",
    "        logp_initial_state,\n",
    "        logp_transition,\n",
    "    ):\n",
    "        # Convert our inputs to symbolic variables\n",
    "        inputs = [\n",
    "            pt.as_tensor_variable(emission_observed),\n",
    "            pt.as_tensor_variable(emission_signal),\n",
    "            pt.as_tensor_variable(emission_noise),\n",
    "            pt.as_tensor_variable(logp_initial_state),\n",
    "            pt.as_tensor_variable(logp_transition),\n",
    "        ]\n",
    "        # Define the type of the output returned by the wrapped JAX function\n",
    "        outputs = [pt.dscalar()]\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        result = jitted_vec_hmm_logp(*inputs)\n",
    "        # PyTensor raises an error if the dtype of the returned output is not\n",
    "        # exactly the one expected from the Apply node (in this case\n",
    "        # `dscalar`, which stands for float64 scalar), so we make sure\n",
    "        # to convert to the expected dtype. To avoid unnecessary conversions\n",
    "        # you should make sure the expected output defined in `make_node`\n",
    "        # is already of the correct dtype\n",
    "        outputs[0][0] = np.asarray(result, dtype=node.outputs[0].dtype)\n",
    "\n",
    "    def grad(self, inputs, output_gradients):\n",
    "        (\n",
    "            grad_wrt_emission_obsered,\n",
    "            grad_wrt_emission_signal,\n",
    "            grad_wrt_emission_noise,\n",
    "            grad_wrt_logp_initial_state,\n",
    "            grad_wrt_logp_transition,\n",
    "        ) = hmm_logp_grad_op(*inputs)\n",
    "        # If there are inputs for which the gradients will never be needed or cannot\n",
    "        # be computed, `pytensor.gradient.grad_not_implemented` should  be used as the\n",
    "        # output gradient for that input.\n",
    "        output_gradient = output_gradients[0]\n",
    "        return [\n",
    "            output_gradient * grad_wrt_emission_obsered,\n",
    "            output_gradient * grad_wrt_emission_signal,\n",
    "            output_gradient * grad_wrt_emission_noise,\n",
    "            output_gradient * grad_wrt_logp_initial_state,\n",
    "            output_gradient * grad_wrt_logp_transition,\n",
    "        ]\n",
    "\n",
    "\n",
    "class HMMLogpGradOp(Op):\n",
    "    def make_node(\n",
    "        self,\n",
    "        emission_observed,\n",
    "        emission_signal,\n",
    "        emission_noise,\n",
    "        logp_initial_state,\n",
    "        logp_transition,\n",
    "    ):\n",
    "        inputs = [\n",
    "            pt.as_tensor_variable(emission_observed),\n",
    "            pt.as_tensor_variable(emission_signal),\n",
    "            pt.as_tensor_variable(emission_noise),\n",
    "            pt.as_tensor_variable(logp_initial_state),\n",
    "            pt.as_tensor_variable(logp_transition),\n",
    "        ]\n",
    "        # This `Op` will return one gradient per input. For simplicity, we assume\n",
    "        # each output is of the same type as the input. In practice, you should use\n",
    "        # the exact dtype to avoid overhead when saving the results of the computation\n",
    "        # in `perform`\n",
    "        outputs = [inp.type() for inp in inputs]\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        (\n",
    "            grad_wrt_emission_obsered_result,\n",
    "            grad_wrt_emission_signal_result,\n",
    "            grad_wrt_emission_noise_result,\n",
    "            grad_wrt_logp_initial_state_result,\n",
    "            grad_wrt_logp_transition_result,\n",
    "        ) = jitted_vec_hmm_logp_grad(*inputs)\n",
    "        outputs[0][0] = np.asarray(grad_wrt_emission_obsered_result, dtype=node.outputs[0].dtype)\n",
    "        outputs[1][0] = np.asarray(grad_wrt_emission_signal_result, dtype=node.outputs[1].dtype)\n",
    "        outputs[2][0] = np.asarray(grad_wrt_emission_noise_result, dtype=node.outputs[2].dtype)\n",
    "        outputs[3][0] = np.asarray(grad_wrt_logp_initial_state_result, dtype=node.outputs[3].dtype)\n",
    "        outputs[4][0] = np.asarray(grad_wrt_logp_transition_result, dtype=node.outputs[4].dtype)\n",
    "\n",
    "\n",
    "# Initialize our `Op`s\n",
    "hmm_logp_op = HMMLogpOp()\n",
    "hmm_logp_grad_op = HMMLogpGradOp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend using the debug helper `eval` method to confirm we specified everything correctly. We should get the same outputs as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_logp_op(\n",
    "    emission_observed,\n",
    "    emission_signal_true,\n",
    "    emission_noise_true,\n",
    "    logp_initial_state_true,\n",
    "    logp_transition_true,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_logp_grad_op(\n",
    "    emission_observed,\n",
    "    emission_signal_true,\n",
    "    emission_noise_true,\n",
    "    logp_initial_state_true,\n",
    "    logp_transition_true,\n",
    ")[1].eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It's also useful to check the gradient of our {class}`~pytensor.graph.op.Op` can be requested via the PyTensor `grad` interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the symbolic `emission_signal` variable outside of the `Op`\n",
    "# so that we can request the gradient wrt to it\n",
    "emission_signal_variable = pt.as_tensor_variable(emission_signal_true)\n",
    "x = hmm_logp_op(\n",
    "    emission_observed,\n",
    "    emission_signal_variable,\n",
    "    emission_noise_true,\n",
    "    logp_initial_state_true,\n",
    "    logp_transition_true,\n",
    ")\n",
    "x_grad_wrt_emission_signal = pt.grad(x, wrt=emission_signal_variable)\n",
    "x_grad_wrt_emission_signal.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling with PyMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to make inferences about our HMM model with PyMC. We will define priors for each model parameter and use {class}`~pymc.Potential` to add the joint log-likelihood term to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    emission_signal = pm.Normal(\"emission_signal\", 0, 1)\n",
    "    emission_noise = pm.HalfNormal(\"emission_noise\", 1)\n",
    "\n",
    "    p_initial_state = pm.Dirichlet(\"p_initial_state\", np.ones(3))\n",
    "    logp_initial_state = pt.log(p_initial_state)\n",
    "\n",
    "    p_transition = pm.Dirichlet(\"p_transition\", np.ones(3), size=3)\n",
    "    logp_transition = pt.log(p_transition)\n",
    "\n",
    "    loglike = pm.Potential(\n",
    "        \"hmm_loglike\",\n",
    "        hmm_logp_op(\n",
    "            emission_observed,\n",
    "            emission_signal,\n",
    "            emission_noise,\n",
    "            logp_initial_state,\n",
    "            logp_transition,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start sampling, we check the logp of each variable at the model initial point. Bugs tend to manifest themselves in the form of `nan` or `-inf` for the initial probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = model.initial_point()\n",
    "initial_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.point_logps(initial_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    idata = pm.sample(chains=2, cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(idata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = [\n",
    "    emission_signal_true,\n",
    "    emission_noise_true,\n",
    "    *p_initial_state_true,\n",
    "    *p_transition_true.ravel(),\n",
    "]\n",
    "\n",
    "az.plot_posterior(idata, ref_val=true_values, grid=(3, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posteriors look reasonably centered around the true values used to generate our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unwrapping the wrapped JAX function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the beginning, PyTensor can compile an entire graph to JAX. To do this, it needs to know how each {class}`~pytensor.graph.op.Op` in the graph can be converted to a JAX function. This can be done by {term}`dispatch <dispatching>` with {func}`pytensor.link.jax.dispatch.jax_funcify`. Most of the default PyTensor {class}`~pytensor.graph.op.Op`s already have such a dispatch function, but we will need to add a new one for our custom `HMMLogpOp`, as PyTensor has never seen that before.\n",
    "\n",
    "For that we need a function which returns (another) JAX function, that performs the same computation as in our `perform` method. Fortunately, we started exactly with such function, so this amounts to 3 short lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax_funcify.register(HMMLogpOp)\n",
    "def hmm_logp_dispatch(op, **kwargs):\n",
    "    return vec_hmm_logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "We do not return the jitted function, so that the entire PyTensor graph can be jitted together after being converted to JAX.\n",
    ":::\n",
    "\n",
    "For a better understanding of {class}`~pytensor.graph.op.Op` JAX conversions, we recommend reading PyTensor's {doc}`Adding JAX and Numba support for Ops guide <pytensor:extending/creating_a_numba_jax_op>`.\n",
    "\n",
    "We can test that our conversion function is working properly by compiling a {func}`pytensor.function` with `mode=\"JAX\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = hmm_logp_op(\n",
    "    emission_observed,\n",
    "    emission_signal_true,\n",
    "    emission_noise_true,\n",
    "    logp_initial_state_true,\n",
    "    logp_transition_true,\n",
    ")\n",
    "jax_fn = pytensor.function(inputs=[], outputs=out, mode=\"JAX\")\n",
    "jax_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compile a JAX function that computes the log probability of each variable in our PyMC model, similar to {meth}`~pymc.Model.point_logps`. We will use the helper method {meth}`~pymc.Model.compile_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logp_jax_fn = model.compile_fn(model.logp(sum=False), mode=\"JAX\")\n",
    "model_logp_jax_fn(initial_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have added an equally simple function to convert our `HMMLogpGradOp`, in case we wanted to convert PyTensor gradient graphs to JAX. In our case, we don't need to do this because we will rely on JAX `grad` function (or more precisely, NumPyro will rely on it) to obtain these again from our compiled JAX function.\n",
    "\n",
    "We include a {ref}`short discussion <pytensor_vs_jax>` at the end of this document, to help you better understand the trade-offs between working with PyTensor graphs vs JAX functions, and when you might want to use one or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling with NumPyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know our model logp can be entirely compiled to JAX, we can use the handy {func}`pymc.sampling_jax.sample_numpyro_nuts` to sample our model using the pure JAX sampler implemented in NumPyro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    idata_numpyro = pm.sampling_jax.sample_numpyro_nuts(chains=2, progressbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata_numpyro);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(idata_numpyro, ref_val=true_values, grid=(3, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, sampling results look pretty similar! \n",
    "\n",
    "Depending on the model and computer architecture you are using, a pure JAX sampler can provide considerable speedups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(pytensor_vs_jax)=\n",
    "## Some brief notes on using PyTensor vs JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should you use JAX?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, it is pretty straightforward to interface between PyTensor graphs and JAX functions. \n",
    "\n",
    "This can be very handy when you want to combine previously implemented JAX function with PyMC models. We used a marginalized HMM log-likelihood in this example, but the same strategy could be used to do Bayesian inference with Deep Neural Networks or Differential Equations, or pretty much any other functions implemented in JAX that can be used in the context of a Bayesian model.\n",
    "\n",
    "It can also be worth it, if you need to make use of JAX's unique features like vectorization, support for tree structures, or its fine-grained parallelization, and GPU and TPU capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should you not use JAX?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like JAX, PyTensor has the goal of mimicking the NumPy and Scipy APIs, so that writing code in PyTensor should feel very similar to how code is written in those libraries.\n",
    "\n",
    "There are, however, some of advantages to working with PyTensor:\n",
    "\n",
    "1. PyTensor graphs are considerably easier to {ref}`inspect and debug <pytensor:debug_faq>` than JAX functions\n",
    "2. PyTensor has clever {ref}`optimization and stabilization routines <pytensor:optimizations>` that are not possible or implemented in JAX\n",
    "3. PyTensor graphs can be easily {ref}`manipulated after creation <pytensor:graph_rewriting>`\n",
    "\n",
    "Point 2 means your graphs are likely to perform better if written in PyTensor. In general you don't have to worry about using specialized functions like `log1p` or `logsumexp`, as PyTensor will be able to detect the equivalent naive expressions and replace them by their specialized counterparts. Importantly, you still benefit from these optimizations when your graph is later compiled to JAX.\n",
    "\n",
    "The catch is that PyTensor cannot reason about JAX functions, and by association {class}`~pytensor.graph.op.Op`s that wrap them. This means that the larger the portion of the graph is \"hidden\" inside a JAX function, the less a user will benefit from PyTensor's rewrite and debugging abilities.\n",
    "\n",
    "Point 3 is more important for library developers. It is the main reason why PyMC developers opted to use PyTensor (and before that, its predecessor Theano) as its backend. Many of the user-facing utilities provided by PyMC rely on the ability to easily parse and manipulate PyTensor graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Using a single Op that can compute its own gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to create two {class}`~pytensor.graph.op.Op`s, one for the function we cared about and a separate one for its gradients. However, JAX provides a `value_and_grad` utility that can return both the value of a function and its gradients. We can do something similar and get away with a single {class}`~pytensor.graph.op.Op` if we are clever about it.\n",
    "\n",
    "By doing this we can (potentially) save memory and reuse computation that is shared between the function and its gradients. This may be relevant when working with very large JAX functions.\n",
    "\n",
    "Note that this is only useful if you are interested in taking gradients with respect to your {class}`~pytensor.graph.op.Op` using PyTensor. If your endgoal is to compile your graph to JAX, and only then take the gradients (as NumPyro does), then it's better to use the first approach. You don't even need to implement the `grad` method and associated {class}`~pytensor.graph.op.Op` in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "jitted_hmm_logp_value_and_grad = jax.jit(jax.value_and_grad(vec_hmm_logp, argnums=list(range(5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HmmLogpValueGradOp(Op):\n",
    "    # By default only show the first output, and \"hide\" the other ones\n",
    "    default_output = 0\n",
    "\n",
    "    def make_node(self, *inputs):\n",
    "        inputs = [pt.as_tensor_variable(inp) for inp in inputs]\n",
    "        # We now have one output for the function value, and one output for each gradient\n",
    "        outputs = [pt.dscalar()] + [inp.type() for inp in inputs]\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        result, grad_results = jitted_hmm_logp_value_and_grad(*inputs)\n",
    "        outputs[0][0] = np.asarray(result, dtype=node.outputs[0].dtype)\n",
    "        for i, grad_result in enumerate(grad_results, start=1):\n",
    "            outputs[i][0] = np.asarray(grad_result, dtype=node.outputs[i].dtype)\n",
    "\n",
    "    def grad(self, inputs, output_gradients):\n",
    "        # The `Op` computes its own gradients, so we call it again.\n",
    "        value = self(*inputs)\n",
    "        # We hid the gradient outputs by setting `default_update=0`, but we\n",
    "        # can retrieve them anytime by accessing the `Apply` node via `value.owner`\n",
    "        gradients = value.owner.outputs[1:]\n",
    "\n",
    "        # Make sure the user is not trying to take the gradient with respect to\n",
    "        # the gradient outputs! That would require computing the second order\n",
    "        # gradients\n",
    "        assert all(\n",
    "            isinstance(g.type, pytensor.gradient.DisconnectedType) for g in output_gradients[1:]\n",
    "        )\n",
    "\n",
    "        return [output_gradients[0] * grad for grad in gradients]\n",
    "\n",
    "\n",
    "hmm_logp_value_grad_op = HmmLogpValueGradOp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check again that we can take the gradient using PyTensor `grad` interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_signal_variable = pt.as_tensor_variable(emission_signal_true)\n",
    "# Only the first output is assigned to the variable `x`, due to `default_output=0`\n",
    "x = hmm_logp_value_grad_op(\n",
    "    emission_observed,\n",
    "    emission_signal_variable,\n",
    "    emission_noise_true,\n",
    "    logp_initial_state_true,\n",
    "    logp_transition_true,\n",
    ")\n",
    "pt.grad(x, emission_signal_variable).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authored by [Ricardo Vieira](https://github.com/ricardoV94/) in March 24, 2022 ([pymc-examples#302](https://github.com/pymc-devs/pymc-examples/pull/302))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p pytensor,aeppl,xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{include} ../page_footer.md\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "myst": {
   "substitutions": {
    "extra_dependencies": "jax numpyro"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "490.667px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
