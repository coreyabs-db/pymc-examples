{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Approximations\n",
    "\n",
    "\n",
    "The `gp.MarginalSparse` class implements sparse, or inducing point, GP approximations.  It works identically to `gp.Marginal`, except it additionally requires the locations of the inducing points (denoted `Xu`), and it accepts the argument `sigma` instead of `noise` because these sparse approximations assume white IID noise.\n",
    "\n",
    "Three approximations are currently implemented, FITC, DTC and VFE.  For most problems, they produce fairly similar results.  These GP approximations don't form the full covariance matrix over all $n$ training inputs.  Instead they rely on $m < n$ *inducing points*, which are \"strategically\" placed throughout the domain.  Both of these approximations reduce the $\\mathcal{O(n^3)}$ complexity of GPs down to $\\mathcal{O(nm^2)}$ --- a significant speed up.  The memory requirements scale down a bit too, but not as much.  They are commonly referred to as *sparse* approximations, in the sense of being data sparse.  The downside of sparse approximations is that they reduce the expressiveness of the GP.  Reducing the dimension of the covariance matrix effectively reduces the number of covariance matrix eigenvectors that can be used to fit the data.  \n",
    "\n",
    "A choice that needs to be made is where to place the inducing points.  One option is to use a subset of the inputs.  Another possibility is to use K-means.  The location of the inducing points can also be an unknown and optimized as part of the model.  These sparse approximations are useful for speeding up calculations when the density of data points is high and the lengthscales is larger than the separations between inducing points. \n",
    "\n",
    "For more information on these approximations, see [Quinonero-Candela+Rasmussen, 2006](http://www.jmlr.org/papers/v6/quinonero-candela05a.html) and [Titsias 2009](https://pdfs.semanticscholar.org/9c13/b87b5efb4bb011acc89d90b15f637fa48593.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "For the following examples, we use the same data set as was used in the `gp.Marginal` example, but with more data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T03:33:52.801767Z",
     "start_time": "2017-08-28T03:33:52.079385Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T03:34:37.358690Z",
     "start_time": "2017-08-28T03:34:29.708517Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 2000  # The number of data points\n",
    "X = 10 * np.sort(np.random.rand(n))[:, None]\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "ℓ_true = 1.0\n",
    "η_true = 3.0\n",
    "cov_func = η_true**2 * pm.gp.cov.Matern52(1, ℓ_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "# Note that we have to call `eval()` because PyMC3 built on top of Theano\n",
    "f_true = np.random.multivariate_normal(\n",
    "    mean_func(X).eval(), cov_func(X).eval() + 1e-8 * np.eye(n), 1\n",
    ").flatten()\n",
    "\n",
    "# The observed data is the latent function plus a small amount of IID Gaussian noise\n",
    "# The standard deviation of the noise is `sigma`\n",
    "σ_true = 2.0\n",
    "y = f_true + σ_true * np.random.randn(n)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.gca()\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\")\n",
    "ax.plot(X, y, \"ok\", ms=3, alpha=0.5, label=\"Data\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"The true f(x)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the inducing points with K-means\n",
    "\n",
    "We use the NUTS sampler and the `FITC` approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T03:39:03.184846Z",
     "start_time": "2017-08-28T03:34:44.700386Z"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    ℓ = pm.Gamma(\"ℓ\", alpha=2, beta=1)\n",
    "    η = pm.HalfCauchy(\"η\", beta=5)\n",
    "\n",
    "    cov = η**2 * pm.gp.cov.Matern52(1, ℓ)\n",
    "    gp = pm.gp.MarginalSparse(cov_func=cov, approx=\"FITC\")\n",
    "\n",
    "    # initialize 20 inducing points with K-means\n",
    "    # gp.util\n",
    "    Xu = pm.gp.util.kmeans_inducing_points(20, X)\n",
    "\n",
    "    σ = pm.HalfCauchy(\"σ\", beta=5)\n",
    "    y_ = gp.marginal_likelihood(\"y\", X=X, Xu=Xu, y=y, noise=σ)\n",
    "\n",
    "    trace = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T03:39:47.274502Z",
     "start_time": "2017-08-28T03:39:03.193266Z"
    }
   },
   "outputs": [],
   "source": [
    "X_new = np.linspace(-1, 11, 200)[:, None]\n",
    "\n",
    "# add the GP conditional to the model, given the new X values\n",
    "with model:\n",
    "    f_pred = gp.conditional(\"f_pred\", X_new)\n",
    "\n",
    "# To use the MAP values, you can just replace the trace with a length-1 list with `mp`\n",
    "with model:\n",
    "    pred_samples = pm.sample_posterior_predictive(trace, vars=[f_pred], samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T03:42:00.274968Z",
     "start_time": "2017-08-28T03:41:59.288053Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc3.gp.util import plot_gp_dist\n",
    "\n",
    "plot_gp_dist(ax, pred_samples[\"f_pred\"], X_new)\n",
    "\n",
    "# plot the data and the true latent function\n",
    "plt.plot(X, y, \"ok\", ms=3, alpha=0.5, label=\"Observed data\")\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\")\n",
    "plt.plot(Xu, 10 * np.ones(Xu.shape[0]), \"cx\", ms=10, label=\"Inducing point locations\")\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylim([-13, 13])\n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing inducing point locations as part of the model\n",
    "\n",
    "For demonstration purposes, we set `approx=\"VFE\"`.  Any inducing point initialization can be done with any approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T03:40:02.849751Z",
     "start_time": "2017-08-28T03:39:48.421441Z"
    }
   },
   "outputs": [],
   "source": [
    "Xu_init = 10 * np.random.rand(20)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    ℓ = pm.Gamma(\"ℓ\", alpha=2, beta=1)\n",
    "    η = pm.HalfCauchy(\"η\", beta=5)\n",
    "\n",
    "    cov = η**2 * pm.gp.cov.Matern52(1, ℓ)\n",
    "    gp = pm.gp.MarginalSparse(cov_func=cov, approx=\"VFE\")\n",
    "\n",
    "    # set flat prior for Xu\n",
    "    Xu = pm.Flat(\"Xu\", shape=20, testval=Xu_init)\n",
    "\n",
    "    σ = pm.HalfCauchy(\"σ\", beta=5)\n",
    "    y_ = gp.marginal_likelihood(\"y\", X=X, Xu=Xu[:, None], y=y, noise=σ)\n",
    "\n",
    "    mp = pm.find_MAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T03:41:56.410281Z",
     "start_time": "2017-08-28T03:41:54.841763Z"
    }
   },
   "outputs": [],
   "source": [
    "mu, var = gp.predict(X_new, point=mp, diag=True)\n",
    "sd = np.sqrt(var)\n",
    "\n",
    "# draw plot\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.gca()\n",
    "\n",
    "# plot mean and 2σ intervals\n",
    "plt.plot(X_new, mu, \"r\", lw=2, label=\"mean and 2σ region\")\n",
    "plt.plot(X_new, mu + 2 * sd, \"r\", lw=1)\n",
    "plt.plot(X_new, mu - 2 * sd, \"r\", lw=1)\n",
    "plt.fill_between(X_new.flatten(), mu - 2 * sd, mu + 2 * sd, color=\"r\", alpha=0.5)\n",
    "\n",
    "# plot original data and true function\n",
    "plt.plot(X, y, \"ok\", ms=3, alpha=1.0, label=\"observed data\")\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"true f\")\n",
    "Xu = mp[\"Xu\"]\n",
    "plt.plot(Xu, 10 * np.ones(Xu.shape[0]), \"cx\", ms=10, label=\"Inducing point locations\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylim([-13, 13])\n",
    "plt.title(\"predictive mean and 2σ interval\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
