{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(gp_latent)=\n",
    "# Gaussian Processes: Latent Variable Implementation\n",
    "\n",
    ":::{post} June 6, 2023\n",
    ":tags: gaussian processes, time series\n",
    ":category: reference, intermediate\n",
    ":author: Bill Engels\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The {class}`gp.Latent <pymc.gp.Latent>` class is a direct implementation of a Gaussian process without approximation.  Given a mean and covariance function, we can place a prior on the function $f(x)$,\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x),\\, k(x, x')) \\,.\n",
    "$$\n",
    "\n",
    "It is called \"Latent\" because the GP itself is included in the model as a latent variable, it is not marginalized out as is the case with {class}`gp.Marginal <pymc.gp.Marginal>`.  Unlike `gp.Latent`, you won't find samples from the GP posterior in the trace with `gp.Marginal`.  This is the most direct implementation of a GP because it doesn't assume a particular likelihood function or structure in the data or in the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `.prior` method\n",
    "\n",
    "The `prior` method adds a multivariate normal prior distribution to the PyMC model over the vector of GP function values, $\\mathbf{f}$,\n",
    "\n",
    "$$\n",
    "\\mathbf{f} \\sim \\text{MvNormal}(\\mathbf{m}_{x},\\, \\mathbf{K}_{xx}) \\,,\n",
    "$$\n",
    "\n",
    "where the vector $\\mathbf{m}_x$ and the matrix $\\mathbf{K}_{xx}$ are the mean vector and covariance matrix evaluated over the inputs $x$.  By default, PyMC reparameterizes the prior on `f` under the hood by rotating it with the Cholesky factor of its covariance matrix.  This improves sampling by reducing covariances in the posterior of the transformed random variable, `v`.  The reparameterized model is,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\mathbf{v} \\sim \\text{N}(0, 1)& \\\\\n",
    "  \\mathbf{L} = \\text{Cholesky}(\\mathbf{K}_{xx})& \\\\\n",
    "  \\mathbf{f} = \\mathbf{m}_{x} + \\mathbf{Lv} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For more information on this reparameterization, see the section on [drawing values from a multivariate distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Drawing_values_from_the_distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `.conditional` method\n",
    "\n",
    "The conditional method implements the predictive distribution for function values that were not part of the original data set.  This distribution is,\n",
    "\n",
    "$$\n",
    "\\mathbf{f}_* \\mid \\mathbf{f} \\sim \\text{MvNormal} \\left(\n",
    "  \\mathbf{m}_* + \\mathbf{K}_{*x}\\mathbf{K}_{xx}^{-1} \\mathbf{f} ,\\,\n",
    "  \\mathbf{K}_{**} - \\mathbf{K}_{*x}\\mathbf{K}_{xx}^{-1}\\mathbf{K}_{x*} \\right)\n",
    "$$\n",
    "\n",
    "Using the same `gp` object we defined above, we can construct a random variable with this\n",
    "distribution by,\n",
    "\n",
    "```python\n",
    "# vector of new X points we want to predict the function at\n",
    "X_star = np.linspace(0, 2, 100)[:, None]\n",
    "\n",
    "with latent_gp_model:\n",
    "    f_star = gp.conditional(\"f_star\", X_star)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Regression with Student-T distributed noise\n",
    "\n",
    "The following is an example showing how to specify a simple model with a GP prior using the {class}`gp.Latent` class.  We use a GP to generate the data so we can verify that the inference we perform is correct.  Note that the likelihood is not normal, but IID Student-T.  For a more efficient implementation when the likelihood is Gaussian, use {class}`gp.Marginal`.\n",
    "\n",
    ":::{include} ../extra_installs.md\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T05:45:33.296658Z",
     "start_time": "2017-08-28T05:45:32.563463Z"
    }
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "RANDOM_SEED = 8998\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T05:45:35.398938Z",
     "start_time": "2017-08-28T05:45:33.438323Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n = 50  # The number of data points\n",
    "X = np.linspace(0, 10, n)[:, None]  # The inputs to the GP must be arranged as a column vector\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "ell_true = 1.0\n",
    "eta_true = 4.0\n",
    "cov_func = eta_true**2 * pm.gp.cov.ExpQuad(1, ell_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "f_true = pm.draw(pm.MvNormal.dist(mu=mean_func(X), cov=cov_func(X)), 1, random_seed=rng)\n",
    "\n",
    "# The observed data is the latent function plus a small amount of T distributed noise\n",
    "# The standard deviation of the noise is `sigma`, and the degrees of freedom is `nu`\n",
    "sigma_true = 1.0\n",
    "nu_true = 5.0\n",
    "y = f_true + sigma_true * rng.standard_t(df=nu_true, size=n)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = fig.gca()\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True generating function 'f'\")\n",
    "ax.plot(X, y, \"ok\", ms=3, label=\"Observed data\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"y\")\n",
    "plt.legend(frameon=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data above shows the observations, marked with black dots, of the unknown function $f(x)$ that has been corrupted by noise.  The true function is in blue.\n",
    "\n",
    "### Coding the model in PyMC\n",
    "\n",
    "Here's the model in PyMC.  We use an informative {class}`pm.Gamma(alpha=2, beta=1)` prior over the lengthscale parameter, and weakly informative {class}`pm.HalfNormal(sigma=5)` priors over the covariance function scale, and noise scale.  A `pm.Gamma(2, 0.5)` prior is assigned to the degrees of freedom parameter of the noise.  Finally, a GP prior is placed on the unknown function.  For more information on choosing priors in Gaussian process models, check out some of [recommendations by the Stan folks](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations#priors-for-gaussian-processes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T05:49:32.027063Z",
     "start_time": "2017-08-28T05:45:35.633110Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    ell = pm.Gamma(\"ell\", alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal(\"eta\", sigma=5)\n",
    "\n",
    "    cov = eta**2 * pm.gp.cov.ExpQuad(1, ell)\n",
    "    gp = pm.gp.Latent(cov_func=cov)\n",
    "\n",
    "    f = gp.prior(\"f\", X=X)\n",
    "\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=2.0)\n",
    "    nu = 1 + pm.Gamma(\n",
    "        \"nu\", alpha=2, beta=0.1\n",
    "    )  # add one because student t is undefined for degrees of freedom less than one\n",
    "    y_ = pm.StudentT(\"y\", mu=f, lam=1.0 / sigma, nu=nu, observed=y)\n",
    "\n",
    "    idata = pm.sample(nuts_sampler=\"numpyro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Rhat, values above 1 may indicate convergence issues\n",
    "n_nonconverged = int(\n",
    "    np.sum(az.rhat(idata)[[\"eta\", \"ell\", \"sigma\", \"f_rotated_\"]].to_array() > 1.03).values\n",
    ")\n",
    "if n_nonconverged == 0:\n",
    "    print(\"No Rhat values above 1.03, \\N{check mark}\")\n",
    "else:\n",
    "    print(f\"The MCMC chains for {n_nonconverged} RVs appear not to have converged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The joint posterior of the two covariance function hyperparameters is plotted below in the left panel.  In the right panel is the joint posterior of the standard deviation of the noise, and the degrees of freedom parameter of the likelihood.  The light blue lines show the true values that were used to draw the function from the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# plot eta vs ell\n",
    "az.plot_pair(\n",
    "    idata,\n",
    "    var_names=[\"eta\", \"ell\"],\n",
    "    kind=[\"hexbin\"],\n",
    "    ax=axs[0],\n",
    "    gridsize=25,\n",
    "    divergences=True,\n",
    ")\n",
    "axs[0].axvline(x=eta_true, color=\"dodgerblue\")\n",
    "axs[0].axhline(y=ell_true, color=\"dodgerblue\")\n",
    "\n",
    "# plot nu vs sigma\n",
    "az.plot_pair(\n",
    "    idata,\n",
    "    var_names=[\"nu\", \"sigma\"],\n",
    "    kind=[\"hexbin\"],\n",
    "    ax=axs[1],\n",
    "    gridsize=25,\n",
    "    divergences=True,\n",
    ")\n",
    "\n",
    "axs[1].axvline(x=nu_true, color=\"dodgerblue\")\n",
    "axs[1].axhline(y=sigma_true, color=\"dodgerblue\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_post = az.extract(idata, var_names=\"f\").transpose(\"sample\", ...)\n",
    "f_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the posterior of the GP,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T05:49:34.363422Z",
     "start_time": "2017-08-28T05:49:33.477956Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc.gp.util import plot_gp_dist\n",
    "\n",
    "f_post = az.extract(idata, var_names=\"f\").transpose(\"sample\", ...)\n",
    "plot_gp_dist(ax, f_post, X)\n",
    "\n",
    "# plot the data and the true latent function\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True generating function 'f'\")\n",
    "ax.plot(X, y, \"ok\", ms=3, label=\"Observed data\")\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"True f(x)\")\n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see by the red shading, the posterior of the GP prior over the function does a great job of representing both the fit, and the uncertainty caused by the additive noise.  The result also doesn't over fit due to outliers from the Student-T noise model.\n",
    "\n",
    "### Prediction using `.conditional`\n",
    "\n",
    "Next, we extend the model by adding the conditional distribution so we can predict at new $x$ locations.  Lets see how the extrapolation looks out to higher $x$.  To do this, we extend our `model` with the `conditional` distribution of the GP. Then, we can sample from it using the `trace` and the `sample_posterior_predictive` function.\n",
    "\n",
    "This is similar to how Stan uses its `generated quantities {...}` block.  We could have included `gp.conditional` in the model *before* we did the NUTS sampling, but it is more efficient to separate these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T05:49:49.545718Z",
     "start_time": "2017-08-28T05:49:34.365046Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_new = 200\n",
    "X_new = np.linspace(-4, 14, n_new)[:, None]\n",
    "\n",
    "with model:\n",
    "    # add the GP conditional to the model, given the new X values\n",
    "    f_pred = gp.conditional(\"f_pred\", X_new, jitter=1e-4)\n",
    "\n",
    "    # Sample from the GP conditional distribution\n",
    "    idata.extend(pm.sample_posterior_predictive(idata, var_names=[\"f_pred\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T05:49:50.752661Z",
     "start_time": "2017-08-28T05:49:49.555973Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = fig.gca()\n",
    "\n",
    "f_pred = az.extract(idata.posterior_predictive, var_names=\"f_pred\").transpose(\"sample\", ...)\n",
    "plot_gp_dist(ax, f_pred, X_new)\n",
    "\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True generating function 'f'\")\n",
    "ax.plot(X, y, \"ok\", ms=3, label=\"Observed data\")\n",
    "\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"True f(x)\")\n",
    "ax.set_title(\"Conditional distribution of f_*, given f\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Classification\n",
    "\n",
    "First we use a GP to generate some data that follows a Bernoulli distribution, where $p$, the probability of a one instead of a zero is a function of $x$.  I reset the seed and added more fake data points, because it can be difficult for the model to discern variations around 0.5 with few observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T05:49:52.127506Z",
     "start_time": "2017-08-28T05:49:50.754522Z"
    }
   },
   "outputs": [],
   "source": [
    "# reset the random seed for the new example\n",
    "RANDOM_SEED = 8888\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# number of data points\n",
    "n = 300\n",
    "\n",
    "# x locations\n",
    "x = np.linspace(0, 10, n)\n",
    "\n",
    "# true covariance\n",
    "ell_true = 0.5\n",
    "eta_true = 1.0\n",
    "cov_func = eta_true**2 * pm.gp.cov.ExpQuad(1, ell_true)\n",
    "K = cov_func(x[:, None]).eval()\n",
    "\n",
    "# zero mean function\n",
    "mean = np.zeros(n)\n",
    "\n",
    "# sample from the gp prior\n",
    "f_true = pm.draw(pm.MvNormal.dist(mu=mean, cov=K), 1, random_seed=rng)\n",
    "\n",
    "# Sample the GP through the likelihood\n",
    "y = pm.Bernoulli.dist(p=pm.math.invlogit(f_true)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T05:49:52.453288Z",
     "start_time": "2017-08-28T05:49:52.130511Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.plot(x, pm.math.invlogit(f_true).eval(), \"dodgerblue\", lw=3, label=\"True rate\")\n",
    "# add some noise to y to make the points in the plot more visible\n",
    "ax.plot(x, y + np.random.randn(n) * 0.01, \"kx\", ms=6, label=\"Observed data\")\n",
    "\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_xlim([0, 11])\n",
    "plt.legend(loc=(0.35, 0.65), frameon=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T05:57:46.095641Z",
     "start_time": "2017-08-28T05:49:52.455076Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    ell = pm.InverseGamma(\"ell\", mu=1.0, sigma=0.5)\n",
    "    eta = pm.Exponential(\"eta\", lam=1.0)\n",
    "    cov = eta**2 * pm.gp.cov.ExpQuad(1, ell)\n",
    "\n",
    "    gp = pm.gp.Latent(cov_func=cov)\n",
    "    f = gp.prior(\"f\", X=x[:, None])\n",
    "\n",
    "    # logit link and Bernoulli likelihood\n",
    "    p = pm.Deterministic(\"p\", pm.math.invlogit(f))\n",
    "    y_ = pm.Bernoulli(\"y\", p=p, observed=y)\n",
    "\n",
    "    idata = pm.sample(1000, chains=2, cores=2, nuts_sampler=\"numpyro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Rhat, values above 1 may indicate convergence issues\n",
    "n_nonconverged = int(np.sum(az.rhat(idata)[[\"eta\", \"ell\", \"f_rotated_\"]].to_array() > 1.03).values)\n",
    "if n_nonconverged == 0:\n",
    "    print(\"No Rhat values above 1.03, \\N{check mark}\")\n",
    "else:\n",
    "    print(f\"The MCMC chains for {n_nonconverged} RVs appear not to have converged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_pair(\n",
    "    idata,\n",
    "    var_names=[\"eta\", \"ell\"],\n",
    "    kind=[\"kde\", \"scatter\"],\n",
    "    scatter_kwargs={\"color\": \"darkslategray\", \"alpha\": 0.4},\n",
    "    gridsize=25,\n",
    "    divergences=True,\n",
    ")\n",
    "\n",
    "ax.axvline(x=eta_true, color=\"dodgerblue\")\n",
    "ax.axhline(y=ell_true, color=\"dodgerblue\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T05:58:12.840482Z",
     "start_time": "2017-08-28T05:57:54.082911Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_pred = 200\n",
    "X_new = np.linspace(0, 12, n_pred)[:, None]\n",
    "\n",
    "with model:\n",
    "    f_pred = gp.conditional(\"f_pred\", X_new, jitter=1e-4)\n",
    "    p_pred = pm.Deterministic(\"p_pred\", pm.math.invlogit(f_pred))\n",
    "\n",
    "with model:\n",
    "    idata.extend(pm.sample_posterior_predictive(idata.posterior, var_names=[\"f_pred\", \"p_pred\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T05:58:14.395655Z",
     "start_time": "2017-08-28T05:58:12.866843Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "p_pred = az.extract(idata.posterior_predictive, var_names=\"p_pred\").transpose(\"sample\", ...)\n",
    "plot_gp_dist(ax, p_pred, X_new)\n",
    "\n",
    "# plot the data (with some jitter) and the true latent function\n",
    "plt.plot(x, pm.math.invlogit(f_true).eval(), \"dodgerblue\", lw=3, label=\"True f\")\n",
    "plt.plot(\n",
    "    x,\n",
    "    y + np.random.randn(y.shape[0]) * 0.01,\n",
    "    \"kx\",\n",
    "    ms=6,\n",
    "    alpha=0.5,\n",
    "    label=\"Observed data\",\n",
    ")\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"True f(x)\")\n",
    "plt.xlim([0, 12])\n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\")\n",
    "plt.legend(loc=(0.32, 0.65), frameon=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "* Created by [Bill Engels](https://github.com/bwengals) in 2017 ([pymc#1674](https://github.com/pymc-devs/pymc/pull/1674))\n",
    "* Reexecuted by [Colin Caroll](https://github.com/ColCarroll) in 2019 ([pymc#3397](https://github.com/pymc-devs/pymc/pull/3397))\n",
    "* Updated for V4 by Bill Engels in September 2022 ([pymc-examples#237](https://github.com/pymc-devs/pymc-examples/pull/237))\n",
    "* Updated for V5 by Chris Fonnesbeck in July 2023 ([pymc-examples#549](https://github.com/pymc-devs/pymc-examples/pull/549))\n",
    "* Updated by [Alexandre Andorra](https://github.com/AlexAndorra) in May 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p pytensor,aeppl,xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{include} ../page_footer.md\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pymc-examples",
   "language": "python",
   "name": "pymc-examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "myst": {
   "substitutions": {
    "extra_dependencies": "jax numpyro"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
