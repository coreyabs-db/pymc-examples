{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(gp_marginal)=\n",
    "# Marginal Likelihood Implementation\n",
    "\n",
    ":::{post} June 4, 2023\n",
    ":tags: gaussian processes, time series\n",
    ":category: reference, intermediate\n",
    ":author: Bill Engels, Chris Fonnesbeck\n",
    ":::\n",
    "\n",
    "The `gp.Marginal` class implements the more common case of GP regression:  the observed data are the sum of a GP and Gaussian noise.  `gp.Marginal` has a `marginal_likelihood` method, a `conditional` method, and a `predict` method.  Given a mean and covariance function, the function $f(x)$ is modeled as,\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x),\\, k(x, x')) \\,.\n",
    "$$\n",
    "\n",
    "The observations $y$ are the unknown function plus noise\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\epsilon &\\sim N(0, \\Sigma) \\\\\n",
    "  y &= f(x) + \\epsilon \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `.marginal_likelihood` method\n",
    "\n",
    "The unknown latent function can be analytically integrated out of the product of the GP prior probability with a normal likelihood.  This quantity is called the marginal likelihood. \n",
    "\n",
    "$$\n",
    "p(y \\mid x) = \\int p(y \\mid f, x) \\, p(f \\mid x) \\, df\n",
    "$$\n",
    "\n",
    "The log of the marginal likelihood, $p(y \\mid x)$, is\n",
    "\n",
    "$$\n",
    "\\log p(y \\mid x) = \n",
    "  -\\frac{1}{2} (\\mathbf{y} - \\mathbf{m}_x)^{T} \n",
    "               (\\mathbf{K}_{xx} + \\boldsymbol\\Sigma)^{-1} \n",
    "               (\\mathbf{y} - \\mathbf{m}_x)\n",
    "  - \\frac{1}{2}\\log(\\mathbf{K}_{xx} + \\boldsymbol\\Sigma)\n",
    "  - \\frac{n}{2}\\log (2 \\pi)\n",
    "$$\n",
    "\n",
    "$\\boldsymbol\\Sigma$ is the covariance matrix of the Gaussian noise.  Since the Gaussian noise doesn't need to be white to be conjugate, the `marginal_likelihood` method supports either using a white noise term when a scalar is provided, or a noise covariance function when a covariance function is provided.\n",
    "\n",
    "The `gp.marginal_likelihood` method implements the quantity given above.  Some sample code would be,\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "\n",
    "# A one dimensional column vector of inputs.\n",
    "X = np.linspace(0, 1, 10)[:,None]\n",
    "\n",
    "with pm.Model() as marginal_gp_model:\n",
    "    # Specify the covariance function.\n",
    "    cov_func = pm.gp.cov.ExpQuad(1, ls=0.1)\n",
    "\n",
    "    # Specify the GP.  The default mean function is `Zero`.\n",
    "    gp = pm.gp.Marginal(cov_func=cov_func)\n",
    "\n",
    "    # The scale of the white noise term can be provided,\n",
    "    sigma = pm.HalfCauchy(\"sigma\", beta=5)\n",
    "    y_ = gp.marginal_likelihood(\"y\", X=X, y=y, sigma=sigma)\n",
    "    \n",
    "    # OR a covariance function for the noise can be given\n",
    "    # noise_l = pm.Gamma(\"noise_l\", alpha=2, beta=2)\n",
    "    # cov_func_noise = pm.gp.cov.Exponential(1, noise_l) + pm.gp.cov.WhiteNoise(sigma=0.1)\n",
    "    # y_ = gp.marginal_likelihood(\"y\", X=X, y=y, sigma=cov_func_noise)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `.conditional` distribution\n",
    "\n",
    "The `.conditional` has an optional flag for `pred_noise`, which defaults to `False`.  When `pred_sigma=False`, the `conditional` method produces the predictive distribution for the underlying function represented by the GP.  When `pred_sigma=True`, the `conditional` method produces the predictive distribution for the GP plus noise.  Using the same `gp` object defined above, \n",
    "\n",
    "```python\n",
    "# vector of new X points we want to predict the function at\n",
    "Xnew = np.linspace(0, 2, 100)[:, None]\n",
    "\n",
    "with marginal_gp_model:\n",
    "    f_star = gp.conditional(\"f_star\", Xnew=Xnew)\n",
    "    \n",
    "    # or to predict the GP plus noise\n",
    "    y_star = gp.conditional(\"y_star\", Xnew=Xnew, pred_sigma=True)\n",
    "```\n",
    "If using an additive GP model, the conditional distribution for individual components can be constructed by setting the optional argument `given`.  For more information on building additive GPs, see the main documentation page.  For an example, see the Mauna Loa CO$_2$ notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "The `.predict` method returns the conditional mean and variance of the `gp` given a `point` as NumPy arrays.  The `point` can be the result of `find_MAP` or a sample from the trace.  The `.predict` method can be used outside of a `Model` block.  Like `.conditional`, `.predict` accepts `given` so it can produce predictions from components of additive GPs.\n",
    "\n",
    "```python\n",
    "# The mean and full covariance\n",
    "mu, cov = gp.predict(Xnew, point=trace[-1])\n",
    "\n",
    "# The mean and variance (diagonal of the covariance)\n",
    "mu, var = gp.predict(Xnew, point=trace[-1],  diag=True)\n",
    "\n",
    "# With noise included\n",
    "mu, var = gp.predict(Xnew, point=trace[-1],  diag=True, pred_sigma=True)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Regression with white, Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T05:12:49.739412Z",
     "start_time": "2017-09-04T05:12:48.977594Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import scipy as sp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T05:12:56.120937Z",
     "start_time": "2017-09-04T05:12:49.877135Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 100  # The number of data points\n",
    "X = np.linspace(0, 10, n)[:, None]  # The inputs to the GP, they must be arranged as a column vector\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "ell_true = 1.0\n",
    "eta_true = 3.0\n",
    "cov_func = eta_true**2 * pm.gp.cov.Matern52(1, ell_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "# Note that we have to call `eval()` because PyMC3 built on top of Theano\n",
    "f_true = np.random.multivariate_normal(\n",
    "    mean_func(X).eval(), cov_func(X).eval() + 1e-8 * np.eye(n), 1\n",
    ").flatten()\n",
    "\n",
    "# The observed data is the latent function plus a small amount of IID Gaussian noise\n",
    "# The standard deviation of the noise is `sigma`\n",
    "sigma_true = 2.0\n",
    "y = f_true + sigma_true * np.random.randn(n)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.gca()\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\")\n",
    "ax.plot(X, y, \"ok\", ms=3, alpha=0.5, label=\"Data\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"The true f(x)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T05:13:02.744988Z",
     "start_time": "2017-09-04T05:12:56.122906Z"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    ell = pm.Gamma(\"ell\", alpha=2, beta=1)\n",
    "    eta = pm.HalfCauchy(\"eta\", beta=5)\n",
    "\n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ell)\n",
    "    gp = pm.gp.Marginal(cov_func=cov)\n",
    "\n",
    "    sigma = pm.HalfCauchy(\"sigma\", beta=5)\n",
    "    y_ = gp.marginal_likelihood(\"y\", X=X, y=y, sigma=sigma)\n",
    "\n",
    "    with model:\n",
    "        marginal_post = pm.sample(500, tune=2000, nuts_sampler=\"numpyro\", chains=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T05:13:02.780531Z",
     "start_time": "2017-09-04T05:13:02.747697Z"
    }
   },
   "outputs": [],
   "source": [
    "summary = az.summary(marginal_post, var_names=[\"ell\", \"eta\", \"sigma\"], round_to=2, kind=\"stats\")\n",
    "summary[\"True value\"] = [ell_true, eta_true, sigma_true]\n",
    "summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated values are close to their true values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `.conditional`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T05:14:54.499149Z",
     "start_time": "2017-09-04T05:13:02.782161Z"
    }
   },
   "outputs": [],
   "source": [
    "# new values from x=0 to x=20\n",
    "X_new = np.linspace(0, 20, 600)[:, None]\n",
    "\n",
    "# add the GP conditional to the model, given the new X values\n",
    "with model:\n",
    "    f_pred = gp.conditional(\"f_pred\", X_new)\n",
    "\n",
    "with model:\n",
    "    pred_samples = pm.sample_posterior_predictive(\n",
    "        marginal_post.sel(draw=slice(0, 20)), var_names=[\"f_pred\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T05:14:57.904355Z",
     "start_time": "2017-09-04T05:14:54.505114Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc.gp.util import plot_gp_dist\n",
    "\n",
    "f_pred_samples = az.extract(pred_samples, group=\"posterior_predictive\", var_names=[\"f_pred\"])\n",
    "plot_gp_dist(ax, samples=f_pred_samples.T, x=X_new)\n",
    "\n",
    "# plot the data and the true latent function\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\")\n",
    "plt.plot(X, y, \"ok\", ms=3, alpha=0.5, label=\"Observed data\")\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylim([-13, 13])\n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction also matches the results from `gp.Latent` very closely.  What about predicting new data points?  Here we only predicted $f_*$, not $f_*$ + noise, which is what we actually observe.\n",
    "\n",
    "The `conditional` method of `gp.Marginal` contains the flag `pred_noise` whose default value is `False`.  To draw from the *posterior predictive* distribution, we simply set this flag to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T05:16:37.290226Z",
     "start_time": "2017-09-04T05:14:57.906373Z"
    }
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    y_pred = gp.conditional(\"y_pred\", X_new, pred_noise=True)\n",
    "    y_samples = pm.sample_posterior_predictive(\n",
    "        marginal_post.sel(draw=slice(0, 50)), var_names=[\"y_pred\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T05:16:40.545932Z",
     "start_time": "2017-09-04T05:16:37.321425Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.gca()\n",
    "\n",
    "# posterior predictive distribution\n",
    "y_pred_samples = az.extract(y_samples, group=\"posterior_predictive\", var_names=[\"y_pred\"])\n",
    "plot_gp_dist(ax, y_pred_samples.T, X_new, plot_samples=False, palette=\"bone_r\")\n",
    "\n",
    "# overlay a scatter of one draw of random points from the\n",
    "#   posterior predictive distribution\n",
    "plt.plot(X_new, y_pred_samples.sel(sample=1), \"co\", ms=2, label=\"Predicted data\")\n",
    "\n",
    "# plot original data and true function\n",
    "plt.plot(X, y, \"ok\", ms=3, alpha=1.0, label=\"observed data\")\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"true f\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylim([-13, 13])\n",
    "plt.title(\"posterior predictive distribution, y_*\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-07T22:22:33.216443Z",
     "start_time": "2017-08-07T22:22:33.210493Z"
    }
   },
   "source": [
    "Notice that the posterior predictive density is wider than the conditional distribution of the noiseless function, and reflects the predictive distribution of the noisy data, which is marked as black dots.  The light colored dots don't follow the spread of the predictive density exactly because they are a single draw from the posterior of the GP plus noise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `.predict`\n",
    "\n",
    "We can use the `.predict` method to return the mean and variance given a particular `point`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T05:16:41.567782Z",
     "start_time": "2017-09-04T05:16:40.547616Z"
    }
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "with model:\n",
    "    mu, var = gp.predict(\n",
    "        X_new, point=az.extract(marginal_post.posterior.sel(draw=[0])).squeeze(), diag=True\n",
    "    )\n",
    "sd = np.sqrt(var)\n",
    "\n",
    "# draw plot\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.gca()\n",
    "\n",
    "# plot mean and 2sigma intervals\n",
    "plt.plot(X_new, mu, \"r\", lw=2, label=\"mean and 2σ region\")\n",
    "plt.plot(X_new, mu + 2 * sd, \"r\", lw=1)\n",
    "plt.plot(X_new, mu - 2 * sd, \"r\", lw=1)\n",
    "plt.fill_between(X_new.flatten(), mu - 2 * sd, mu + 2 * sd, color=\"r\", alpha=0.5)\n",
    "\n",
    "# plot original data and true function\n",
    "plt.plot(X, y, \"ok\", ms=3, alpha=1.0, label=\"observed data\")\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"true f\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylim([-13, 13])\n",
    "plt.title(\"predictive mean and 2sigma interval\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "* Created by [Bill Engels](https://github.com/bwengals) in 2017 ([pymc#1674](https://github.com/pymc-devs/pymc/pull/1674))\n",
    "* Reexecuted by [Colin Caroll](https://github.com/ColCarroll) in 2019 ([pymc#3397](https://github.com/pymc-devs/pymc/pull/3397))\n",
    "* Updated for V4 by Bill Engels in September 2022 ([pymc-examples#237](https://github.com/pymc-devs/pymc-examples/pull/237))\n",
    "* Updated for V5 by Chris Fonnesbeck in July 2023 ([pymc-examples#549](https://github.com/pymc-devs/pymc-examples/pull/549))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
