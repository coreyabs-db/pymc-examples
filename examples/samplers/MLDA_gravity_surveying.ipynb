{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilevel Gravity Survey with MLDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MLDA sampler\n",
    "This notebook is designed to demonstrate the Multi-Level Delayed Acceptance MCMC algorithm (MLDA) proposed in Dodwell (2019), as implemented within PyMC3. If you are using MLDA for the first time, we recommend first running the `MLDA_simple_linear_regression.ipynb` notebook in the same folder.\n",
    "\n",
    "The MLDA sampler can be more efficient than other MCMC samplers when dealing with computationally intensive problems where we have access not only to the desired (fine) posterior distribution but also to a set of approximate (coarse) posteriors of decreasing accuracy and decreasing computational cost. In simple terms, we can use multiple chains on different coarseness levels and coarser chains' samples are used as proposals for the finer chains. This has been shown to improve the effective sample size of the finest chain and this allows us to reduce the number of expensive fine-chain likelihood evaluations. \n",
    "\n",
    "The notebook initially defines the necessary classes that describe the model. These classes use scipy to do the numerical solve in the forward model. It then instantiates models in two levels (with different granularities) and generates data for inference. Finally, the model classes are passed to two pymc3 models using Theano Ops and inference is done using three different MCMC methods (including MLDA). Some summary results and comparison plots are shown at the end to demonstrate the results. The use of Theano Ops is common when users want to use external code to calculate their likelihood (e.g. some fast PDE solver) and this example is designed to serve as a starting point for users to employ MLDA in their own problems.\n",
    "\n",
    "Please note that the MLDA sampler is new in PyMC3. The user should be extra critical about the results and report any problems as issues in the pymc3's github repository.\n",
    "\n",
    "The notebook results shown below were generated on a MacBook Pro with a 2.6 GHz 6-core Intel Core i7, 32 GB DDR4 and macOS 10.15.4.\n",
    "\n",
    "### Gravity Surveying\n",
    "In this notebook, we solve a 2-dimensional gravity surveying problem, adapted from the 1D problem presented in Hansen (2010). \n",
    "\n",
    "Our aim is to recover a two-dimensional mass distribution $f(\\vec{t})$ at a known depth $d$ below the surface from measurements $g(\\vec{s})$ of the vertical component of the gravitational field at the surface. The contribution to $g(\\vec{s})$ from infinitesimally small areas of the subsurface mass distribution are given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    dg = \\frac{\\sin \\theta}{r^2} f(\\vec{t}) \\: d\\vec{t}\n",
    "\\end{equation}\n",
    "where $\\theta$ is the angle between the vertical plane and a straight line between two points $f(\\vec{t})$ and $g(\\vec{s})$, and $r = | \\vec{s} - \\vec{t} |$ is the Eucledian distance between the points. We exploit that $\\sin \\theta = \\frac{d}{r}$, so that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\sin \\theta}{r^2} f(\\vec{t}) \\: d\\vec{t} = \\frac{d}{r^3} f(\\vec{t}) \\: d\\vec{t} = \\frac{d}{ | \\vec{s} - \\vec{t} |^3} f(\\vec{t}) \\: d\\vec{t}\n",
    "\\end{equation}\n",
    "\n",
    "This yields the integral equation,\n",
    "\n",
    "\\begin{equation}\n",
    "    g(\\vec{s}) = \\iint_T \\frac{d}{ | \\vec{s} - \\vec{t} |^3} f(\\vec{t}) \\: d\\vec{t}\n",
    "\\end{equation}\n",
    "\n",
    "where $T = [0,1]^2$ is the domain of the function $f(\\vec{t})$. This constitutes our forward model.\n",
    "\n",
    "We solve this integral numerically using midpoint quadrature. For simplicity, we use the same number of quadrature points along each axis, so that in discrete form our forward model becomes\n",
    "\n",
    "\\begin{equation}\n",
    "    g(\\vec{s}_i) = \\sum_{j=1}^{m} \\omega_j \\frac{d}{ | \\vec{s}_i - \\vec{t}_j |^3} \\hat{f}(\\vec{t}_j), \\quad i = 1, \\dots, n, \\quad j = 1, \\dots, m\n",
    "\\end{equation}\n",
    "\n",
    "where $\\omega_j = \\frac{1}{m}$ are quadrature weights, $\\hat{f}(\\vec{t}_j)$ is the approximate subsurface mass at quadrature points $j = 1, \\dots, m$, and  $g(\\vec{s}_i)$ is surface measurements at collocation points $i = 1, \\dots, n$. Hence when $n > m$, we are dealing with an overdetermined problem and vice versa. \n",
    "\n",
    "This results in a linear system $\\mathbf{Ax = b}$, where\n",
    "\\begin{equation}\n",
    "    a_{ij} = \\omega_j \\frac{d}{ | \\vec{s}_i - \\vec{t}_j |^3}, \\quad x_j = \\hat{f}(\\vec{t}_j), \\quad b_i = g(\\vec{s}_i).\n",
    "\\end{equation}\n",
    "In this particular problem, the matrix $\\mathbf{A}$ has a very high condition number, leading to an ill-posed inverse problem, which entails numerical instability and spurious, often oscillatory, solutions for noisy right hand sides. These types of problems are traditionally solved by way of some manner of *regularisation*, but they can be handled in a natural and elegant fashion in the context of a Bayesian inverse problem.\n",
    "\n",
    "### Mass Distribution as a Gaussian Random Process\n",
    "We model the unknown mass distribution as a Gaussian Random Process with a Matern 5/2 covariance kernel (Rasmussen and Williams, 2006):\n",
    "\\begin{equation}\n",
    "    C_{5/2}(\\vec{t}, \\vec{t}') = \\sigma^2 \\left( 1 + \\frac{\\sqrt{5} | \\vec{t}-\\vec{t}' | }{l} + \\frac{5 | \\vec{t}-\\vec{t}' |^2}{3l^2} \\right) \\exp \\left( - \\frac{\\sqrt{5} | \\vec{t}-\\vec{t}' | }{l} \\right)\n",
    "\\end{equation}\n",
    "where $l$ is the covariance length scale and $\\sigma^2$ is the variance.\n",
    "\n",
    "### Comparison\n",
    "Within this notebook, a simple MLDA sampler is compared to a Metropolis and a DEMetropolisZ sampler. The example demonstrates that MLDA is more efficient than the other samplers when measured by the Effective Samples per Second they can generate from the posterior. \n",
    "\n",
    "### References\n",
    "\n",
    "Dodwell, Tim & Ketelsen, Chris & Scheichl, Robert & Teckentrup, Aretha. (2019). Multilevel Markov Chain Monte Carlo. SIAM Review. 61. 509-545. https://doi.org/10.1137/19M126966X\n",
    "\n",
    "Per Christian Hansen. *Discrete Inverse Problems: Insight and Algorithms*. Society for Industrial and Applied Mathematics, January 2010.\n",
    "\n",
    "Carl Edward Rasmussen and Christopher K. I. Williams. *Gaussian processes for machine learning*. Adaptive computation and machine learning. 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "import warnings\n",
    "\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"  # Set environment variable\n",
    "\n",
    "import sys as sys\n",
    "import time as time\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "\n",
    "from numpy.linalg import inv\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from scipy.linalg import eigh\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123446\n",
    "np.random.seed(RANDOM_SEED)\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking versions\n",
    "print(f\"Theano version: {theano.__version__}\")\n",
    "print(f\"PyMC3 version: {pm.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Matern52 kernel for modelling Gaussian Random Field\n",
    "This is utility code which is necessary for defining the model later - you are free to ignore it or place it in an external file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredExponential:\n",
    "    def __init__(self, coords, mkl, lamb):\n",
    "        \"\"\"\n",
    "        This class sets up a random process\n",
    "        on a grid and generates\n",
    "        a realisation of the process, given\n",
    "        parameters or a random vector.\n",
    "        \"\"\"\n",
    "\n",
    "        # Internalise the grid and set number of vertices.\n",
    "        self.coords = coords\n",
    "        self.n_points = self.coords.shape[0]\n",
    "        self.eigenvalues = None\n",
    "        self.eigenvectors = None\n",
    "        self.parameters = None\n",
    "        self.random_field = None\n",
    "\n",
    "        # Set some random field parameters.\n",
    "        self.mkl = mkl\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.assemble_covariance_matrix()\n",
    "\n",
    "    def assemble_covariance_matrix(self):\n",
    "        \"\"\"\n",
    "        Create a snazzy distance-matrix for rapid\n",
    "        computation of the covariance matrix.\n",
    "        \"\"\"\n",
    "        dist = distance_matrix(self.coords, self.coords)\n",
    "\n",
    "        # Compute the covariance between all\n",
    "        # points in the space.\n",
    "        self.cov = np.exp(-0.5 * dist**2 / self.lamb**2)\n",
    "\n",
    "    def plot_covariance_matrix(self):\n",
    "        \"\"\"\n",
    "        Plot the covariance matrix.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(self.cov, cmap=\"binary\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    def compute_eigenpairs(self):\n",
    "        \"\"\"\n",
    "        Find eigenvalues and eigenvectors using Arnoldi iteration.\n",
    "        \"\"\"\n",
    "        eigvals, eigvecs = eigh(self.cov, eigvals=(self.n_points - self.mkl, self.n_points - 1))\n",
    "\n",
    "        order = np.flip(np.argsort(eigvals))\n",
    "        self.eigenvalues = eigvals[order]\n",
    "        self.eigenvectors = eigvecs[:, order]\n",
    "\n",
    "    def generate(self, parameters=None):\n",
    "        \"\"\"\n",
    "        Generate a random field, see\n",
    "        Scarth, C., Adhikari, S., Cabral, P. H.,\n",
    "        Silva, G. H. C., & Prado, A. P. do. (2019).\n",
    "        Random field simulation over curved surfaces:\n",
    "        Applications to computational structural mechanics.\n",
    "        Computer Methods in Applied Mechanics and Engineering,\n",
    "        345, 283â€“301. https://doi.org/10.1016/j.cma.2018.10.026\n",
    "        \"\"\"\n",
    "\n",
    "        if parameters is None:\n",
    "            self.parameters = np.random.normal(size=self.mkl)\n",
    "        else:\n",
    "            self.parameters = np.array(parameters).flatten()\n",
    "\n",
    "        self.random_field = np.linalg.multi_dot(\n",
    "            (self.eigenvectors, np.sqrt(np.diag(self.eigenvalues)), self.parameters)\n",
    "        )\n",
    "\n",
    "    def plot(self, lognormal=True):\n",
    "        \"\"\"\n",
    "        Plot the random field.\n",
    "        \"\"\"\n",
    "\n",
    "        if lognormal:\n",
    "            random_field = self.random_field\n",
    "            contour_levels = np.linspace(min(random_field), max(random_field), 20)\n",
    "        else:\n",
    "            random_field = np.exp(self.random_field)\n",
    "            contour_levels = np.linspace(min(random_field), max(random_field), 20)\n",
    "\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.tricontourf(\n",
    "            self.coords[:, 0],\n",
    "            self.coords[:, 1],\n",
    "            random_field,\n",
    "            levels=contour_levels,\n",
    "            cmap=\"plasma\",\n",
    "        )\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class Matern52(SquaredExponential):\n",
    "    def assemble_covariance_matrix(self):\n",
    "        \"\"\"\n",
    "        This class inherits from RandomProcess and creates a Matern 5/2 covariance matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute scaled distances.\n",
    "        dist = np.sqrt(5) * distance_matrix(self.coords, self.coords) / self.lamb\n",
    "\n",
    "        # Set up Matern 5/2 covariance matrix.\n",
    "        self.cov = (1 + dist + dist**2 / 3) * np.exp(-dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Gravity model and generate data\n",
    "This is a bit lengthy due to the model used in this case, it contains class definitions and also instantiation of class objects and data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model parameters.\n",
    "depth = 0.1\n",
    "n_quad = 64\n",
    "n_data = 64\n",
    "\n",
    "# noise level\n",
    "noise_level = 0.02\n",
    "\n",
    "# Set random process parameters.\n",
    "lamb = 0.1\n",
    "mkl = 14\n",
    "\n",
    "# Set the quadrature degree for each model level (coarsest first)\n",
    "n_quadrature = [16, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gravity:\n",
    "    \"\"\"\n",
    "    Gravity is a class that implements a simple gravity surveying problem,\n",
    "    as described in Hansen, P. C. (2010). Discrete Inverse Problems: Insight and Algorithms.\n",
    "    Society for Industrial and Applied Mathematics.\n",
    "    It uses midpoint quadrature to evaluate a Fredholm integral of the first kind.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f_function, depth, n_quad, n_data):\n",
    "        # Set the function describing the distribution of subsurface density.\n",
    "        self.f_function = f_function\n",
    "\n",
    "        # Set the depth of the density (distance to the surface measurements).\n",
    "        self.depth = depth\n",
    "\n",
    "        # Set the quadrature degree along one dimension.\n",
    "        self.n_quad = n_quad\n",
    "\n",
    "        # Set the number of data points along one dimension\n",
    "        self.n_data = n_data\n",
    "\n",
    "        # Set the quadrature points.\n",
    "        x = np.linspace(0, 1, self.n_quad + 1)\n",
    "        self.tx = (x[1:] + x[:-1]) / 2\n",
    "        y = np.linspace(0, 1, self.n_quad + 1)\n",
    "        self.ty = (y[1:] + y[:-1]) / 2\n",
    "        TX, TY = np.meshgrid(self.tx, self.ty)\n",
    "\n",
    "        # Set the measurement points.\n",
    "        self.sx = np.linspace(0, 1, self.n_data)\n",
    "        self.sy = np.linspace(0, 1, self.n_data)\n",
    "        SX, SY = np.meshgrid(self.sx, self.sy)\n",
    "\n",
    "        # Create coordinate vectors.\n",
    "        self.T_coords = np.c_[TX.ravel(), TY.ravel(), np.zeros(self.n_quad**2)]\n",
    "        self.S_coords = np.c_[SX.ravel(), SY.ravel(), self.depth * np.ones(self.n_data**2)]\n",
    "\n",
    "        # Set the quadrature weights.\n",
    "        self.w = 1 / self.n_quad**2\n",
    "\n",
    "        # Compute a distance matrix\n",
    "        dist = distance_matrix(self.S_coords, self.T_coords)\n",
    "\n",
    "        # Create the Fremholm kernel.\n",
    "        self.K = self.w * self.depth / dist**3\n",
    "\n",
    "        # Evaluate the density function on the quadrature points.\n",
    "        self.f = self.f_function(TX, TY).flatten()\n",
    "\n",
    "        # Compute the surface density (noiseless measurements)\n",
    "        self.g = np.dot(self.K, self.f)\n",
    "\n",
    "    def plot_model(self):\n",
    "        # Plot the density and the signal.\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        axes[0].set_title(\"Density\")\n",
    "        f = axes[0].imshow(\n",
    "            self.f.reshape(self.n_quad, self.n_quad),\n",
    "            extent=(0, 1, 0, 1),\n",
    "            origin=\"lower\",\n",
    "            cmap=\"plasma\",\n",
    "        )\n",
    "        fig.colorbar(f, ax=axes[0])\n",
    "        axes[1].set_title(\"Signal\")\n",
    "        g = axes[1].imshow(\n",
    "            self.g.reshape(self.n_data, self.n_data),\n",
    "            extent=(0, 1, 0, 1),\n",
    "            origin=\"lower\",\n",
    "            cmap=\"plasma\",\n",
    "        )\n",
    "        fig.colorbar(g, ax=axes[1])\n",
    "        plt.show()\n",
    "\n",
    "    def plot_kernel(self):\n",
    "        # Plot the kernel.\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(self.K, cmap=\"plasma\")\n",
    "        plt.colorbar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function describing the subsurface density.\n",
    "def f(TX, TY):\n",
    "    f = np.sin(np.pi * TX) + np.sin(3 * np.pi * TY) + TY + 1\n",
    "    f = f / f.max()\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a model\n",
    "model_true = Gravity(f, depth, n_quad, n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_true.plot_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the data.\n",
    "np.random.seed(123)\n",
    "noise = np.random.normal(0, noise_level, n_data**2)\n",
    "data = model_true.g + noise\n",
    "\n",
    "# Plot the density and the signal.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "axes[0].set_title(\"Noiseless Signal\")\n",
    "g = axes[0].imshow(\n",
    "    model_true.g.reshape(n_data, n_data),\n",
    "    extent=(0, 1, 0, 1),\n",
    "    origin=\"lower\",\n",
    "    cmap=\"plasma\",\n",
    ")\n",
    "fig.colorbar(g, ax=axes[0])\n",
    "axes[1].set_title(\"Noisy Signal\")\n",
    "d = axes[1].imshow(data.reshape(n_data, n_data), extent=(0, 1, 0, 1), origin=\"lower\", cmap=\"plasma\")\n",
    "fig.colorbar(d, ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gravity_Forward(Gravity):\n",
    "    \"\"\"\n",
    "    Gravity forward is a class that implements the gravity problem,\n",
    "    but computation of signal and density is delayed to the \"solve\"\n",
    "    method, since it relied on a Gaussian Random Field to model\n",
    "    the (unknown) density.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, depth, n_quad, n_data):\n",
    "        # Set the depth of the density (distance to the surface measurements).\n",
    "        self.depth = depth\n",
    "\n",
    "        # Set the quadrature degree along one axis.\n",
    "        self.n_quad = n_quad\n",
    "\n",
    "        # Set the number of data points along one axis.\n",
    "        self.n_data = n_data\n",
    "\n",
    "        # Set the quadrature points.\n",
    "        x = np.linspace(0, 1, self.n_quad + 1)\n",
    "        self.tx = (x[1:] + x[:-1]) / 2\n",
    "        y = np.linspace(0, 1, self.n_quad + 1)\n",
    "        self.ty = (y[1:] + y[:-1]) / 2\n",
    "        TX, TY = np.meshgrid(self.tx, self.ty)\n",
    "\n",
    "        # Set the measurement points.\n",
    "        self.sx = np.linspace(0, 1, self.n_data)\n",
    "        self.sy = np.linspace(0, 1, self.n_data)\n",
    "        SX, SY = np.meshgrid(self.sx, self.sy)\n",
    "\n",
    "        # Create coordinate vectors.\n",
    "        self.T_coords = np.c_[TX.ravel(), TY.ravel(), np.zeros(self.n_quad**2)]\n",
    "        self.S_coords = np.c_[SX.ravel(), SY.ravel(), self.depth * np.ones(self.n_data**2)]\n",
    "\n",
    "        # Set the quadrature weights.\n",
    "        self.w = 1 / self.n_quad**2\n",
    "\n",
    "        # Compute a distance matrix\n",
    "        dist = distance_matrix(self.S_coords, self.T_coords)\n",
    "\n",
    "        # Create the Fremholm kernel.\n",
    "        self.K = self.w * self.depth / dist**3\n",
    "\n",
    "    def set_random_process(self, random_process, lamb, mkl):\n",
    "        # Set the number of KL modes.\n",
    "        self.mkl = mkl\n",
    "\n",
    "        # Initialise a random process on the quadrature points.\n",
    "        # and compute the eigenpairs of the covariance matrix,\n",
    "        self.random_process = random_process(self.T_coords, self.mkl, lamb)\n",
    "        self.random_process.compute_eigenpairs()\n",
    "\n",
    "    def solve(self, parameters):\n",
    "        # Internalise the Random Field parameters\n",
    "        self.parameters = parameters\n",
    "\n",
    "        # Create a realisation of the random process, given the parameters.\n",
    "        self.random_process.generate(self.parameters)\n",
    "        mean = 0.0\n",
    "        stdev = 1.0\n",
    "\n",
    "        # Set the density.\n",
    "        self.f = mean + stdev * self.random_process.random_field\n",
    "\n",
    "        # Compute the signal.\n",
    "        self.g = np.dot(self.K, self.f)\n",
    "\n",
    "    def get_data(self):\n",
    "        # Get the data vector.\n",
    "        return self.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We project the eigenmodes of the fine model to the quadrature points\n",
    "# of the coarse model using linear interpolation.\n",
    "def project_eigenmodes(model_coarse, model_fine):\n",
    "    model_coarse.random_process.eigenvalues = model_fine.random_process.eigenvalues\n",
    "    for i in range(model_coarse.mkl):\n",
    "        interpolator = RectBivariateSpline(\n",
    "            model_fine.tx,\n",
    "            model_fine.ty,\n",
    "            model_fine.random_process.eigenvectors[:, i].reshape(\n",
    "                model_fine.n_quad, model_fine.n_quad\n",
    "            ),\n",
    "        )\n",
    "        model_coarse.random_process.eigenvectors[:, i] = interpolator(\n",
    "            model_coarse.tx, model_coarse.ty\n",
    "        ).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the models, according the quadrature degree.\n",
    "my_models = []\n",
    "for i, n_quad in enumerate(n_quadrature):\n",
    "    my_models.append(Gravity_Forward(depth, n_quad, n_data))\n",
    "    my_models[i].set_random_process(Matern52, lamb, mkl)\n",
    "\n",
    "# Project the eigenmodes of the fine model to the coarse model.\n",
    "for m in my_models[:-1]:\n",
    "    project_eigenmodes(m, my_models[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve and plot models to demonstrate coarse/fine difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the same random realisation for each level, and the corresponding signal,\n",
    "# to validate that the levels are equivalents.\n",
    "for i, m in enumerate(my_models):\n",
    "    print(f\"Level {i}:\")\n",
    "    np.random.seed(2)\n",
    "    m.solve(np.random.normal(size=mkl))\n",
    "    m.plot_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f\"Largest {mkl} KL eigenvalues of GP prior\")\n",
    "plt.plot(my_models[-1].random_process.eigenvalues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare computation cost of coarse and fine model solve\n",
    "The bigger the difference in time, the more MLDA has potential to increase efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "my_models[0].solve(np.random.normal(size=mkl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "my_models[-1].solve(np.random.normal(size=mkl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set MCMC parameters for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of draws from the distribution\n",
    "ndraws = 15000\n",
    "\n",
    "# Number of burn-in samples\n",
    "nburn = 10000\n",
    "\n",
    "# MLDA and Metropolis tuning parameters\n",
    "tune = True\n",
    "tune_interval = 100  # Set high to prevent tuning.\n",
    "discard_tuning = True\n",
    "\n",
    "# Number of independent chains.\n",
    "nchains = 3\n",
    "\n",
    "# Subsampling rate for MLDA\n",
    "nsub = 5\n",
    "\n",
    "# Set prior parameters for multivariate Gaussian prior distribution.\n",
    "mu_prior = np.zeros(mkl)\n",
    "cov_prior = np.eye(mkl)\n",
    "\n",
    "# Set the sigma for inference.\n",
    "sigma = 1.0\n",
    "\n",
    "# Sampling seed\n",
    "sampling_seed = RANDOM_SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Theano Op for the likelihood\n",
    "This creates the theano op needed to pass the above model to the PyMC3 sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loglik(my_model, theta, data, sigma):\n",
    "    \"\"\"\n",
    "    This returns the log-likelihood of my_model given theta,\n",
    "    datapoints, the observed data and sigma. It uses the\n",
    "    model_wrapper function to do a model solve.\n",
    "    \"\"\"\n",
    "    my_model.solve(theta)\n",
    "    output = my_model.get_data()\n",
    "    return -(0.5 / sigma**2) * np.sum((output - data) ** 2)\n",
    "\n",
    "\n",
    "class LogLike(tt.Op):\n",
    "    \"\"\"\n",
    "    Theano Op that wraps the log-likelihood computation, necessary to\n",
    "    pass \"black-box\" code into pymc3.\n",
    "    Based on the work in:\n",
    "    https://docs.pymc.io/notebooks/blackbox_external_likelihood.html\n",
    "    https://docs.pymc.io/Advanced_usage_of_Theano_in_PyMC3.html\n",
    "    \"\"\"\n",
    "\n",
    "    # Specify what type of object will be passed and returned to the Op when it is\n",
    "    # called. In our case we will be passing it a vector of values (the parameters\n",
    "    # that define our model and a model object) and returning a single \"scalar\"\n",
    "    # value (the log-likelihood)\n",
    "    itypes = [tt.dvector]  # expects a vector of parameter values when called\n",
    "    otypes = [tt.dscalar]  # outputs a single scalar value (the log likelihood)\n",
    "\n",
    "    def __init__(self, my_model, loglike, data, sigma):\n",
    "        \"\"\"\n",
    "        Initialise the Op with various things that our log-likelihood function\n",
    "        requires. Below are the things that are needed in this particular\n",
    "        example.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        my_model:\n",
    "            A Model object (defined in model.py) that contains the parameters\n",
    "            and functions of out model.\n",
    "        loglike:\n",
    "            The log-likelihood function we've defined, in this example it is\n",
    "            my_loglik.\n",
    "        data:\n",
    "            The \"observed\" data that our log-likelihood function takes in. These\n",
    "            are the true data generated by the finest model in this example.\n",
    "        x:\n",
    "            The dependent variable (aka 'x') that our model requires. This is\n",
    "            the datapoints in this example.\n",
    "        sigma:\n",
    "            The noise standard deviation that our function requires.\n",
    "        \"\"\"\n",
    "        # add inputs as class attributes\n",
    "        self.my_model = my_model\n",
    "        self.likelihood = loglike\n",
    "        self.data = data\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        # the method that is used when calling the Op\n",
    "        theta = inputs  # this will contain my variables\n",
    "\n",
    "        # call the log-likelihood function\n",
    "        logl = self.likelihood(self.my_model, theta, self.data, self.sigma)\n",
    "\n",
    "        outputs[0][0] = np.array(logl)  # output the log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Theano Ops to wrap likelihoods of all model levels and store them in list\n",
    "logl = []\n",
    "for i, m_i in enumerate(my_models):\n",
    "    logl.append(LogLike(m_i, my_loglik, data, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create coarse model in PyMC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up models in pymc3 for each level - excluding finest model level\n",
    "coarse_models = []\n",
    "for j in range(len(my_models) - 1):\n",
    "    with pm.Model() as model:\n",
    "        # Multivariate normal prior.\n",
    "        theta = pm.MvNormal(\"theta\", mu=mu_prior, cov=cov_prior, shape=mkl)\n",
    "\n",
    "        # Use the Potential class to evaluate likelihood\n",
    "        pm.Potential(\"likelihood\", logl[j](theta))\n",
    "\n",
    "    coarse_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fine model and perform inference\n",
    "Note that we sample using all three methods and that we use the MAP as the starting point for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up finest model and perform inference with PyMC3, using the MLDA algorithm\n",
    "# and passing the coarse_models list created above.\n",
    "method_names = []\n",
    "traces = []\n",
    "runtimes = []\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # Multivariate normal prior.\n",
    "    theta = pm.MvNormal(\"theta\", mu=mu_prior, cov=cov_prior, shape=mkl)\n",
    "\n",
    "    # Use the Potential class to evaluate likelihood\n",
    "    pm.Potential(\"likelihood\", logl[-1](theta))\n",
    "\n",
    "    # Find the MAP estimate which is used as the starting point for sampling\n",
    "    MAP = pm.find_MAP()\n",
    "\n",
    "    # Initialise a Metropolis, DEMetropolisZ and MLDA step method objects (passing the subsampling rate and\n",
    "    # coarse models list for the latter)\n",
    "    step_metropolis = pm.Metropolis(tune=tune, tune_interval=tune_interval)\n",
    "    step_demetropolisz = pm.DEMetropolisZ(tune_interval=tune_interval)\n",
    "    step_mlda = pm.MLDA(\n",
    "        coarse_models=coarse_models, subsampling_rates=nsub, base_tune_interval=tune_interval\n",
    "    )\n",
    "\n",
    "    # Inference!\n",
    "    # Metropolis\n",
    "    t_start = time.time()\n",
    "    method_names.append(\"Metropolis\")\n",
    "    traces.append(\n",
    "        pm.sample(\n",
    "            draws=ndraws,\n",
    "            step=step_metropolis,\n",
    "            chains=nchains,\n",
    "            tune=nburn,\n",
    "            discard_tuned_samples=discard_tuning,\n",
    "            random_seed=sampling_seed,\n",
    "            start=MAP,\n",
    "            cores=1,\n",
    "            mp_ctx=\"forkserver\",\n",
    "        )\n",
    "    )\n",
    "    runtimes.append(time.time() - t_start)\n",
    "\n",
    "    # DEMetropolisZ\n",
    "    t_start = time.time()\n",
    "    method_names.append(\"DEMetropolisZ\")\n",
    "    traces.append(\n",
    "        pm.sample(\n",
    "            draws=ndraws,\n",
    "            step=step_demetropolisz,\n",
    "            chains=nchains,\n",
    "            tune=nburn,\n",
    "            discard_tuned_samples=discard_tuning,\n",
    "            random_seed=sampling_seed,\n",
    "            start=MAP,\n",
    "            cores=1,\n",
    "            mp_ctx=\"forkserver\",\n",
    "        )\n",
    "    )\n",
    "    runtimes.append(time.time() - t_start)\n",
    "\n",
    "    # MLDA\n",
    "    t_start = time.time()\n",
    "    method_names.append(\"MLDA\")\n",
    "    traces.append(\n",
    "        pm.sample(\n",
    "            draws=ndraws,\n",
    "            step=step_mlda,\n",
    "            chains=nchains,\n",
    "            tune=nburn,\n",
    "            discard_tuned_samples=discard_tuning,\n",
    "            random_seed=sampling_seed,\n",
    "            start=MAP,\n",
    "            cores=1,\n",
    "            mp_ctx=\"forkserver\",\n",
    "        )\n",
    "    )\n",
    "    runtimes.append(time.time() - t_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get post-sampling stats and diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print MAP estimate and pymc3 sampling summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    print(\n",
    "        f\"\\nDetailed summaries and plots:\\nMAP estimate: {MAP['theta']}. Not used as starting point.\"\n",
    "    )\n",
    "    for i, trace in enumerate(traces):\n",
    "        print(f\"\\n{method_names[i]} Sampler:\\n\")\n",
    "        display(az.summary(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show ESS and ESS/sec for all samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "ess = []\n",
    "ess_n = []\n",
    "performances = []\n",
    "\n",
    "# Get some more statistics.\n",
    "with model:\n",
    "    for i, trace in enumerate(traces):\n",
    "        acc.append(trace.get_sampler_stats(\"accepted\").mean())\n",
    "        ess.append(np.array(az.ess(trace).to_array()))\n",
    "        ess_n.append(ess[i] / len(trace) / trace.nchains)\n",
    "        performances.append(ess[i] / runtimes[i])\n",
    "        print(\n",
    "            f\"\\n{method_names[i]} Sampler: {len(trace)} drawn samples in each of \"\n",
    "            f\"{trace.nchains} chains.\"\n",
    "            f\"\\nRuntime: {runtimes[i]} seconds\"\n",
    "            f\"\\nAcceptance rate: {acc[i]}\"\n",
    "            f\"\\nESS list: {np.round(ess[i][0], 3)}\"\n",
    "            f\"\\nNormalised ESS list: {np.round(ess_n[i][0], 3)}\"\n",
    "            f\"\\nESS/sec: {np.round(performances[i][0], 3)}\"\n",
    "        )\n",
    "\n",
    "    # Plot the effective sample size (ESS) and relative ESS (ES/sec) of each of the sampling strategies.\n",
    "    colors = [\"firebrick\", \"darkgoldenrod\", \"darkcyan\", \"olivedrab\"]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    axes[0].set_title(\"ESS\")\n",
    "    for i, e in enumerate(ess):\n",
    "        axes[0].bar(\n",
    "            [j + i * 0.2 for j in range(mkl)],\n",
    "            e.ravel(),\n",
    "            width=0.2,\n",
    "            color=colors[i],\n",
    "            label=method_names[i],\n",
    "        )\n",
    "    axes[0].set_xticks([i + 0.3 for i in range(mkl)])\n",
    "    axes[0].set_xticklabels([f\"theta_{i}\" for i in range(mkl)])\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].set_title(\"ES/sec\")\n",
    "    for i, p in enumerate(performances):\n",
    "        axes[1].bar(\n",
    "            [j + i * 0.2 for j in range(mkl)],\n",
    "            p.ravel(),\n",
    "            width=0.2,\n",
    "            color=colors[i],\n",
    "            label=method_names[i],\n",
    "        )\n",
    "    axes[1].set_xticks([i + 0.3 for i in range(mkl)])\n",
    "    axes[1].set_xticklabels([f\"theta_{i}\" for i in range(mkl)])\n",
    "    axes[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot distributions and trace.\n",
    "Vertical grey lines represent the MAP estimate of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    lines = ((\"theta\", {}, MAP[\"theta\"].tolist()),)\n",
    "    for i, trace in enumerate(traces):\n",
    "        az.plot_trace(trace, lines=lines)\n",
    "\n",
    "        # Ugly hack to get some titles in.\n",
    "        x_offset = -0.1 * ndraws\n",
    "        y_offset = trace.get_values(\"theta\").max() + 0.25 * (\n",
    "            trace.get_values(\"theta\").max() - trace.get_values(\"theta\").min()\n",
    "        )\n",
    "        plt.text(x_offset, y_offset, \"{} Sampler\".format(method_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot true and recovered densities\n",
    "This is useful for verification, i.e. to compare the true model density and signal to the estimated ones from the samplers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Model\")\n",
    "model_true.plot_model()\n",
    "with model:\n",
    "    print(\"MAP estimate:\")\n",
    "    my_models[-1].solve(MAP[\"theta\"])\n",
    "    my_models[-1].plot_model()\n",
    "    for i, t in enumerate(traces):\n",
    "        print(f\"Recovered by: {method_names[i]}\")\n",
    "        my_models[-1].solve(az.summary(t)[\"mean\"].values)\n",
    "        my_models[-1].plot_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show trace of lowest energy mode for Metropolis sampler\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(traces[0][\"theta\"][:5000, -1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show trace of lowest energy mode for MLDA sampler\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(traces[2][\"theta\"][:5000:, -1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure samplers have converged\n",
    "assert all(az.rhat(traces[0]) < 1.03)\n",
    "assert all(az.rhat(traces[1]) < 1.03)\n",
    "assert all(az.rhat(traces[2]) < 1.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python PyMC3 (Dev)",
   "language": "python",
   "name": "pymc3-dev-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
