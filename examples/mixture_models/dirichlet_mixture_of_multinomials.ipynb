{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(dirichlet_mixture_of_multinomials)=\n",
    "# Dirichlet mixtures of multinomials\n",
    "\n",
    ":::{post} Jan 8, 2022\n",
    ":tags: mixture model, \n",
    ":category: advanced\n",
    ":author: Byron J. Smith, Abhipsha Das, Oriol Abril-Pla\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example notebook demonstrates the use of a\n",
    "Dirichlet mixture of multinomials\n",
    "(a.k.a [Dirichlet-multinomial](https://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution) or DM)\n",
    "to model categorical count data.\n",
    "Models like this one are important in a variety of areas, including\n",
    "natural language processing, ecology, bioinformatics, and more.\n",
    "\n",
    "The Dirichlet-multinomial can be understood as draws from a [Multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution)\n",
    "where each sample has a slightly different probability vector, which is itself drawn from a common [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution).\n",
    "This contrasts with the Multinomial distribution, which assumes that all observations arise from a single fixed probability vector.\n",
    "This enables the Dirichlet-multinomial to accommodate more variable (a.k.a, over-dispersed) count data than the Multinomial.\n",
    "\n",
    "Other examples of over-dispersed count distributions are the\n",
    "[Beta-binomial](https://en.wikipedia.org/wiki/Beta-binomial_distribution)\n",
    "(which can be thought of as a special case of the DM) or the\n",
    "[Negative binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution)\n",
    "distributions.\n",
    "\n",
    "The DM is also an example of marginalizing\n",
    "a mixture distribution over its latent parameters.\n",
    "This notebook will demonstrate the performance benefits that come from taking that approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:00:40.367769Z",
     "start_time": "2021-01-25T18:00:37.359820Z"
    }
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import scipy as sp\n",
    "\n",
    "print(f\"Running on PyMC v{pm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:00:40.406019Z",
     "start_time": "2021-01-25T18:00:40.400553Z"
    }
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "RANDOM_SEED = 8927\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us simulate some over-dispersed, categorical count data\n",
    "for this example.\n",
    "\n",
    "Here we are simulating from the DM distribution itself,\n",
    "so it is perhaps tautological to fit that model,\n",
    "but rest assured that data like these really do appear in\n",
    "the counts of different:\n",
    "\n",
    "1. words in text corpuses {cite:p}`madsen2005modelingdirichlet`,\n",
    "2. types of RNA molecules in a cell {cite:p}`nowicka2016drimseq`,\n",
    "3. items purchased by shoppers {cite:p}`goodhardt1984thedirichlet`.\n",
    "\n",
    "Here we will discuss a community ecology example, pretending that we have observed counts of $k=5$ different\n",
    "tree species in $n=10$ different forests.\n",
    "\n",
    "Our simulation will produce a two-dimensional matrix of integers (counts)\n",
    "where each row, (zero-)indexed by $i \\in (0...n-1)$, is an observation (different forest), and each\n",
    "column $j \\in (0...k-1)$ is a category (tree species).\n",
    "We'll parameterize this distribution with three things:\n",
    "- $\\mathrm{frac}$ : the expected fraction of each species,\n",
    "  a $k$-dimensional vector on the simplex (i.e. sums-to-one)\n",
    "- $\\mathrm{total\\_count}$ : the total number of items tallied in each observation,\n",
    "- $\\mathrm{conc}$ : the concentration, controlling the overdispersion of our data,\n",
    "  where larger values result in our distribution more closely approximating the multinomial.\n",
    "  \n",
    "Here, and throughout this notebook, we've used a\n",
    "[convenient reparameterization](https://mc-stan.org/docs/2_26/stan-users-guide/reparameterizations.html#dirichlet-priors)\n",
    "of the Dirichlet distribution\n",
    "from one to two parameters,\n",
    "$\\alpha=\\mathrm{conc} \\times \\mathrm{frac}$, as this\n",
    "fits our desired interpretation.\n",
    "  \n",
    "Each observation from the DM is simulated by:\n",
    "1. first obtaining a value on the $k$-simplex simulated as\n",
    "   $p_i \\sim \\mathrm{Dirichlet}(\\alpha=\\mathrm{conc} \\times \\mathrm{frac})$,\n",
    "2. and then simulating $\\mathrm{counts}_i \\sim \\mathrm{Multinomial}(\\mathrm{total\\_count}, p_i)$.\n",
    "\n",
    "Notice that each observation gets its _own_\n",
    "latent parameter $p_i$, simulated independently from\n",
    "a common Dirichlet distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:00:40.448021Z",
     "start_time": "2021-01-25T18:00:40.422607Z"
    }
   },
   "outputs": [],
   "source": [
    "true_conc = 6.0\n",
    "true_frac = np.array([0.45, 0.30, 0.15, 0.09, 0.01])\n",
    "trees = [\"pine\", \"oak\", \"ebony\", \"rosewood\", \"mahogany\"]  # Tree species observed\n",
    "# fmt: off\n",
    "forests = [  # Forests observed\n",
    "    \"sunderbans\", \"amazon\", \"arashiyama\", \"trossachs\", \"valdivian\",\n",
    "    \"bosc de poblet\", \"font groga\", \"monteverde\", \"primorye\", \"daintree\",\n",
    "]\n",
    "# fmt: on\n",
    "k = len(trees)\n",
    "n = len(forests)\n",
    "total_count = 50\n",
    "\n",
    "true_p = sp.stats.dirichlet(true_conc * true_frac).rvs(size=n, random_state=rng)\n",
    "observed_counts = np.vstack(\n",
    "    [sp.stats.multinomial(n=total_count, p=p_i).rvs(random_state=rng) for p_i in true_p]\n",
    ")\n",
    "\n",
    "observed_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model that we will fit to these data is a plain\n",
    "multinomial model, where the only parameter is the\n",
    "expected fraction of each category, $\\mathrm{frac}$, which we will give a Dirichlet prior.\n",
    "While the uniform prior ($\\alpha_j=1$ for each $j$) works well, if we have independent beliefs about the fraction of each tree,\n",
    "we could encode this into our prior, e.g.\n",
    "increasing the value of $\\alpha_j$ where we expect a higher fraction of species-$j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:00:49.504137Z",
     "start_time": "2021-01-25T18:00:40.451892Z"
    }
   },
   "outputs": [],
   "source": [
    "coords = {\"tree\": trees, \"forest\": forests}\n",
    "with pm.Model(coords=coords) as model_multinomial:\n",
    "    frac = pm.Dirichlet(\"frac\", a=np.ones(k), dims=\"tree\")\n",
    "    counts = pm.Multinomial(\n",
    "        \"counts\", n=total_count, p=frac, observed=observed_counts, dims=(\"forest\", \"tree\")\n",
    "    )\n",
    "\n",
    "pm.model_to_graphviz(model_multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:01:10.459503Z",
     "start_time": "2021-01-25T18:00:49.507208Z"
    }
   },
   "outputs": [],
   "source": [
    "with model_multinomial:\n",
    "    trace_multinomial = pm.sample(chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:01:11.092547Z",
     "start_time": "2021-01-25T18:01:10.461088Z"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(data=trace_multinomial, var_names=[\"frac\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace plots look fairly good; visually, each parameter appears to be moving around the posterior well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:01:11.252668Z",
     "start_time": "2021-01-25T18:01:11.095502Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_multinomial = az.summary(trace_multinomial, var_names=[\"frac\"])\n",
    "\n",
    "summary_multinomial = summary_multinomial.assign(\n",
    "    ess_bulk_per_sec=lambda x: x.ess_bulk / trace_multinomial.posterior.sampling_time,\n",
    ")\n",
    "\n",
    "summary_multinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, diagnostics in the parameter summary table all look fine.\n",
    "Here we've added a column estimating the effective sample size per\n",
    "second of sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:01:11.556554Z",
     "start_time": "2021-01-25T18:01:11.254663Z"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_forest(trace_multinomial, var_names=[\"frac\"])\n",
    "for j, (y_tick, frac_j) in enumerate(zip(plt.gca().get_yticks(), reversed(true_frac))):\n",
    "    plt.vlines(frac_j, ymin=y_tick - 0.45, ymax=y_tick + 0.45, color=\"black\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've drawn a forest-plot, showing the mean and 94% HDIs from our posterior approximation.\n",
    "Interestingly, because we know what the underlying\n",
    "frequencies are for each species (dashed lines), we can comment on the accuracy\n",
    "of our inferences.\n",
    "And now the issues with our model become apparent;\n",
    "notice that the 94% HDIs _don't include the true values_ for\n",
    "tree species 0, 1, 3.\n",
    "We might have seen _one_ HDI miss, but _three_???\n",
    "\n",
    "...what's going on?\n",
    "\n",
    "Let's troubleshoot this model using a posterior-predictive check, comparing our data to simulated data conditioned on our posterior estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:01:21.826527Z",
     "start_time": "2021-01-25T18:01:11.558091Z"
    }
   },
   "outputs": [],
   "source": [
    "with model_multinomial:\n",
    "    pp_samples = pm.sample_posterior_predictive(trace=trace_multinomial)\n",
    "\n",
    "# Concatenate with InferenceData object\n",
    "trace_multinomial.extend(pp_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:01:22.786984Z",
     "start_time": "2021-01-25T18:01:21.828155Z"
    }
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "fig, axs = plt.subplots(k, 1, sharex=True, sharey=True, figsize=(6, 8))\n",
    "for j, ax in enumerate(axs):\n",
    "    c = cmap(j)\n",
    "    ax.hist(\n",
    "        trace_multinomial.posterior_predictive.counts.sel(tree=trees[j]).values.flatten(),\n",
    "        bins=np.arange(total_count),\n",
    "        histtype=\"step\",\n",
    "        color=c,\n",
    "        density=True,\n",
    "        label=\"Post.Pred.\",\n",
    "    )\n",
    "    ax.hist(\n",
    "        (trace_multinomial.observed_data.counts.sel(tree=trees[j]).values.flatten()),\n",
    "        bins=np.arange(total_count),\n",
    "        color=c,\n",
    "        density=True,\n",
    "        alpha=0.25,\n",
    "        label=\"Observed\",\n",
    "    )\n",
    "    ax.axvline(\n",
    "        true_frac[j] * total_count,\n",
    "        color=c,\n",
    "        lw=1.0,\n",
    "        alpha=0.45,\n",
    "        label=\"True\",\n",
    "    )\n",
    "    ax.annotate(\n",
    "        f\"{trees[j]}\",\n",
    "        xy=(0.96, 0.9),\n",
    "        xycoords=\"axes fraction\",\n",
    "        ha=\"right\",\n",
    "        va=\"top\",\n",
    "        color=c,\n",
    "    )\n",
    "\n",
    "axs[-1].legend(loc=\"upper center\", fontsize=10)\n",
    "axs[-1].set_xlabel(\"Count\")\n",
    "axs[-1].set_yticks([0, 0.5, 1.0])\n",
    "axs[-1].set_ylim(0, 0.6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're plotting histograms of the predicted counts\n",
    "against the observed counts for each species.\n",
    "\n",
    "_(Notice that the y-axis isn't full height and clips the distributions for species ``mahogany`` in purple.)_\n",
    "\n",
    "And now we can start to see why our posterior HDI deviates from the _true_ parameters for three of five species (vertical lines).\n",
    "See that for all of the species the observed counts are frequently quite far from the predictions\n",
    "conditioned on the posterior distribution.\n",
    "This is particularly obvious for (e.g.) ``oak`` where we have one observation of more than 30\n",
    "trees of this species, despite the posterior predicitive mass being concentrated far below that.\n",
    "\n",
    "This is overdispersion at work, and a clear sign that we need to adjust our model to accommodate it.\n",
    "\n",
    "Posterior predictive checks are one of the best ways to diagnose model misspecification,\n",
    "and this example is no different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet-Multinomial Model - Explicit Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and model our data using the DM distribution.\n",
    "\n",
    "For this model we'll keep the same prior on the expected frequencies of each\n",
    "species, $\\mathrm{frac}$.\n",
    "We'll also add a strictly positive parameter, $\\mathrm{conc}$, for the concentration.\n",
    "\n",
    "In this iteration of our model we'll explicitly include the latent multinomial\n",
    "probability, $p_i$, modeling the $\\mathrm{true\\_p}_i$ from our simulations (which we would not\n",
    "observe in the real world)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:01:26.125500Z",
     "start_time": "2021-01-25T18:01:22.788748Z"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as model_dm_explicit:\n",
    "    frac = pm.Dirichlet(\"frac\", a=np.ones(k), dims=\"tree\")\n",
    "    conc = pm.Lognormal(\"conc\", mu=1, sigma=1)\n",
    "    p = pm.Dirichlet(\"p\", a=frac * conc, dims=(\"forest\", \"tree\"))\n",
    "    counts = pm.Multinomial(\n",
    "        \"counts\", n=total_count, p=p, observed=observed_counts, dims=(\"forest\", \"tree\")\n",
    "    )\n",
    "\n",
    "pm.model_to_graphviz(model_dm_explicit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T00:26:07.224052Z",
     "start_time": "2021-01-23T00:25:28.046396Z"
    }
   },
   "source": [
    "Compare this diagram to the first.\n",
    "Here the latent, Dirichlet distributed $p$ separates the multinomial from the expected frequencies, $\\mathrm{frac}$,\n",
    "accounting for overdispersion of counts relative to the simple multinomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:04:45.280081Z",
     "start_time": "2021-01-25T18:01:26.144038Z"
    }
   },
   "outputs": [],
   "source": [
    "with model_dm_explicit:\n",
    "    trace_dm_explicit = pm.sample(chains=4, target_accept=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we had to increase ``target_accept`` from 0.8 to 0.9 to not get drowned in divergences. \n",
    "\n",
    "We also got a warning about the ``rhat`` statistic, although we'll ignore it for now.\n",
    "More interesting is how much longer it took to sample this model than the first.\n",
    "This is partly because our model has an additional ~$(n \\times k)$ parameters,\n",
    "but it seems like there are other geometric challenges for NUTS as well.\n",
    "\n",
    "We'll see if we can fix these in the next model, but for now let's take a look at the traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:04:46.373078Z",
     "start_time": "2021-01-25T18:04:45.282240Z"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(data=trace_dm_explicit, var_names=[\"frac\", \"conc\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The divergences seem to occur when the estimated fraction of the rare species (``mahogany``) is very close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:04:46.716191Z",
     "start_time": "2021-01-25T18:04:46.388521Z"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_forest(trace_dm_explicit, var_names=[\"frac\"])\n",
    "for j, (y_tick, frac_j) in enumerate(zip(plt.gca().get_yticks(), reversed(true_frac))):\n",
    "    plt.vlines(frac_j, ymin=y_tick - 0.45, ymax=y_tick + 0.45, color=\"black\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, since we know the ground-truth for $\\mathrm{frac}$,\n",
    "we can congratulate ourselves that\n",
    "the HDIs include the true values for all of our species!\n",
    "\n",
    "Modeling this mixture has made our inferences robust to the overdispersion of counts,\n",
    "while the plain multinomial is very sensitive.\n",
    "Notice that the HDI is much wider than before for each $\\mathrm{frac}_i$.\n",
    "In this case that makes the difference between correct and incorrect inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:04:47.203336Z",
     "start_time": "2021-01-25T18:04:46.718860Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_dm_explicit = az.summary(trace_dm_explicit, var_names=[\"frac\", \"conc\"])\n",
    "summary_dm_explicit = summary_dm_explicit.assign(\n",
    "    ess_bulk_per_sec=lambda x: x.ess_bulk / trace_dm_explicit.posterior.sampling_time,\n",
    ")\n",
    "\n",
    "summary_dm_explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, but _we can do better_.\n",
    "The slightly too large $\\hat{R}$ value for ``frac[mahogany]`` is a bit concerning, and it's surprising\n",
    "that our $\\mathrm{ESS} \\; \\mathrm{sec}^{-1}$ is quite small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet-Multinomial Model - Marginalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happily, the Dirichlet distribution is conjugate to the multinomial\n",
    "and therefore there's a convenient, closed-form for the marginalized\n",
    "distribution, i.e. the Dirichlet-multinomial distribution, which was added to PyMC in [3.11.0](https://github.com/pymc-devs/pymc3/releases/tag/v3.11.0).\n",
    "\n",
    "Let's take advantage of this, marginalizing out the explicit latent parameter, $p_i$,\n",
    "replacing the combination of this node and the multinomial\n",
    "with the DM to make an equivalent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:04:48.312788Z",
     "start_time": "2021-01-25T18:04:47.230311Z"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as model_dm_marginalized:\n",
    "    frac = pm.Dirichlet(\"frac\", a=np.ones(k), dims=\"tree\")\n",
    "    conc = pm.Lognormal(\"conc\", mu=1, sigma=1)\n",
    "    counts = pm.DirichletMultinomial(\n",
    "        \"counts\", n=total_count, a=frac * conc, observed=observed_counts, dims=(\"forest\", \"tree\")\n",
    "    )\n",
    "\n",
    "pm.model_to_graphviz(model_dm_marginalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T23:27:10.619459Z",
     "start_time": "2021-01-22T23:27:10.508488Z"
    }
   },
   "source": [
    "The plate diagram shows that we've collapsed what had been the latent Dirichlet and the multinomial\n",
    "nodes together into a single DM node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:05:28.426056Z",
     "start_time": "2021-01-25T18:04:48.315555Z"
    }
   },
   "outputs": [],
   "source": [
    "with model_dm_marginalized:\n",
    "    trace_dm_marginalized = pm.sample(chains=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It samples much more quickly and without any of the warnings from before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:05:29.376568Z",
     "start_time": "2021-01-25T18:05:28.428858Z"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(data=trace_dm_marginalized, var_names=[\"frac\", \"conc\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace plots look fuzzy and KDEs are clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:05:29.453552Z",
     "start_time": "2021-01-25T18:05:29.378157Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_dm_marginalized = az.summary(trace_dm_marginalized, var_names=[\"frac\", \"conc\"])\n",
    "summary_dm_marginalized = summary_dm_marginalized.assign(\n",
    "    ess_mean_per_sec=lambda x: x.ess_bulk / trace_dm_marginalized.posterior.sampling_time,\n",
    ")\n",
    "assert all(summary_dm_marginalized.r_hat < 1.03)\n",
    "\n",
    "summary_dm_marginalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $\\hat{R}$ is close to $1$ everywhere\n",
    "and $\\mathrm{ESS} \\; \\mathrm{sec}^{-1}$ is much higher.\n",
    "Our reparameterization (marginalization) has greatly improved the sampling!\n",
    "(And, thankfully, the HDIs look similar to the other model.)\n",
    "\n",
    "This all looks very good, but what if we didn't have the ground-truth?\n",
    "\n",
    "Posterior predictive checks to the rescue (again)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:05:31.487411Z",
     "start_time": "2021-01-25T18:05:29.455091Z"
    }
   },
   "outputs": [],
   "source": [
    "with model_dm_marginalized:\n",
    "    pp_samples = pm.sample_posterior_predictive(trace_dm_marginalized)\n",
    "\n",
    "# Concatenate with InferenceData object\n",
    "trace_dm_marginalized.extend(pp_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:05:33.684720Z",
     "start_time": "2021-01-25T18:05:31.489425Z"
    }
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "fig, axs = plt.subplots(k, 2, sharex=True, sharey=True, figsize=(8, 8))\n",
    "for j, row in enumerate(axs):\n",
    "    c = cmap(j)\n",
    "    for _trace, ax in zip([trace_dm_marginalized, trace_multinomial], row):\n",
    "        ax.hist(\n",
    "            _trace.posterior_predictive.counts.sel(tree=trees[j]).values.flatten(),\n",
    "            bins=np.arange(total_count),\n",
    "            histtype=\"step\",\n",
    "            color=c,\n",
    "            density=True,\n",
    "            label=\"Post.Pred.\",\n",
    "        )\n",
    "        ax.hist(\n",
    "            (_trace.observed_data.counts.sel(tree=trees[j]).values.flatten()),\n",
    "            bins=np.arange(total_count),\n",
    "            color=c,\n",
    "            density=True,\n",
    "            alpha=0.25,\n",
    "            label=\"Observed\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            true_frac[j] * total_count,\n",
    "            color=c,\n",
    "            lw=1.0,\n",
    "            alpha=0.45,\n",
    "            label=\"True\",\n",
    "        )\n",
    "    row[1].annotate(\n",
    "        f\"{trees[j]}\",\n",
    "        xy=(0.96, 0.9),\n",
    "        xycoords=\"axes fraction\",\n",
    "        ha=\"right\",\n",
    "        va=\"top\",\n",
    "        color=c,\n",
    "    )\n",
    "\n",
    "axs[-1, -1].legend(loc=\"upper center\", fontsize=10)\n",
    "axs[0, 1].set_title(\"Multinomial\")\n",
    "axs[0, 0].set_title(\"Dirichlet-multinomial\")\n",
    "axs[-1, 0].set_xlabel(\"Count\")\n",
    "axs[-1, 1].set_xlabel(\"Count\")\n",
    "axs[-1, 0].set_yticks([0, 0.5, 1.0])\n",
    "axs[-1, 0].set_ylim(0, 0.6)\n",
    "ax.set_ylim(0, 0.6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T23:47:57.798973Z",
     "start_time": "2021-01-16T23:47:57.655530Z"
    }
   },
   "source": [
    "_(Notice, again, that the y-axis isn't full height, and clips the distributions for ``mahogany`` in purple.)_\n",
    "\n",
    "Compared to the multinomial (plots on the right), PPCs for the DM (left) show that the observed data is\n",
    "an entirely reasonable realization of our model.\n",
    "This is great news!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go a step further and try to put a number on how much better our DM model is\n",
    "relative to the raw multinomial.\n",
    "We'll use leave-one-out cross validation to compare the\n",
    "out-of-sample predictive ability of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:05:33.901820Z",
     "start_time": "2021-01-25T18:05:33.738673Z"
    }
   },
   "outputs": [],
   "source": [
    "with model_multinomial:\n",
    "    pm.compute_log_likelihood(trace_multinomial)\n",
    "\n",
    "with model_dm_marginalized:\n",
    "    pm.compute_log_likelihood(trace_dm_marginalized)\n",
    "\n",
    "az.compare(\n",
    "    {\"multinomial\": trace_multinomial, \"dirichlet_multinomial\": trace_dm_marginalized}, ic=\"loo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the DM outclasses the multinomial by a mile, assigning a weight of 100% to the over-dispersed model.\n",
    "While the ``warning=True`` flag for the multinomial distribution indicates that the numerical value cannot be fully trusted, the large difference in ``elpd_loo`` is further confirmation that between the two, the DM should be greatly favored for prediction, parameter inference, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Obviously the DM is not a perfect model in every case, but it is often a better choice than the multinomial, much more robust while taking on just one additional parameter.\n",
    "\n",
    "There are a number of shortcomings to the DM that we should keep in mind when selecting a model.\n",
    "The biggest problem is that, while more flexible than the multinomial, the DM\n",
    "still ignores the possibility of underlying correlations between categories.\n",
    "If one of our tree species relies on another, for instance, the model we've used here\n",
    "will not effectively account for this.\n",
    "In that case, swapping the vanilla Dirichlet distribution for something fancier (e.g. the [Generalized Dirichlet](https://en.wikipedia.org/wiki/Generalized_Dirichlet_distribution) or [Logistic-Multivariate Normal](https://en.wikipedia.org/wiki/Logit-normal_distribution#Multivariate_generalization)) may be worth considering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    ":::{bibliography}\n",
    ":filter: docname in docnames\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "* Authored by [Byron J. Smith](https://github.com/bsmith89) on Jan, 2021 ([pymc-examples#18](https://github.com/pymc-devs/pymc-examples/pull/18))\n",
    "* Updated by Abhipsha Das and Oriol Abril-Pla on August, 2021 ([pymc-examples#212](https://github.com/pymc-devs/pymc-examples/pull/212))\n",
    "* Updated to PyMC v5 by Erik Werner on Oct, 2023 ([pymc-examples#581](https://github.com/pymc-devs/pymc-examples/pull/581))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T18:05:33.955783Z",
     "start_time": "2021-01-25T18:05:33.904842Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p pytensor,xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{include} page_footer.md\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
