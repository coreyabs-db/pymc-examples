{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15cd228-d1cd-4d52-bc62-92aa975f798c",
   "metadata": {},
   "source": [
    "(interrupted_time_series)=\n",
    "# Interrupted time series analysis\n",
    "\n",
    ":::{post} Oct, 2022\n",
    ":tags: counterfactuals, causal inference, time series, forecasting, causal impact, quasi experiments\n",
    ":category: intermediate\n",
    ":author: Benjamin T. Vincent\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f342a5c-6d5b-4639-9655-a33cb92befab",
   "metadata": {},
   "source": [
    "This notebook focuses on how to conduct a simple Bayesian [interrupted time series analysis](https://en.wikipedia.org/wiki/Interrupted_time_series). This is useful in [quasi-experimental settings](https://en.wikipedia.org/wiki/Quasi-experiment) where an intervention was applied to all treatment units. \n",
    "\n",
    "For example, if a change to a website was made and you want to know the causal impact of the website change then _if_ this change was applied selectively and randomly to a test group of website users, then you may be able to make causal claims using the [A/B testing approach](https://en.wikipedia.org/wiki/A/B_testing).\n",
    "\n",
    "However, if the website change was rolled out to _all_ users of the website then you do not have a control group. In this case you do not have a direct measurement of the counterfactual, what _would have happened if_ the website change was not made. In this case, if you have data over a 'good' number of time points, then you may be able to make use of the interrupted time series approach.\n",
    "\n",
    "Interested readers are directed to the excellent textbook [The Effect](https://theeffectbook.net/) {cite:p}`huntington2021effect`. Chapter 17 covers 'event studies' which the author prefers to the interrupted time series terminology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fdc46-1447-4b8a-af5f-d8fcef695858",
   "metadata": {},
   "source": [
    "## Causal DAG\n",
    "\n",
    "A simple causal DAG for the interrupted time series is given below, but see {cite:p}`huntington2021effect` for a more general DAG. In short it says:\n",
    "\n",
    "* The outcome is causally influenced by time (e.g. other factors that change over time) and by the treatment.\n",
    "* The treatment is causally influenced by time.\n",
    "\n",
    "![](DAG_interrupted_time_series.png)\n",
    "\n",
    "Intuitively, we could describe the logic of the approach as:\n",
    "* We know that the outcome varies over time.\n",
    "* If we build a model of how the outcome varies over time _before_ the treatment, then we can predit the counterfactual of what we would expect to happen _if_ the treatment had not occurred.\n",
    "* We can compare this counterfactual with the observations from the time of the intervention onwards. If there is a meaningful discrepancy then we can attribute this as a causal impact of the intervention. \n",
    "\n",
    "This is reasonable if we have ruled out other plausible causes occurring at the same point in time as (or after) the intervention. This becomes more tricky to justify the more time has passed since the intervention because it is more likely that other relevant events maye have occurred that could provide alternative causal explanations.\n",
    "\n",
    "If this does not make sense immediately, I recommend checking the example data figure below then revisiting this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fbb462-3baf-4b8d-aad4-270bbd0a4018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import xarray as xr\n",
    "\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d3310-56d9-43cb-9baa-178e155eba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "RANDOM_SEED = 8927\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd94c9-c102-4116-91a1-50fe396ca089",
   "metadata": {},
   "source": [
    "Now let's define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaae928-aabe-410d-b345-237a7c2361d2",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def format_x_axis(ax, minor=False):\n",
    "    # major ticks\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y %b\"))\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.grid(which=\"major\", linestyle=\"-\", axis=\"x\")\n",
    "    # minor ticks\n",
    "    if minor:\n",
    "        ax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%Y %b\"))\n",
    "        ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "        ax.grid(which=\"minor\", linestyle=\":\", axis=\"x\")\n",
    "    # rotate labels\n",
    "    for label in ax.get_xticklabels(which=\"both\"):\n",
    "        label.set(rotation=70, horizontalalignment=\"right\")\n",
    "\n",
    "\n",
    "def plot_xY(x, Y, ax):\n",
    "    quantiles = Y.quantile((0.025, 0.25, 0.5, 0.75, 0.975), dim=(\"chain\", \"draw\")).transpose()\n",
    "\n",
    "    az.plot_hdi(\n",
    "        x,\n",
    "        hdi_data=quantiles.sel(quantile=[0.025, 0.975]),\n",
    "        fill_kwargs={\"alpha\": 0.25},\n",
    "        smooth=False,\n",
    "        ax=ax,\n",
    "    )\n",
    "    az.plot_hdi(\n",
    "        x,\n",
    "        hdi_data=quantiles.sel(quantile=[0.25, 0.75]),\n",
    "        fill_kwargs={\"alpha\": 0.5},\n",
    "        smooth=False,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.plot(x, quantiles.sel(quantile=0.5), color=\"C1\", lw=3)\n",
    "\n",
    "\n",
    "# default figure sizes\n",
    "figsize = (10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f2651-5817-40c4-b19f-1b7478e6b167",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "\n",
    "The focus of this example is on making causal claims using the interrupted time series approach. Therefore we will work with some relatively simple synthetic data which only requires a very simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7480e8-d034-400b-98d5-09579532fa4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "treatment_time = \"2017-01-01\"\n",
    "β0 = 0\n",
    "β1 = 0.1\n",
    "dates = pd.date_range(\n",
    "    start=pd.to_datetime(\"2010-01-01\"), end=pd.to_datetime(\"2020-01-01\"), freq=\"M\"\n",
    ")\n",
    "N = len(dates)\n",
    "\n",
    "\n",
    "def causal_effect(df):\n",
    "    return (df.index > treatment_time) * 2\n",
    "\n",
    "\n",
    "df = (\n",
    "    pd.DataFrame()\n",
    "    .assign(time=np.arange(N), date=dates)\n",
    "    .set_index(\"date\", drop=True)\n",
    "    .assign(y=lambda x: β0 + β1 * x.time + causal_effect(x) + norm(0, 0.5).rvs(N))\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ecae9-efc2-4a68-99ac-d1678add5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into pre and post intervention dataframes\n",
    "pre = df[df.index < treatment_time]\n",
    "post = df[df.index >= treatment_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e7541-ae4a-4c0f-9068-5b8299ec7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = pre[\"y\"].plot(label=\"pre\")\n",
    "post[\"y\"].plot(ax=ax, label=\"post\")\n",
    "ax.axvline(treatment_time, c=\"k\", ls=\":\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a77d5-a75c-4be5-b61c-1f4fc036373d",
   "metadata": {},
   "source": [
    "In this simple dataset, we have a noisy linear trend upwards, and because this data is synthetic we know that we have a step increase in the outcome at the intervention time, and this effect is persistent over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec0179-4708-4543-bab2-b0075befcbbd",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "Here we build a simple linear model. Remember that we are building a model of the pre-intervention data with the goal that it would do a reasonable job of forecasting what would have happened if the intervention had not been applied. Put another way, we are _not_ modelling any aspect of the post-intervention observations such as a change in intercept, slope or whether the effect is transient or permenent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742cc93-4b45-4bc2-9e13-caebc5cb217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # observed predictors and outcome\n",
    "    time = pm.MutableData(\"time\", pre[\"time\"].to_numpy(), dims=\"obs_id\")\n",
    "    # priors\n",
    "    beta0 = pm.Normal(\"beta0\", 0, 1)\n",
    "    beta1 = pm.Normal(\"beta1\", 0, 0.2)\n",
    "    # the actual linear model\n",
    "    mu = pm.Deterministic(\"mu\", beta0 + (beta1 * time), dims=\"obs_id\")\n",
    "    sigma = pm.HalfNormal(\"sigma\", 2)\n",
    "    # likelihood\n",
    "    pm.Normal(\"obs\", mu=mu, sigma=sigma, observed=pre[\"y\"].to_numpy(), dims=\"obs_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ac9fe-656e-424d-8647-2d34e583b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d785c4e",
   "metadata": {},
   "source": [
    "## Prior predictive check\n",
    "\n",
    "As part of the Bayesian workflow, we will plot our prior predictions to see what outcomes the model finds before having observed any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3413f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    idata = pm.sample_prior_predictive(random_seed=RANDOM_SEED)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "plot_xY(pre.index, idata.prior_predictive[\"obs\"], ax)\n",
    "format_x_axis(ax)\n",
    "ax.plot(pre.index, pre[\"y\"], label=\"observed\")\n",
    "ax.set(title=\"Prior predictive distribution in the pre intervention era\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c9c0c8-038c-401c-b683-c00e3c7c3950",
   "metadata": {},
   "source": [
    "This seems reasonable in that the priors over the intercept and slope are broad enough to lead to predicted observations which easily contain the actual data. This means that the particular priors chosen will not unduly constrain the posterior parameter estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3619f",
   "metadata": {},
   "source": [
    "## Inference \n",
    "Draw samples for the posterior distribution, and remember we are doing this for the pre intervention data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    idata.extend(pm.sample(random_seed=RANDOM_SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515704f7-b22c-40bf-bac7-a9d633c72911",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata, var_names=[\"~mu\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626b5cd",
   "metadata": {},
   "source": [
    "## Posterior predictive check\n",
    "\n",
    "Another important aspect of the Bayesian workflow is to plot the model's posterior predictions, allowing us to see how well the model can retrodict the already observed data. It is at this point that we can decide whether the model is too simple (then we'd build more complexity into the model) or if it's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    idata.extend(pm.sample_posterior_predictive(idata, random_seed=RANDOM_SEED))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "az.plot_hdi(pre.index, idata.posterior_predictive[\"obs\"], hdi_prob=0.5, smooth=False)\n",
    "az.plot_hdi(pre.index, idata.posterior_predictive[\"obs\"], hdi_prob=0.95, smooth=False)\n",
    "ax.plot(pre.index, pre[\"y\"], label=\"observed\")\n",
    "format_x_axis(ax)\n",
    "ax.set(title=\"Posterior predictive distribution in the pre intervention era\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5f4a0",
   "metadata": {},
   "source": [
    "The next step is not strictly necessary, but we can calculate the difference between the model retrodictions and the data to look at the errors. This can be useful to identify any unexpected inability to retrodict pre-intervention data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262a689",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# convert outcome into an XArray object with a labelled dimension to help in the next step\n",
    "y = xr.DataArray(pre[\"y\"].to_numpy(), dims=[\"obs_id\"])\n",
    "\n",
    "# do the calculation by taking the difference\n",
    "excess = y - idata.posterior_predictive[\"obs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6293e-9af8-4b37-bd42-8e3a8c385665",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "# the transpose is to keep arviz happy, ordering the dimensions as (chain, draw, time)\n",
    "az.plot_hdi(pre.index, excess.transpose(..., \"obs_id\"), hdi_prob=0.5, smooth=False)\n",
    "az.plot_hdi(pre.index, excess.transpose(..., \"obs_id\"), hdi_prob=0.95, smooth=False)\n",
    "format_x_axis(ax)\n",
    "ax.axhline(y=0, color=\"k\")\n",
    "ax.set(title=\"Residuals, pre intervention\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86cff8",
   "metadata": {},
   "source": [
    "## Counterfactual inference\n",
    "Now we will use our model to predict the observed outcome in the 'what if?' scenario of no intervention.\n",
    "\n",
    "So we update the model with the `time` data from the `post` intervention dataframe and run posterior predictive sampling to predict the observations we would observe in this counterfactual scenario. We could also call this 'forecasting'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c1460",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    pm.set_data(\n",
    "        {\n",
    "            \"time\": post[\"time\"].to_numpy(),\n",
    "        }\n",
    "    )\n",
    "    counterfactual = pm.sample_posterior_predictive(\n",
    "        idata, var_names=[\"obs\"], random_seed=RANDOM_SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606e4f8-e87b-479a-8876-14f58d3fd709",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "plot_xY(post.index, counterfactual.posterior_predictive[\"obs\"], ax)\n",
    "format_x_axis(ax, minor=False)\n",
    "ax.plot(post.index, post[\"y\"], label=\"observed\")\n",
    "ax.set(\n",
    "    title=\"Counterfactual: Posterior predictive forecast of outcome if intervention not taken place\"\n",
    ")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39dbf52-22b7-49e1-9680-6ff5b2c43768",
   "metadata": {},
   "source": [
    "We now have the ingredients needed to calculate the causal impact. This is simply the difference between the Bayesian counterfactual predictions and the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbe879-c745-49aa-8229-217aa2659de7",
   "metadata": {},
   "source": [
    "## Causal impact: since the intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c89720",
   "metadata": {},
   "source": [
    "Now we'll use the predicted outcome under the counterfactual scenario and compare that to the observed outcome to come up with our counterfactual estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0236db3-5bca-4fae-99dc-a209c7b77d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert outcome into an XArray object with a labelled dimension to help in the next step\n",
    "outcome = xr.DataArray(post[\"y\"].to_numpy(), dims=[\"obs_id\"])\n",
    "\n",
    "# do the calculation by taking the difference\n",
    "excess = outcome - counterfactual.posterior_predictive[\"obs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3922b24c-b84b-45b2-8592-0e319ba202aa",
   "metadata": {},
   "source": [
    "And we can easily compute the cumulative causal impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdb01f-7a8e-4fda-92d3-4a921ac099e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cumulative causal impact\n",
    "cumsum = excess.cumsum(dim=\"obs_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fc14d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(figsize[0], 9), sharex=True)\n",
    "\n",
    "# Plot the excess\n",
    "# The transpose is to keep arviz happy, ordering the dimensions as (chain, draw, t)\n",
    "plot_xY(post.index, excess.transpose(..., \"obs_id\"), ax[0])\n",
    "format_x_axis(ax[0], minor=True)\n",
    "ax[0].axhline(y=0, color=\"k\")\n",
    "ax[0].set(title=\"Causal impact, since intervention\")\n",
    "\n",
    "# Plot the cumulative excess\n",
    "plot_xY(post.index, cumsum.transpose(..., \"obs_id\"), ax[1])\n",
    "format_x_axis(ax[1], minor=False)\n",
    "ax[1].axhline(y=0, color=\"k\")\n",
    "ax[1].set(title=\"Cumulative causal impact, since intervention\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29725c72-2de9-493a-a1d2-3aa80a5fe866",
   "metadata": {},
   "source": [
    "And there we have it - we've done some Bayesian counterfactual inference in PyMC using the interrupted time series approach! In just a few steps we've:\n",
    "- Built a simple model to predict a time series.\n",
    "- Inferred the model parameters based on pre intervention data, running prior and posterior predictive checks. We note that the model is pretty good.\n",
    "- Used the model to create counterfactual predictions of what would happen after the intervention time if the intervention had not occurred.\n",
    "- Calculated the causal impact (and cumulative causal impact) by comparing the observed outcome to our counterfactual expected outcome in the case of no intervention.\n",
    "\n",
    "There are of course many ways that the interrupted time series approach could be more involved in real world settings. For example there could be more temporal structure, such as seasonality. If so then we might want to use a specific time series model, not just a linear regression model. There could also be additional informative predictor variables to incorporate into the model. Additionally some designs do not just consist of pre and post intervention periods (also known as A/B designs), but could also involve a period where the intervention is inactive, active, and then inactive (also known as an ABA design)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2ee6b-2581-4ee5-a305-b9712dd49f09",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    ":::{bibliography}\n",
    ":filter: docname in docnames\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80160e7d",
   "metadata": {},
   "source": [
    "## Authors\n",
    "- Authored by [Benjamin T. Vincent](https://github.com/drbenvincent) in October 2022.\n",
    "- Updated by Benjamin T. Vincent in February 2023 to run on PyMC v5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f5198",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f75a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -p pytensor,aeppl,xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a462f",
   "metadata": {},
   "source": [
    ":::{include} ../page_footer.md\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "pymc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
